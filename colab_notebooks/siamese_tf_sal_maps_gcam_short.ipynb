{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese_tf_sal_maps_gcam_short.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1IwE5Y2g7afZDpCVWcfS8FCBV9-oJMyGa",
      "authorship_tag": "ABX9TyNcwZI6ciT5DgpWYe+LHt8I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashank3110/Master_Thesis_BA_DeepVis/blob/master/colab_notebooks/siamese_tf_sal_maps_gcam_short.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuM4VvCQ1UD3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "8deeffc0-6e0f-40d9-9a15-5e5155ad5736"
      },
      "source": [
        "!git clone https://github.com/LLNL/fastcam.git\n",
        "\n",
        "!pip install pytorch_gradcam"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fastcam'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 553 (delta 89), reused 108 (delta 62), pack-reused 414\u001b[K\n",
            "Receiving objects: 100% (553/553), 18.13 MiB | 22.13 MiB/s, done.\n",
            "Resolving deltas: 100% (329/329), done.\n",
            "Requirement already satisfied: pytorch_gradcam in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from pytorch_gradcam) (4.1.2.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_gradcam) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spgq0ILhQ0X_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4O2sCIFpzeH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d495b993-7013-4004-e4f4-3eff84148840"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "%cd fastcam\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "import logging\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import concatenate\n",
        "import torch.nn as nn\n",
        "from torch.nn import BatchNorm3d,Conv3d,ReLU,MaxPool3d,Linear,AdaptiveAvgPool3d,Flatten,Softmax\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from datetime import datetime\n",
        "from torch.utils import data\n",
        "import time\n",
        "\n",
        "\n",
        "import skimage.io as sio\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "\n",
        "from skimage.transform import resize\n",
        "import skimage.io as sio\n",
        "from scipy.io import savemat,loadmat\n",
        "import cv2\n",
        "\n",
        "import mask\n",
        "import draw\n",
        "import norm\n",
        "import misc\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "from random import shuffle\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "import pandas as pd\n",
        "from gradcam.utils import visualize_cam\n",
        "from gradcam import GradCAMpp, GradCAM\n",
        "from matplotlib import pyplot as plt\n",
        "\"\"\"#**Read Nii images code**\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "def get_smoe_map(x,relu=True):\n",
        "  print(f' smoe input shape={x.shape}')\n",
        "  if relu:\n",
        "    x=tf.nn.relu(x).numpy()\n",
        "  print(f'x range={np.amax(x),np.amin(x)}')\n",
        "  \n",
        "  m   = np.mean(x,axis=0)+0.0000001 #avoid log 0\n",
        "  \n",
        "  x   = x + 0.0000001\n",
        "  k   = np.log2(m) - np.mean(np.log2(x), axis=0)\n",
        "  k   = k + 0.0000001\n",
        "  # k   = np.log10(m) - np.mean(np.log10(x), axis=-1)\n",
        "  print(np.array_equal(np.zeros(k.shape),k))\n",
        "  print(f'{x.shape,k.shape,np.amin(k)}')\n",
        "  print(f'smoe map={k}')\n",
        "  print(f'mean={m}')\n",
        "  th  = k * m\n",
        "  print(f'smoe output shape={th.shape}')\n",
        "  return th\n",
        "\n",
        "def get_std_map(x):\n",
        "  print(f'before std map shape ={x.shape}')\n",
        "  m = np.std(x,axis=-1)\n",
        "\n",
        "  print(f'std map shape ={m.shape}')\n",
        "\n",
        "  return m\n",
        "\n",
        "def get_norm(x,const_mean=None,const_std=None):\n",
        "  # print(f'x shape={x.shape}')\n",
        "  s0      = x.shape[0]\n",
        "  s1      = x.shape[1]\n",
        "  # s2      = x.shape[2]\n",
        "  # s3      = x.shape[3]\n",
        "  # print(s0,s1,s2)\n",
        "  x       = np.reshape(x,(1,s0*s1))\n",
        "  # x       = np.reshape(x,(s0,s1*s2*s3))\n",
        "  # print(x)\n",
        "  '''\n",
        "      Compute Mean\n",
        "  '''\n",
        "  if const_mean is None:\n",
        "      m       = np.mean(x,axis=1)\n",
        "      m       = np.reshape(m,(m.shape[0],1))\n",
        "  else:\n",
        "      m       = const_mean\n",
        "      \n",
        "  '''\n",
        "      Compute Standard Deviation\n",
        "  '''\n",
        "  if const_std is None:\n",
        "      s       = np.std(x,axis=1)\n",
        "      s       = np.reshape(s,(s.shape[0],1))\n",
        "  else:\n",
        "      s       = const_std\n",
        "  \n",
        "  '''\n",
        "      The normal cumulative distribution function is used to squash the values from within the range of 0 to 1\n",
        "  '''\n",
        "  # print(s)\n",
        "  s=torch.tensor(s)\n",
        "  x       = 0.5*(1.0 + torch.erf((x-m)/(s*torch.sqrt(torch.tensor(2.0)))))\n",
        "  print(x.shape)    \n",
        "  x       = x.reshape(1,s0,s1)\n",
        "  # x       = x.reshape(s0,s1,s2,s3)\n",
        "  \n",
        "  return x\n",
        "  # return torch.mean(x,dim=-1)\n",
        "\n",
        "def combine_sal_maps(smaps,output_size,weights,map_num,resize_mode='bilinear',do_relu=False):\n",
        "  bn  = smaps[0].shape[0]\n",
        "  cm  = torch.zeros((bn, 1, output_size[0], output_size[1]), dtype=smaps[0].dtype, device=smaps[0].device)\n",
        "  ww  = []\n",
        "  \n",
        "  '''\n",
        "      Now get each saliency map and resize it. Then store it and also create a combined saliency map.\n",
        "  '''\n",
        "  for i in range(len(smaps)):\n",
        "      # assert torch.is_tensor(smaps[i]), \"Each saliency map must be a Torch Tensor.\"\n",
        "      wsz = smaps[i].shape\n",
        "      w   = np.reshape(smaps[i],(wsz[0], 1, wsz[1], wsz[2]))#smaps[i].reshape(wsz[0], 1, wsz[1], wsz[2])\n",
        "      # w=torch.from_numpy(w)\n",
        "      # print(type(w))\n",
        "      w   = nn.functional.interpolate(w, size=output_size, mode=resize_mode, align_corners=False) \n",
        "      ww.append(w)        # should we weight the raw maps ... hmmm\n",
        "      \n",
        "      cm  += (w * weights[i])\n",
        "\n",
        "  '''\n",
        "      Finish the combined saliency map to make it a weighted average.\n",
        "  '''\n",
        "  weight_sum =sum(weights)\n",
        "  cm  = cm / weight_sum\n",
        "  cm  = cm.reshape(bn, output_size[0],output_size[1])\n",
        "  \n",
        "  ww  = torch.stack(ww,dim=1)\n",
        "  ww  = ww.reshape(bn, map_num, output_size[0], output_size[1])\n",
        "  \n",
        "  # if do_relu:\n",
        "  #     cm = F.relu(cm)\n",
        "  #     ww = F.relu(ww)\n",
        "  \n",
        "  return cm, ww\n",
        "\n",
        "def compute_saliency_tf(base_path,inputs,tf_model):\n",
        "\n",
        "  # gender=inputs[1]\n",
        "  # gender=tf.reshape(gender,[1,1])\n",
        "  img=inputs\n",
        "  img_chunk= img #tf.convert_to_tensor(img)\n",
        "  print(img_chunk.shape)\n",
        "  img_chunk = tf.reshape(img_chunk,[1,121,145,1])\n",
        "  layers=[layer.name for layer in tf_model.layers]\n",
        "  outputs=[]\n",
        "\n",
        " \n",
        "  j=0\n",
        "  for i,l in enumerate(layers):\n",
        "    if 'activation' in l:\n",
        "      val=tf_model.get_layer(name=l).output\n",
        "      print(i,j,l,val.shape)\n",
        "      j+=1\n",
        "      outputs.append(val) \n",
        "  outputs.append(encoder.output) \n",
        "# test_tf_model=tf.keras.models.Model(inputs=encoder.inputs,outputs= outputs)\n",
        "  # for l in layers:\n",
        "  #   # if l in ['activation','activation_2','activation_1']:\n",
        "  #   if l.startswith('activation'):\n",
        "  #   # if l.startswith('batch_normalization') or l in ['conv3d_48','conv3d_49','conv3d_50','conv3d_63',\\\n",
        "  #   # \n",
        "  #       outputs.append(tf_model.get_layer(name=l).output) \n",
        "  # outputs.append(tf_model.output)                                         \n",
        "  test_tf_model=tf.keras.models.Model(tf_model.inputs, outputs)\n",
        "  # test_tf_model=tf.keras.models.Model([tf_model.inputs], [tf_model.get_layer(name=layers[2]).output,tf_model.get_layer(name=layers[6]).output,tf_model.get_layer(name=layers[9]).output, tf_model.output])\n",
        "\n",
        "  predictions = test_tf_model(img_chunk)\n",
        "\n",
        "  # hooks=[predictions[0],predictions[1],predictions[2],predictions[8],predictions[14],predictions[20],predictions[23]\\\n",
        "  #        ,predictions[29],predictions[35],predictions[41],predictions[47],predictions[50],\\\n",
        "  #        predictions[56],predictions[62],predictions[65]]#predictions[:layer_end]\n",
        "  # hooks= [predictions[0],predictions[2],predictions[17],predictions[47],predictions[62]] #1x1 and 3x3 cnn\n",
        "  hooks=[predictions[0],predictions[9],predictions[22],predictions[41],predictions[51]] #1x1 cnn\n",
        "  # hooks=predictions[:9]+predictions[-9:layer_end]\n",
        "\n",
        "  # choose specific channels / filters\n",
        "  for x in hooks:\n",
        "    print('ouput shapes layerwise')\n",
        "    print(x.shape)\n",
        "  #   print(x.numpy().shape)\n",
        "  #   print(np.mean(x.numpy(),axis=-2)[:,:,:,-1].shape)\n",
        "  # sal_maps       = [ get_norm(get_smoe_map(np.expand_dims(np.mean(x.numpy()[:,:,:,:,:],axis=-2)[:,:,:,2],axis=-1))) for x in hooks ]\n",
        "\n",
        "  #smoe\n",
        "  sal_maps       = [ get_norm(get_smoe_map(np.mean(x.numpy()[:,:,:,:],axis=-1))) for x in hooks ]\n",
        "\n",
        "  #std dev\n",
        "  # sal_maps       = [ get_norm(get_std_map(np.mean(x.numpy()[:,:,:,:,:],axis=-2))) for x in hooks ]\n",
        "\n",
        "  # sal_maps       = [ get_norm(get_smoe_map(x.numpy()[:,:,:,:,:])) for x in hooks ]\n",
        "  for smaps in sal_maps:\n",
        "    print(smaps.shape)\n",
        "  # return\n",
        "  # sal_maps = [np.reshape(smaps.numpy(),(121,121)) for smaps in sal_maps]\n",
        "\n",
        "  # all layer scale maps with equal weightage\n",
        "  weights=np.ones(len(hooks))\n",
        "   # all layer scale maps with progressive increasing weightage\n",
        "  # weights=[i+1 for i in range(len(hooks))]\n",
        "  # weights = [i for i in range(len(hooks),0,-1)]\n",
        "  map_num=len(hooks)\n",
        "\n",
        "  f, axarr = plt.subplots(1,1,figsize=(10,10))\n",
        "  raw= np.mean(img_chunk[0,:,:,:],axis=-1)\n",
        "  raw= raw/np.max(raw)\n",
        "  r=axarr.imshow(raw,cmap='jet')\n",
        "  axarr.set_title('Input image')\n",
        "  plt.colorbar(r,fraction=0.01, pad=0.04)\n",
        "  plt.savefig(base_path+'input_chunk.png')\n",
        "\n",
        "  csal_maps,sal_maps = combine_sal_maps(sal_maps,output_size=[in_height,in_width],weights=weights,map_num=map_num)\n",
        "  output_path = base_path +'Map_Combined.png'\n",
        "  f, axarr = plt.subplots(1,1,figsize=(10,10))\n",
        "  csal_map=csal_maps[0,:,:].numpy()\n",
        "  imcs=csal_map/np.max(csal_map)\n",
        "  im = axarr.imshow(imcs,cmap='jet')\n",
        "  axarr.set_title('Combined saliency map')\n",
        "  plt.colorbar(im,fraction=0.01, pad=0.04)\n",
        "  plt.savefig(output_path)\n",
        "\n",
        "  il = [sal_maps[0,i,:,:] for i in range(map_num)] # Put each saliency map into the figure\n",
        "  il.append(csal_maps[0,:,:])                       # add in the combined map at the end of the figure\n",
        "  images        = [torch.stack(il, 0)]          \n",
        "  images        = make_grid(images, nrow=5)\n",
        "  sal_img=images.unsqueeze(1)\n",
        "  output_path=base_path +'Sal_Maps.png'\n",
        "  save_image(sal_img,output_path)\n",
        "\n",
        "  input_path = output_path\n",
        "  f, axarr = plt.subplots(1,1,figsize=(10,10))\n",
        "  im=sio.imread(input_path)\n",
        "  im=axarr.imshow(np.mean(im,axis=-1)/255, cmap='jet');\n",
        "  axarr.set_title('layerwise saliency maps')\n",
        "  plt.colorbar(im,fraction=0.01, pad=0.04)\n",
        "  output_path=base_path +'Sal_Maps_jet.png'\n",
        "  plt.savefig(output_path)\n",
        "  return csal_maps\n",
        "\n",
        "\n",
        "\n",
        "# \"\"\"#**Saliency Map section begins**\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %cd fastcam/\n",
        "from random import shuffle\n",
        "def get_scan_from_subjects(data_path,subject_ids,label_df):\n",
        "    scans=[]\n",
        "    labels=[]\n",
        "    gender=[]\n",
        "    cdr=[]\n",
        "    ids=[]\n",
        "    for subject in subject_ids :\n",
        "        path=os.path.join(data_path,subject)\n",
        "        paths=os.listdir(path)\n",
        "        ids.extend([scan_id.split('.')[0] for scan_id in paths])\n",
        "        scans.extend([ os.path.join(path,scan_path) for scan_path in paths])\n",
        "        #ll=[label_df[label_df['MRI ID']==scan_id.split('.')[0]]['Age'] for scan_id in paths ]\n",
        "        #print(f'********{ll}**********')\n",
        "        #print(label_df['MRI ID'])\n",
        "        '''\n",
        "        for scan_id in paths:\n",
        "            age=label_df[label_df['MRI ID']==scan_id.split('.')[0]]['Age']\n",
        "            gender=label_df[label_df['MRI ID']==scan_id.split('.')[0]]['M/F']\n",
        "            print(f\"age={age},gender={gender}\")\n",
        "            labels.append(age)\n",
        "            gender.append(gender)\n",
        "        '''\n",
        "        labels.extend([label_df[label_df['MRI ID']==scan_id.split('.')[0]]['Age'].to_list()[0] for scan_id in paths ])\n",
        "        gender.extend([label_df[label_df['MRI ID']==scan_id.split('.')[0]]['M/F'].to_list()[0] for scan_id in paths ])\n",
        "        cdr.extend([label_df[label_df['MRI ID']==scan_id.split('.')[0]]['CDR'].to_list()[0] for scan_id in paths ])\n",
        "    #print(labels,gender)\n",
        "           \n",
        "    # shuffle(scans)\n",
        "    \n",
        "    return scans,labels,gender,ids,cdr\n",
        "\n",
        "def get_test_files(label_path,data_path,debug_mode_subject=None):\n",
        "\n",
        "    data = pd.read_csv(label_path)\n",
        "    data = data.rename(columns={'MR ID':'MRI ID'})\n",
        "    data['M/F'] = encode_gender(data)\n",
        "    if debug_mode_subject is None:\n",
        "      test_ids = os.listdir(data_path)\n",
        "    else:\n",
        "      test_ids=debug_mode_subject\n",
        "\n",
        "    shuffle(test_ids)\n",
        "    test_patients,test_labels,test_gender,scan_ids,test_cdr = get_scan_from_subjects(data_path,test_ids,data)\n",
        "    #test_patients = [os.path.join(data_path, id) for id in test_ids ]\n",
        "    \n",
        "    #test_labels = [data[data['MRI ID']==id.split('.')[0]]['Age'].values[0] for id in test_ids]   \n",
        "    #test_gender = [data[data['MRI ID']==id.split('.')[0]]['M/F'] for id in test_ids]  \n",
        "    return test_patients,scan_ids, test_labels,test_gender,test_cdr\n",
        "\n",
        "def encode_gender(data):\n",
        "    data['M/F'] = pd.Categorical(data['M/F'])\n",
        "    \n",
        "    return data['M/F'].cat.codes\n",
        "\n",
        "def parse_function_image(example_proto):\n",
        "\n",
        "    features = {\n",
        "        'image': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image_shape': tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "\n",
        "    content = tf.io.parse_single_example(example_proto, features=features)\n",
        "\n",
        "    content['image_shape'] = tf.io.decode_raw(content['image_shape'], tf.int32)\n",
        "    content['image'] = tf.io.decode_raw(content['image'], tf.float32)\n",
        "    content['image'] = tf.reshape(content['image'], content['image_shape'])\n",
        "\n",
        "    return content['image']\n",
        "\n",
        "\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "# LAYER_NAME='conv3d'\n",
        "def get_grads(layer_name,tf_model,inputs):\n",
        "\n",
        "  cam_list=[]\n",
        "\n",
        "  img=inputs\n",
        "  grad_model = tf.keras.models.Model([tf_model.inputs], [tf_model.get_layer(name=layer_name).output, tf_model.output])\n",
        "  \n",
        "  img_chunk=img #tf.convert_to_tensor(img)\n",
        "  img_chunk = tf.reshape(img_chunk,[1,121,145,1])\n",
        " \n",
        "  with tf.GradientTape() as tape:\n",
        "      conv_outputs, predictions = grad_model(img_chunk)\n",
        "      print(f'predictions={predictions}')\n",
        "      # loss=predictions\n",
        "      loss = predictions[0] #classification\n",
        "      # predictions=np.argmax(predictions[0])#[::-1]\n",
        "      # print(f'sorted args={predictions}')\n",
        "      # loss = tf.convert_to_tensor(cdr_ohe_dict[cdr_keys[predictions]],dtype=tf.float32)#[0][0]#[:, :]\n",
        "      # print(loss,type(loss))\n",
        "      # loss=tf.convert_to_tensor(np.max(predictions),dtype=tf.float32)\n",
        "  output = conv_outputs[0]#[0,:,:,:,100]\n",
        "  print(f'entering tape gradients')\n",
        "  # grads = tape.gradient(loss, conv_outputs)[0]\n",
        "  grads = tape.gradient(loss, conv_outputs)[0]#[0,:,:,:,100]\n",
        "  print(type(grads))\n",
        "  print(f'Crossed tape gradients')\n",
        "  gate_f = tf.cast(output > 0, 'float32')\n",
        "  gate_r = tf.cast(grads > 0, 'float32')\n",
        "  # now there are 2 choice either use grads(raw grads) or use guided grads)\n",
        "  guided_grads = tf.cast(output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads\n",
        "\n",
        "  print(f'Entering reduce mean using guided_grads with shape={guided_grads.shape}')\n",
        "  #guided grads\n",
        "  weights = tf.reduce_mean(guided_grads, axis=(0,1,2))\n",
        "\n",
        "\n",
        "  print(f'Computing CAM using output with shape:{output.shape}')\n",
        "\n",
        "  print(f'weights={weights.shape}')\n",
        "  cam = np.zeros(output.shape[0:3], dtype=np.float32)\n",
        "  print(cam.shape)\n",
        "\n",
        "   \n",
        "  cam=tf.reduce_sum(tf.multiply(output,weights),axis=-1)\n",
        "  cam_list.append(cam)\n",
        "  return cam_list,grads,loss,weights,output,img_chunk\n",
        "\n",
        "from skimage.transform import resize\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "# p='/content/drive/My Drive/Uni-Sem4/'+mname+'/layer_'+LAYER_NAME+'/'\n",
        "\n",
        "def compute_gcam_and_gcam_pp(layer_name,model,inputs):\n",
        "  # print(f'{inputs[0].shape,inputs[1].shape}')\n",
        "  cam_list,grads,loss,weights,output,img_chunk = get_grads(layer_name,model,inputs)\n",
        "  # if not os.path.exists(p) :\n",
        "  #   os.makedirs(p)\n",
        "  # j=chunk_start\n",
        "  heatmap_list=[]\n",
        "  for i,cam in enumerate(cam_list):#as we are doing chunk wise so this camlist will have only one cam\n",
        "    # img_chunk=tf.convert_to_tensor(img[:,:,j:j+6])\n",
        "    # j+=6\n",
        "    # img_chunk = tf.reshape(img_chunk,[1,121,145,6])\n",
        "    print(f'cam shape={cam.shape}')\n",
        "    \n",
        "    #gcam\n",
        "    cam_map=resize(cam,(img_chunk.shape[1],img_chunk.shape[2],img_chunk.shape[3]))\n",
        "\n",
        "    cam_map = np.maximum(cam_map,0)\n",
        "    original_image=img_chunk.numpy()\n",
        "   \n",
        "    heatmap = (cam_map - cam_map.min()) / (cam_map.max() - cam_map.min())\n",
        "\n",
        "  \n",
        "    print(original_image.shape)\n",
        "    image=np.mean(original_image[0,:,:,:],axis=-1)\n",
        "    print(image.shape)\n",
        "\n",
        "    mri_img=image#np.squeeze(image)\n",
        "    heatmap_list.append(heatmap)\n",
        "    # print(heatmap.min(),heatmap.max())\n",
        "\n",
        "    heatmap_gcam = (cam_map - cam_map.min()) / (cam_map.max() - cam_map.min())\n",
        "\n",
        "      \n",
        "    print(f'heatmap_gcam shape={heatmap_gcam.shape}')\n",
        "    gcam_img=(np.mean(heatmap_gcam,axis=-1)* 255).astype(\"uint8\")\n",
        "   \n",
        "    #gcam++\n",
        "    print(f'grads shape ={grads.shape},tf.exp(loss) shape={tf.exp(loss).shape}')\n",
        "    loss = np.mean(loss)\n",
        "    conv_first_grad = tf.exp(loss)*grads\n",
        "    #second_derivative\n",
        "    conv_second_grad = tf.exp(loss)*grads*grads\n",
        "    #triple_derivative\n",
        "    conv_third_grad = tf.exp(loss)*grads*grads*grads\n",
        "    global_sum = np.sum(tf.reshape(output,(-1,conv_first_grad.shape[2])), axis=0)\n",
        "    print(f'conv_first_grad shape={conv_first_grad.shape},output.shape={output.shape},conv_second_grad shape={conv_second_grad.shape} ,  conv_third_grad shape={conv_third_grad.shape}, global_sum.shape={global_sum.shape}  ')\n",
        "    # alpha_num = conv_second_grad[0]\n",
        "    alpha_num = conv_second_grad\n",
        "    # alpha_denom = conv_second_grad[0]*2.0 + conv_third_grad[0]*global_sum.reshape((1,1,conv_first_grad[0].shape[2]))\n",
        "    alpha_denom = conv_second_grad*2.0 + conv_third_grad*global_sum.reshape((1,1,conv_first_grad.shape[2]))\n",
        "    alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, np.ones(alpha_denom.shape))\n",
        "    alphas = alpha_num/alpha_denom\n",
        "\n",
        "    # conv_third_grad = tf.exp(loss)[0]*grads*grads*grads\n",
        "    # global_sum = np.sum(tf.reshape(output,(-1,conv_first_grad[0].shape[2])), axis=0)\n",
        "    # alpha_num = conv_second_grad[0]\n",
        "    # # print(f'alpha_num shape global_sum shape,conv_third_grad[0] shape conv_first_grad[0] shape global_sum  shape={alpha_num.shape,global_sum.shape,conv_third_grad[0].shape,conv_first_grad[0].shape,global_sum.shape}')\n",
        "    # # global_sum=global_sum.reshape((1,1,conv_first_grad[0].shape[2])\n",
        "    # # print(conv_third_grad[0]*global_sum)\n",
        "    # alpha_denom = conv_second_grad[0]*2.0 + conv_third_grad[0]*global_sum.reshape((1,1,conv_first_grad[0].shape[2]))\n",
        "    # alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, np.ones(alpha_denom.shape))\n",
        "    # alphas = alpha_num/alpha_denom\n",
        "\n",
        "    alphas_thresholding = np.where(weights, alphas, 0.0)\n",
        "    print(f'alphas_thresholding shape={alphas_thresholding.shape}')\n",
        "    alpha_normalization_constant = np.sum(np.sum(alphas_thresholding, axis=0),axis=0)\n",
        "    alpha_normalization_constant_processed = np.where(alpha_normalization_constant != 0.0, alpha_normalization_constant, np.ones(alpha_normalization_constant.shape))\n",
        "    \n",
        "    print(f'alpha_normalization_constant_processed shape={alpha_normalization_constant_processed.shape}')\n",
        "    \n",
        "    alphas /= alpha_normalization_constant_processed.reshape((1,1,conv_first_grad.shape[2]))\n",
        "    weights_alpha=tf.reduce_sum(tf.multiply(weights,alphas),axis=0)\n",
        "    \n",
        "    cam=tf.reduce_sum(tf.multiply(output,weights_alpha),axis=-1)\n",
        "    \n",
        "    cam_map=resize(cam,(img_chunk.shape[1],img_chunk.shape[2],img_chunk.shape[3]))\n",
        "  \n",
        "    \n",
        "    print(f'cam_map={cam_map.shape}')\n",
        "    cam_map = np.maximum(cam_map, 0)\n",
        "\n",
        "    heatmap_gcam_pp = (cam_map - cam_map.min()) / (cam_map.max() - cam_map.min())\n",
        "\n",
        "\n",
        "    gcam_pp_img=(np.mean(heatmap_gcam_pp,axis=-1) * 255).astype(\"uint8\")\n",
        "    ##\n",
        "    print(img_chunk.shape,mri_img.shape,gcam_img.shape,type(mri_img),type(gcam_img))\n",
        "  \n",
        "        \n",
        "    return image, gcam_img,gcam_pp_img\n",
        "\n",
        "def combine_sal_gcam(base_path,csmap,gcam_img,gcam_pp_img,image,layer_name ):\n",
        "  print(gcam_img.shape,csmap.shape,gcam_pp_img.shape,image.shape)\n",
        "  #gcam\n",
        "  if np.max(gcam_img) ==0:\n",
        "    gcam_img = gcam_img+0.001\n",
        "  if np.max(gcam_pp_img) ==0:\n",
        "    gcam_pp_img = gcam_pp_img+0.001\n",
        "  gcam_img_tensor=torch.from_numpy(gcam_img).unsqueeze(0)\n",
        "  mask_gcam = csmap*(gcam_img_tensor)\n",
        "  mask_gcam=mask_gcam/mask_gcam.max()\n",
        "  raw_tensor=torch.from_numpy(image).unsqueeze(0)\n",
        "  heatmap_gcam, result_gcam = visualize_cam(mask_gcam, raw_tensor)\n",
        "  getMask                 = mask.SaliencyMaskDropout(keep_percent = 0.1, scale_map=False)\n",
        "  hard_masked_gcam,_       = getMask(raw_tensor.unsqueeze(0),mask_gcam)#.squeeze(0))\n",
        "  hard_masked_gcam        = hard_masked_gcam.squeeze(0)\n",
        "  masked_gcam             = misc.AlphaMask(raw_tensor, mask_gcam.squeeze(0)).squeeze(0)\n",
        "  masked_gcam              = misc.RangeNormalize(masked_gcam)\n",
        "\n",
        "  #gcam log operation\n",
        "  # raw_tensor = 20*np.log10(raw_tensor*255)\n",
        "  # mask_gcam = 20*np.log10(mask_gcam*255)\n",
        "  # masked_gcam =20*np.log10(masked_gcam*255)\n",
        "  # hard_masked_gcam = 20*np.log10(hard_masked_gcam*255)\n",
        "  # heatmap_gcam = 20*np.log10(heatmap_gcam*255)\n",
        "  # result_gcam = 20*np.log10(result_gcam*255)\n",
        "  ##\n",
        "\n",
        "  # image_list2 = []\n",
        "  # image_list2.append(torch.stack([raw_tensor.squeeze(), heatmap_pp2[0,:,:], \n",
        "  #                           result_pp2[0,:,:], masked_gcam, hard_masked_pp2[0,:,:]], 0))\n",
        "  #gcam++\n",
        "  gcam_pp_img_tensor=torch.from_numpy(gcam_pp_img).unsqueeze(0)\n",
        "  mask_gcam_pp = csmap*(gcam_pp_img_tensor)\n",
        "  mask_gcam_pp=mask_gcam_pp/mask_gcam_pp.max()\n",
        "  raw_tensor=torch.from_numpy(image).unsqueeze(0)\n",
        "  heatmap_gcam_pp, result_gcam_pp = visualize_cam(mask_gcam_pp, raw_tensor)\n",
        "\n",
        "  hard_masked_gcam_pp,_       = getMask(raw_tensor.unsqueeze(0),mask_gcam_pp)#.squeeze(0))\n",
        "  hard_masked_gcam_pp         = hard_masked_gcam_pp.squeeze(0)\n",
        "  masked_gcam_pp           = misc.AlphaMask(raw_tensor, mask_gcam_pp.squeeze(0)).squeeze(0)\n",
        "  masked_gcam_pp           = misc.RangeNormalize(masked_gcam_pp)\n",
        "\n",
        "  #input image\n",
        "  #log gcam++\n",
        " \n",
        "  # mask_gcam_pp = 20*np.log10(mask_gcam_pp*255)\n",
        "  # masked_gcam_pp =20*np.log10(masked_gcam_pp*255)\n",
        "  # hard_masked_gcam_pp = 20*np.log10(hard_masked_gcam_pp*255)\n",
        "  # heatmap_gcam_pp = 20*np.log10(heatmap_gcam_pp*255)\n",
        "  # result_gcam_pp = 20*np.log10(result_gcam_pp*255)\n",
        "  #\n",
        "  output_path   = base_path+\"raw_img.png\"\n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':raw_tensor.numpy() ,'shape':raw_tensor.shape})\n",
        "  # plt.imsave(output_path,gcam_img)#provision for png if required.\n",
        "  base_path+='_'+layer_name\n",
        "\n",
        "  #save gcam and gcam++ fig\n",
        "  vmin=np.amin([np.min(gcam_img),np.min(gcam_pp_img)])\n",
        "  vmax=np.amax([np.max(gcam_img),np.max(gcam_pp_img)])\n",
        "  # vmin=0\n",
        "  # vmax=1.0\n",
        "  # vmax=20*np.log10(255)\n",
        "  f, axarr = plt.subplots(1,2,figsize=(10,10))\n",
        "  img_plot = axarr[0].imshow(gcam_img,vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[0].set_title('Gradcam')\n",
        "  img_plot = axarr[1].imshow(gcam_pp_img,vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[1].set_title('Gradcam++')\n",
        "  plt.colorbar(img_plot,fraction=0.046, pad=0.04)\n",
        "  plt.savefig(base_path+'gcam_gcam++_fig.png')\n",
        "  #gcam\n",
        "# \n",
        "  vmin=np.amin([torch.min(raw_tensor),torch.min(csmap),torch.min(heatmap_gcam),torch.min(result_gcam),torch.min(masked_gcam),torch.min(hard_masked_gcam)])\n",
        "  vmax=np.amax([torch.max(raw_tensor),torch.max(csmap),torch.max(heatmap_gcam),torch.max(result_gcam),torch.max(masked_gcam),torch.max(hard_masked_gcam)])\n",
        "  vmin=0\n",
        "  vmax=1.0\n",
        "  f, axarr = plt.subplots(1,6,figsize=(20,20))\n",
        "  img_plot = axarr[0].imshow(torch.mean(raw_tensor,axis=0),vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[0].set_title('input')\n",
        "  img_plot = axarr[1].imshow(torch.mean(csmap,axis=0),vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[1].set_title('combined saliency map')\n",
        "  img_plot = axarr[2].imshow(torch.mean(heatmap_gcam,axis=0),vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[2].set_title('saliency map + gradcam')\n",
        "  img_plot = axarr[3].imshow(torch.mean(result_gcam,axis=0),vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[3].set_title('saliency map+gradcam with alpha blend')\n",
        "  img_plot = axarr[4].imshow(masked_gcam,vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[4].set_title('mask')\n",
        "  img_plot = axarr[5].imshow(hard_masked_gcam[0],vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[5].set_title('hard mask')\n",
        "  plt.colorbar(img_plot,fraction=0.046, pad=0.04)\n",
        "  plt.savefig(base_path+'sal+gcam_fig.png')\n",
        "\n",
        "  print(hard_masked_gcam.permute([2,0,1]).shape)\n",
        "  output_path   = base_path+\"gcam_img.png\"\n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':gcam_img ,'shape':gcam_img.shape})\n",
        "  output_path   = base_path+\"heatmap_gcam.png\"\n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':heatmap_gcam.permute([1,2,0]).numpy() ,'shape':heatmap_gcam.permute([1,2,0]).numpy().shape})\n",
        "  output_path   = base_path+\"result_gcam.png\"\n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':result_gcam.permute([1,2,0]).numpy() ,'shape':result_gcam.permute([1,2,0]).numpy().shape})\n",
        "  output_path   = base_path+\"hard_masked_gcam.png\" \n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':hard_masked_gcam.permute([1,2,0]).numpy() ,'shape':hard_masked_gcam.permute([1,2,0]).shape})\n",
        "  output_path   = base_path+\"masked_gcam.png\" \n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':masked_gcam.numpy() ,'shape':masked_gcam.numpy().shape})\n",
        "\n",
        "  #gcam_pp\n",
        "  vmin=np.amin([torch.min(raw_tensor),torch.min(csmap),torch.min(heatmap_gcam_pp),torch.min(result_gcam_pp),torch.min(masked_gcam_pp),torch.min(hard_masked_gcam_pp)])\n",
        "  vmax=np.amax([torch.max(raw_tensor),torch.max(csmap),torch.max(heatmap_gcam_pp),torch.max(result_gcam_pp),torch.max(masked_gcam_pp),torch.max(hard_masked_gcam_pp)])\n",
        "\n",
        "  vmin=0\n",
        "  vmax=1.0\n",
        "\n",
        "  f, axarr = plt.subplots(1,6,figsize=(20,20))\n",
        "  img_plot = axarr[0].imshow(torch.mean(raw_tensor,axis=0),vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[0].set_title('input')\n",
        "  img_plot = axarr[1].imshow(torch.mean(csmap,axis=0),vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[1].set_title('combined saliency map')\n",
        "  img_plot = axarr[2].imshow(torch.mean(heatmap_gcam_pp,axis=0),vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[2].set_title('saliency map + gradcam++')\n",
        "  img_plot = axarr[3].imshow(torch.mean(result_gcam_pp,axis=0),vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[3].set_title('saliency map+gradcam++ with alpha blend')\n",
        "  img_plot = axarr[4].imshow(masked_gcam_pp,vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[4].set_title('mask')\n",
        "  img_plot = axarr[5].imshow(hard_masked_gcam_pp[0],vmin=vmin,vmax=vmax, cmap='jet');\n",
        "  axarr[5].set_title('hard mask')\n",
        "  plt.colorbar(img_plot,fraction=0.046, pad=0.04)\n",
        "  plt.savefig(base_path+'sal+gcam++_fig.png')\n",
        "\n",
        "\n",
        "  raw_img = torch.mean(raw_tensor,axis=0).numpy()\n",
        "  output_path   = base_path+\"raw_input.png\"\n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':raw_img ,'shape':raw_img.shape})\n",
        "  csmap_img = torch.mean(csmap,axis=0).numpy()\n",
        "  output_path   = base_path+\"csmap.png\"\n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':csmap_img ,'shape':csmap_img.shape})\n",
        "  output_path   = base_path+\"gcam_pp_img.png\"\n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':gcam_pp_img ,'shape':gcam_pp_img.shape})\n",
        "  output_path   = base_path+\"heatmap_gcam_pp.png\"\n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':heatmap_gcam_pp.permute([1,2,0]).numpy() ,'shape':heatmap_gcam_pp.permute([1,2,0]).numpy().shape})\n",
        "  output_path   = base_path+\"result_gcam_pp.png\"\n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':result_gcam_pp.permute([1,2,0]).numpy() ,'shape':result_gcam_pp.permute([1,2,0]).numpy().shape})\n",
        "  output_path   = base_path+\"hard_masked_gcam_pp.png\" \n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':hard_masked_gcam_pp.permute([1,2,0]).numpy() ,'shape':hard_masked_gcam_pp.permute([1,2,0]).numpy().shape})\n",
        "  output_path   = base_path+\"masked_gcam_pp.png\" \n",
        "  savemat(output_path.split('.png')[0] +'.mat',{'data':masked_gcam_pp.numpy() ,'shape':masked_gcam_pp.numpy().shape})\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'fastcam'\n",
            "/content/fastcam/fastcam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W7Ed1NDQ8xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "siam_model=tf.saved_model.load('/content/drive/My Drive/BA_Estimation/models/exp_siam/Model')#/content/drive/My Drive/BA_Estimation/models/exp_siam/saved_model.pb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-rsfdbgfWyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir(siam_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIoc5ZLEguqp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5253caa8-5201-45a5-f6b4-94b7f54fb7f0"
      },
      "source": [
        "\n",
        "siam_model=tf.keras.models.load_model('/content/drive/My Drive/BA_Estimation/models/exp_siam/Model',compile=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:1045: UserWarning: Res34_model is not loaded, but a Lambda layer uses it. It may cause errors.\n",
            "  , UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbKo7okSnOgC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "outputId": "dc996119-d550-4c81-ec12-ccdf63ec46fc"
      },
      "source": [
        "siam_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"triplet\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "positive_input (InputLayer)     [(None, 121, 145, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "anchor_input (InputLayer)       [(None, 121, 145, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "negative_input (InputLayer)     [(None, 121, 145, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Functional)            (None, 128)          21713792                                     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_sub (TensorFlowOpLa (None, 128)          0           encoder[0][0]                    \n",
            "                                                                 encoder[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_sub_1 (TensorFlowOp (None, 128)          0           encoder[1][0]                    \n",
            "                                                                 encoder[2][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Square (TensorFlowO (None, 128)          0           tf_op_layer_sub[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Square_1 (TensorFlo (None, 128)          0           tf_op_layer_sub_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sum (TensorFlowOpLa (None, 1)            0           tf_op_layer_Square[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sum_1 (TensorFlowOp (None, 1)            0           tf_op_layer_Square_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Maximum (TensorFlow (None, 1)            0           tf_op_layer_Sum[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Maximum_1 (TensorFl (None, 1)            0           tf_op_layer_Sum_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_clip_by_value/Minim (None, 1)            0           tf_op_layer_Maximum[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_clip_by_value_1/Min (None, 1)            0           tf_op_layer_Maximum_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_clip_by_value (Tens (None, 1)            0           tf_op_layer_clip_by_value/Minimum\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_clip_by_value_1 (Te (None, 1)            0           tf_op_layer_clip_by_value_1/Minim\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sqrt (TensorFlowOpL (None, 1)            0           tf_op_layer_clip_by_value[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Sqrt_1 (TensorFlowO (None, 1)            0           tf_op_layer_clip_by_value_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "stacked_dists (Lambda)          (None, 2, 1)         0           tf_op_layer_Sqrt[0][0]           \n",
            "                                                                 tf_op_layer_Sqrt_1[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 21,713,792\n",
            "Trainable params: 21,696,768\n",
            "Non-trainable params: 17,024\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC9hId9cpXk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5698ae11-6f12-422a-f5ea-d20742a6eb3b"
      },
      "source": [
        "dir(siam_model)\n",
        "print(siam_model.inputs)\n",
        "print(siam_model.layers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'anchor_input_2:0' shape=(None, 121, 145, 1) dtype=float32>, <tf.Tensor 'positive_input_2:0' shape=(None, 121, 145, 1) dtype=float32>, <tf.Tensor 'negative_input_2:0' shape=(None, 121, 145, 1) dtype=float32>]\n",
            "[<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f18d4e77be0>, <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f18d4e77cf8>, <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f18d4e77eb8>, <tensorflow.python.keras.engine.functional.Functional object at 0x7f18d4dc12b0>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d49f6e80>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d49fe748>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d49fe630>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d49fe710>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d49fea20>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d49fe550>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d49fe588>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d49fe5c0>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d49fe828>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d49fecf8>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d4a274e0>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d4a27358>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d4a27518>, <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f18d4a27390>, <tensorflow.python.keras.layers.core.Lambda object at 0x7f18d4a27b38>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFyRvDtAo3Pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cf={'Pretrained_Model':{'path':'/content/drive/My Drive/BA_Estimation/models/exp4/age_net_oasis1_3.hdf5'},'Paths':\\\n",
        "      {'labels':'/content/drive/My Drive/BA_Estimation/csv_data/oasis1_oasis3_labels.csv',\\\n",
        "       'test_tfrecord':'/content/drive/My Drive/BA_Estimation/tf_records_data/testing_all_cdr'}}\n",
        "label_path= cf['Paths']['labels'] #'/content/drive/My Drive/BA_Estimation/csv_data/oasis1_oasis3_labels.csv'#/media/shashanks/My Passport/documents/Master_Thesis_Backup/data/\n",
        "data_path= cf['Paths']['test_tfrecord']\n",
        "debug_mode_subject = ['OAS31054']\n",
        "test_patients,scan_ids, test_labels,test_gender,test_cdr = get_test_files(label_path,data_path,debug_mode_subject)\n",
        "tfr=tf.data.TFRecordDataset(test_patients)\n",
        "img_tf=tfr.map(map_func=lambda a:parse_function_image(a))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKWzDMBTJngH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda, Layer, Add, Multiply, add, Activation\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "latent_dim = 128\n",
        "  \n",
        "  \n",
        "def conv2d_bn(x,\n",
        "              filters,\n",
        "              strides,\n",
        "              padding='same'\n",
        "              ):\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters, (3, 3),\n",
        "        strides=strides,\n",
        "        padding=padding, kernel_initializer='he_normal', kernel_regularizer=l2(5e-4),\n",
        "        use_bias=False)(x)\n",
        "\n",
        "    bn_axis = -1\n",
        "    x = tf.keras.layers.BatchNormalization(axis=bn_axis, scale=False)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "    \n",
        "\n",
        "def conv2d_bn_1x1(x,\n",
        "              filters,\n",
        "              strides,\n",
        "              padding='same'\n",
        "              ):\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters, (1, 1),\n",
        "        strides=strides,\n",
        "        padding=padding, kernel_initializer='he_normal', kernel_regularizer=l2(5e-4),\n",
        "        use_bias=False)(x)\n",
        "\n",
        "    bn_axis = -1\n",
        "    x = tf.keras.layers.BatchNormalization(axis=bn_axis, scale=False)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def conv2d_bn_7x7(x,\n",
        "              filters,\n",
        "              strides=(2, 2),\n",
        "              padding='same'\n",
        "              ):\n",
        "\t\t\t  \n",
        "    x = tf.keras.layers.Conv2D(filters, (7, 7),\n",
        "        strides=strides,\n",
        "        padding=padding, kernel_initializer='he_normal', kernel_regularizer=l2(5e-4),\n",
        "        use_bias=False)(x)\n",
        "\n",
        "    bn_axis = -1\n",
        "    x = tf.keras.layers.BatchNormalization(axis=bn_axis, scale=False)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\t\n",
        "\t\n",
        "def residual_module1(layer_in, n_filters):\n",
        "    # conv1\n",
        "    x = conv2d_bn(layer_in, n_filters, strides=(1, 1), padding='same')\n",
        "\n",
        "    # conv2\n",
        "    conv2 = conv2d_bn(x, n_filters, strides=(1, 1), padding='same')\n",
        "\n",
        "    # add filters, assumes filters/channels last\n",
        "    layer_out = add([conv2, layer_in])\n",
        "    # activation function\n",
        "    layer_out = Activation('relu')(layer_out)\n",
        "\n",
        "    return layer_out\n",
        "\n",
        "\n",
        "def residual_module2(layer_in, n_filters):\n",
        "    # conv1\n",
        "    x = conv2d_bn(layer_in, n_filters, strides=(2, 2), padding='same')\n",
        "    # conv2\n",
        "    conv2 = conv2d_bn(x, n_filters, strides=(1, 1), padding='same')\n",
        "    \n",
        "    #projection shortcut for mismatch in number of channels\n",
        "    y = conv2d_bn_1x1(layer_in, n_filters, strides=(2, 2), padding='same')\n",
        "\n",
        "    # add filters, assumes filters/channels last\n",
        "    layer_out = add([conv2, y])\n",
        "    # activation function\n",
        "    layer_out = Activation('relu')(layer_out)\n",
        "\n",
        "    return layer_out\n",
        "\t\n",
        "\t\n",
        "def res34(inp):\n",
        "    channel_axis = -1\n",
        "    #Instantiates the Inflated 3D Inception v1 architecture.\n",
        "\n",
        "    # Downsampling via convolution (spatial and temporal)\n",
        "    x = conv2d_bn_7x7(inp, 64, strides=(2, 2), padding='same')\n",
        "    x = tf.keras.layers.MaxPool2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\t\n",
        "    x = residual_module1(x,64)\n",
        "    x = residual_module1(x,64)\n",
        "    x = residual_module1(x,64)\n",
        "\n",
        "    x = residual_module2(x,128)\n",
        "    x = residual_module1(x,128)\n",
        "    x = residual_module1(x,128)\n",
        "    x = residual_module1(x,128)\n",
        "\n",
        "    x = residual_module2(x,256)\n",
        "    x = residual_module1(x,256)\n",
        "    x = residual_module1(x,256)\n",
        "    x = residual_module1(x,256)\n",
        "    x = residual_module1(x,256)\n",
        "    x = residual_module1(x,256)\n",
        "\n",
        "    x = residual_module2(x,512)\n",
        "    x = residual_module1(x,512)\n",
        "    x = residual_module1(x,512)\n",
        "\t\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D(name = 'gap')(x)\n",
        "\t\n",
        "    # FCN with Relu activation, kernel_regularizer=l2(1e-3)\n",
        "    x = tf.keras.layers.Dense(units=512, kernel_regularizer=l2(1e-3), name='dense1')(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "    # FCN with Relu activation, kernel_regularizer=l2(1e-3)\n",
        "    x = tf.keras.layers.Dense(units=256, kernel_regularizer=l2(1e-3), name='dense2')(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "    z = tf.keras.layers.Dense(units = latent_dim)(x)\n",
        "    z = tf.keras.layers.Activation('relu')(z)\n",
        "    z = tf.keras.layers.Lambda(lambda x: K.l2_normalize(x, axis=1))(z)\n",
        "\n",
        "    return z\n",
        "\t\n",
        "def model_create():\n",
        "    # defining input shape\n",
        "    input = tf.keras.Input(shape=(121, 145, 1))\n",
        "\n",
        "    # base for encoder\n",
        "    z = res34(input)\n",
        "\n",
        "    # defining model for encoder\n",
        "    encoder = Model(inputs = input, outputs = z, name='encoder')\n",
        "    print(encoder.summary())\n",
        "\n",
        "    return encoder"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmuxC-ZMuIGu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcc6cfdf-add4-4ee4-ac26-006c0ecf777f"
      },
      "source": [
        "img=np.expand_dims(im,0)\n",
        "# img=np.expand_dims(img,0)\n",
        "img = img[:,:,:,82]\n",
        "img=np.expand_dims(img,-1)\n",
        "print(img.shape)\n",
        "predictions=siam_model([img,img,img])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 121, 145, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwOIrK3CvxAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b3f852d-8ae9-40fc-c464-c5b4b951719a"
      },
      "source": [
        "# dir(siam_model)\n",
        "siam_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_TF_MODULE_IGNORED_PROPERTIES',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_activity_regularizer',\n",
              " '_add_trackable',\n",
              " '_add_variable_with_custom_getter',\n",
              " '_assert_compile_was_called',\n",
              " '_assert_weights_created',\n",
              " '_auto_track_sub_layers',\n",
              " '_autocast',\n",
              " '_autographed_call',\n",
              " '_base_model_initialized',\n",
              " '_build_input_shape',\n",
              " '_call_accepts_kwargs',\n",
              " '_call_arg_was_passed',\n",
              " '_call_fn_arg_defaults',\n",
              " '_call_fn_arg_positions',\n",
              " '_call_fn_args',\n",
              " '_call_full_argspec',\n",
              " '_callable_losses',\n",
              " '_cast_single_input',\n",
              " '_check_call_args',\n",
              " '_checkpoint_dependencies',\n",
              " '_clear_losses',\n",
              " '_compile_was_called',\n",
              " '_compiled_trainable_state',\n",
              " '_compute_dtype',\n",
              " '_compute_dtype_object',\n",
              " '_compute_output_and_mask_jointly',\n",
              " '_compute_tensor_usage_count',\n",
              " '_configure_steps_per_execution',\n",
              " '_conform_to_reference_input',\n",
              " '_dedup_weights',\n",
              " '_default_training_arg',\n",
              " '_deferred_dependencies',\n",
              " '_distribution_strategy',\n",
              " '_dtype',\n",
              " '_dtype_defaulted_to_floatx',\n",
              " '_dtype_policy',\n",
              " '_dynamic',\n",
              " '_eager_losses',\n",
              " '_enable_dict_to_input_mapping',\n",
              " '_expects_mask_arg',\n",
              " '_expects_training_arg',\n",
              " '_feed_input_names',\n",
              " '_feed_input_shapes',\n",
              " '_feed_inputs',\n",
              " '_flatten',\n",
              " '_flatten_layers',\n",
              " '_flatten_to_reference_inputs',\n",
              " '_functional_construction_call',\n",
              " '_gather_children_attribute',\n",
              " '_gather_saveables_for_checkpoint',\n",
              " '_get_call_arg_value',\n",
              " '_get_callback_model',\n",
              " '_get_compile_args',\n",
              " '_get_distribution_strategy',\n",
              " '_get_existing_metric',\n",
              " '_get_input_masks',\n",
              " '_get_node_attribute_at_index',\n",
              " '_get_optimizer',\n",
              " '_get_save_spec',\n",
              " '_get_trainable_state',\n",
              " '_graph_network_add_loss',\n",
              " '_graph_network_add_metric',\n",
              " '_handle_activity_regularization',\n",
              " '_handle_deferred_dependencies',\n",
              " '_handle_deferred_layer_dependencies',\n",
              " '_handle_weight_regularization',\n",
              " '_in_multi_worker_mode',\n",
              " '_inbound_nodes',\n",
              " '_infer_output_signature',\n",
              " '_init_batch_counters',\n",
              " '_init_call_fn_args',\n",
              " '_init_graph_network',\n",
              " '_init_set_name',\n",
              " '_initial_weights',\n",
              " '_input_coordinates',\n",
              " '_input_layers',\n",
              " '_input_spec',\n",
              " '_insert_layers',\n",
              " '_is_compiled',\n",
              " '_is_graph_network',\n",
              " '_is_layer',\n",
              " '_keras_api_names',\n",
              " '_keras_api_names_v1',\n",
              " '_keras_tensor_symbolic_call',\n",
              " '_layer_call_argspecs',\n",
              " '_layer_checkpoint_dependencies',\n",
              " '_layers',\n",
              " '_list_extra_dependencies_for_serialization',\n",
              " '_list_functions_for_serialization',\n",
              " '_lookup_dependency',\n",
              " '_losses',\n",
              " '_map_resources',\n",
              " '_maybe_build',\n",
              " '_maybe_cast_inputs',\n",
              " '_maybe_create_attribute',\n",
              " '_maybe_initialize_trackable',\n",
              " '_maybe_load_initial_epoch_from_ckpt',\n",
              " '_metrics',\n",
              " '_metrics_lock',\n",
              " '_must_restore_from_config',\n",
              " '_name',\n",
              " '_name_based_attribute_restore',\n",
              " '_name_based_restores',\n",
              " '_name_scope',\n",
              " '_nested_inputs',\n",
              " '_nested_outputs',\n",
              " '_network_nodes',\n",
              " '_no_dependency',\n",
              " '_nodes_by_depth',\n",
              " '_non_trainable_weights',\n",
              " '_obj_reference_counts',\n",
              " '_obj_reference_counts_dict',\n",
              " '_object_identifier',\n",
              " '_outbound_nodes',\n",
              " '_output_coordinates',\n",
              " '_output_layers',\n",
              " '_output_mask_cache',\n",
              " '_output_shape_cache',\n",
              " '_output_tensor_cache',\n",
              " '_predict_counter',\n",
              " '_preload_simple_restoration',\n",
              " '_reset_compile_cache',\n",
              " '_restore_from_checkpoint_position',\n",
              " '_run_eagerly',\n",
              " '_run_internal_graph',\n",
              " '_saved_model_inputs_spec',\n",
              " '_self_name_based_restores',\n",
              " '_self_saveable_object_factories',\n",
              " '_self_setattr_tracking',\n",
              " '_self_unconditional_checkpoint_dependencies',\n",
              " '_self_unconditional_deferred_dependencies',\n",
              " '_self_unconditional_dependency_names',\n",
              " '_self_update_uid',\n",
              " '_serialized_attributes',\n",
              " '_set_call_arg_value',\n",
              " '_set_connectivity_metadata',\n",
              " '_set_dtype_policy',\n",
              " '_set_inputs',\n",
              " '_set_mask_keras_history_checked',\n",
              " '_set_mask_metadata',\n",
              " '_set_output_names',\n",
              " '_set_save_spec',\n",
              " '_set_trainable_state',\n",
              " '_set_training_mode',\n",
              " '_setattr_tracking',\n",
              " '_should_cast_single_input',\n",
              " '_should_compute_mask',\n",
              " '_should_eval',\n",
              " '_single_restoration_from_checkpoint_position',\n",
              " '_split_out_first_arg',\n",
              " '_stateful',\n",
              " '_steps_per_execution',\n",
              " '_supports_masking',\n",
              " '_symbolic_call',\n",
              " '_tensor_usage_count',\n",
              " '_test_counter',\n",
              " '_tf_api_names',\n",
              " '_tf_api_names_v1',\n",
              " '_thread_local',\n",
              " '_track_trackable',\n",
              " '_trackable_saved_model_saver',\n",
              " '_trackable_saver',\n",
              " '_tracking_metadata',\n",
              " '_train_counter',\n",
              " '_trainable',\n",
              " '_trainable_weights',\n",
              " '_training_state',\n",
              " '_unconditional_checkpoint_dependencies',\n",
              " '_unconditional_dependency_names',\n",
              " '_undeduplicated_weights',\n",
              " '_update_uid',\n",
              " '_updated_config',\n",
              " '_updates',\n",
              " '_validate_compile',\n",
              " '_validate_graph_inputs_and_outputs',\n",
              " '_warn_about_input_casting',\n",
              " 'activity_regularizer',\n",
              " 'add_loss',\n",
              " 'add_metric',\n",
              " 'add_update',\n",
              " 'add_variable',\n",
              " 'add_weight',\n",
              " 'apply',\n",
              " 'build',\n",
              " 'built',\n",
              " 'call',\n",
              " 'compile',\n",
              " 'compiled_loss',\n",
              " 'compiled_metrics',\n",
              " 'compute_mask',\n",
              " 'compute_output_shape',\n",
              " 'compute_output_signature',\n",
              " 'count_params',\n",
              " 'distribute_strategy',\n",
              " 'dtype',\n",
              " 'dynamic',\n",
              " 'evaluate',\n",
              " 'evaluate_generator',\n",
              " 'fit',\n",
              " 'fit_generator',\n",
              " 'from_config',\n",
              " 'get_config',\n",
              " 'get_input_at',\n",
              " 'get_input_mask_at',\n",
              " 'get_input_shape_at',\n",
              " 'get_layer',\n",
              " 'get_losses_for',\n",
              " 'get_output_at',\n",
              " 'get_output_mask_at',\n",
              " 'get_output_shape_at',\n",
              " 'get_updates_for',\n",
              " 'get_weights',\n",
              " 'graph_debug_info',\n",
              " 'history',\n",
              " 'inbound_nodes',\n",
              " 'input',\n",
              " 'input_mask',\n",
              " 'input_names',\n",
              " 'input_shape',\n",
              " 'input_spec',\n",
              " 'inputs',\n",
              " 'layers',\n",
              " 'load_weights',\n",
              " 'losses',\n",
              " 'make_predict_function',\n",
              " 'make_test_function',\n",
              " 'make_train_function',\n",
              " 'metrics',\n",
              " 'metrics_names',\n",
              " 'name',\n",
              " 'name_scope',\n",
              " 'non_trainable_variables',\n",
              " 'non_trainable_weights',\n",
              " 'optimizer',\n",
              " 'outbound_nodes',\n",
              " 'output',\n",
              " 'output_mask',\n",
              " 'output_names',\n",
              " 'output_shape',\n",
              " 'outputs',\n",
              " 'predict',\n",
              " 'predict_function',\n",
              " 'predict_generator',\n",
              " 'predict_on_batch',\n",
              " 'predict_step',\n",
              " 'reset_metrics',\n",
              " 'reset_states',\n",
              " 'run_eagerly',\n",
              " 'save',\n",
              " 'save_weights',\n",
              " 'set_weights',\n",
              " 'signatures',\n",
              " 'state_updates',\n",
              " 'stateful',\n",
              " 'stop_training',\n",
              " 'submodules',\n",
              " 'summary',\n",
              " 'supports_masking',\n",
              " 'tensorflow_git_version',\n",
              " 'tensorflow_version',\n",
              " 'test_function',\n",
              " 'test_on_batch',\n",
              " 'test_step',\n",
              " 'to_json',\n",
              " 'to_yaml',\n",
              " 'train_function',\n",
              " 'train_on_batch',\n",
              " 'train_step',\n",
              " 'trainable',\n",
              " 'trainable_variables',\n",
              " 'trainable_weights',\n",
              " 'updates',\n",
              " 'variables',\n",
              " 'weights',\n",
              " 'with_name_scope']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDd9NXpJstWa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8f751c0-1b83-4d3f-bfb4-2f7a91878778"
      },
      "source": [
        "dir(predictions)\n",
        "encoder=model_create()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 121, 145, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 61, 73, 64)   3136        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 61, 73, 64)   192         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 61, 73, 64)   0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 31, 37, 64)   0           activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 31, 37, 64)   36864       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 31, 37, 64)   192         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 31, 37, 64)   0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 31, 37, 64)   36864       activation_111[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 31, 37, 64)   192         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 31, 37, 64)   0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 31, 37, 64)   0           activation_112[0][0]             \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 31, 37, 64)   0           add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 31, 37, 64)   36864       activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 31, 37, 64)   192         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 31, 37, 64)   0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 31, 37, 64)   36864       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 31, 37, 64)   192         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 31, 37, 64)   0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_33 (Add)                    (None, 31, 37, 64)   0           activation_115[0][0]             \n",
            "                                                                 activation_113[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 31, 37, 64)   0           add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 31, 37, 64)   36864       activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 31, 37, 64)   192         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 31, 37, 64)   0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 31, 37, 64)   36864       activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 31, 37, 64)   192         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 31, 37, 64)   0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_34 (Add)                    (None, 31, 37, 64)   0           activation_118[0][0]             \n",
            "                                                                 activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 31, 37, 64)   0           add_34[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 16, 19, 128)  73728       activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 16, 19, 128)  384         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 16, 19, 128)  0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 16, 19, 128)  147456      activation_120[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 16, 19, 128)  8192        activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 16, 19, 128)  384         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 16, 19, 128)  384         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 16, 19, 128)  0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 16, 19, 128)  0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_35 (Add)                    (None, 16, 19, 128)  0           activation_121[0][0]             \n",
            "                                                                 activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 16, 19, 128)  0           add_35[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 16, 19, 128)  147456      activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 16, 19, 128)  384         conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 16, 19, 128)  0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 16, 19, 128)  147456      activation_124[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 16, 19, 128)  384         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 16, 19, 128)  0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_36 (Add)                    (None, 16, 19, 128)  0           activation_125[0][0]             \n",
            "                                                                 activation_123[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 16, 19, 128)  0           add_36[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 16, 19, 128)  147456      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 16, 19, 128)  384         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 16, 19, 128)  0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 16, 19, 128)  147456      activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 16, 19, 128)  384         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 16, 19, 128)  0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 16, 19, 128)  0           activation_128[0][0]             \n",
            "                                                                 activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 16, 19, 128)  0           add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 16, 19, 128)  147456      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 16, 19, 128)  384         conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 16, 19, 128)  0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 16, 19, 128)  147456      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 16, 19, 128)  384         conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 16, 19, 128)  0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 16, 19, 128)  0           activation_131[0][0]             \n",
            "                                                                 activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 16, 19, 128)  0           add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 10, 256)   294912      activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 8, 10, 256)   768         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 8, 10, 256)   0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 10, 256)   589824      activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 10, 256)   32768       activation_132[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 8, 10, 256)   768         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 8, 10, 256)   768         conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 8, 10, 256)   0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 8, 10, 256)   0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 8, 10, 256)   0           activation_134[0][0]             \n",
            "                                                                 activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 8, 10, 256)   0           add_39[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 10, 256)   589824      activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 8, 10, 256)   768         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 8, 10, 256)   0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 10, 256)   589824      activation_137[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 8, 10, 256)   768         conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 8, 10, 256)   0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 8, 10, 256)   0           activation_138[0][0]             \n",
            "                                                                 activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 8, 10, 256)   0           add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 8, 10, 256)   589824      activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 8, 10, 256)   768         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 8, 10, 256)   0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 8, 10, 256)   589824      activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 8, 10, 256)   768         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 8, 10, 256)   0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 8, 10, 256)   0           activation_141[0][0]             \n",
            "                                                                 activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 8, 10, 256)   0           add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 8, 10, 256)   589824      activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 8, 10, 256)   768         conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 8, 10, 256)   0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 8, 10, 256)   589824      activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 8, 10, 256)   768         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 8, 10, 256)   0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 8, 10, 256)   0           activation_144[0][0]             \n",
            "                                                                 activation_142[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 8, 10, 256)   0           add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 8, 10, 256)   589824      activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 8, 10, 256)   768         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 8, 10, 256)   0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 8, 10, 256)   589824      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 8, 10, 256)   768         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 8, 10, 256)   0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 8, 10, 256)   0           activation_147[0][0]             \n",
            "                                                                 activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 8, 10, 256)   0           add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 8, 10, 256)   589824      activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 8, 10, 256)   768         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 8, 10, 256)   0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 8, 10, 256)   589824      activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 8, 10, 256)   768         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 8, 10, 256)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 8, 10, 256)   0           activation_150[0][0]             \n",
            "                                                                 activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 8, 10, 256)   0           add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 4, 5, 512)    1179648     activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 4, 5, 512)    1536        conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 4, 5, 512)    0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 4, 5, 512)    2359296     activation_152[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 4, 5, 512)    131072      activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 4, 5, 512)    1536        conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 4, 5, 512)    1536        conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 4, 5, 512)    0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 4, 5, 512)    0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 4, 5, 512)    0           activation_153[0][0]             \n",
            "                                                                 activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 4, 5, 512)    0           add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 4, 5, 512)    2359296     activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 4, 5, 512)    1536        conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 4, 5, 512)    0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 4, 5, 512)    2359296     activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 4, 5, 512)    1536        conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 4, 5, 512)    0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_46 (Add)                    (None, 4, 5, 512)    0           activation_157[0][0]             \n",
            "                                                                 activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 4, 5, 512)    0           add_46[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 4, 5, 512)    2359296     activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 4, 5, 512)    1536        conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 4, 5, 512)    0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 4, 5, 512)    2359296     activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 4, 5, 512)    1536        conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 4, 5, 512)    0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_47 (Add)                    (None, 4, 5, 512)    0           activation_160[0][0]             \n",
            "                                                                 activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 4, 5, 512)    0           add_47[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "gap (GlobalAveragePooling2D)    (None, 512)          0           activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense1 (Dense)                  (None, 512)          262656      gap[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 512)          0           dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 512)          0           activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense2 (Dense)                  (None, 256)          131328      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 256)          0           dense2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 256)          0           activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          32896       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 128)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 128)          0           activation_164[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 21,713,792\n",
            "Trainable params: 21,696,768\n",
            "Non-trainable params: 17,024\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6dnINkwOz0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb3083b0-707d-4452-d7b3-2985cd6c12db"
      },
      "source": [
        "dir(encoder.layers[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_TF_MODULE_IGNORED_PROPERTIES',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_activity_regularizer',\n",
              " '_add_trackable',\n",
              " '_add_variable_with_custom_getter',\n",
              " '_auto_track_sub_layers',\n",
              " '_autocast',\n",
              " '_autographed_call',\n",
              " '_batch_input_shape',\n",
              " '_build_input_shape',\n",
              " '_call_accepts_kwargs',\n",
              " '_call_arg_was_passed',\n",
              " '_call_fn_arg_defaults',\n",
              " '_call_fn_arg_positions',\n",
              " '_call_fn_args',\n",
              " '_call_full_argspec',\n",
              " '_callable_losses',\n",
              " '_cast_single_input',\n",
              " '_checkpoint_dependencies',\n",
              " '_clear_losses',\n",
              " '_compute_dtype',\n",
              " '_compute_dtype_object',\n",
              " '_dedup_weights',\n",
              " '_default_training_arg',\n",
              " '_deferred_dependencies',\n",
              " '_dtype',\n",
              " '_dtype_defaulted_to_floatx',\n",
              " '_dtype_policy',\n",
              " '_dynamic',\n",
              " '_eager_losses',\n",
              " '_expects_mask_arg',\n",
              " '_expects_training_arg',\n",
              " '_flatten',\n",
              " '_flatten_layers',\n",
              " '_functional_construction_call',\n",
              " '_gather_children_attribute',\n",
              " '_gather_saveables_for_checkpoint',\n",
              " '_get_call_arg_value',\n",
              " '_get_existing_metric',\n",
              " '_get_input_masks',\n",
              " '_get_node_attribute_at_index',\n",
              " '_get_save_spec',\n",
              " '_get_trainable_state',\n",
              " '_handle_activity_regularization',\n",
              " '_handle_deferred_dependencies',\n",
              " '_handle_weight_regularization',\n",
              " '_inbound_nodes',\n",
              " '_infer_output_signature',\n",
              " '_init_call_fn_args',\n",
              " '_init_set_name',\n",
              " '_initial_weights',\n",
              " '_input_spec',\n",
              " '_is_layer',\n",
              " '_keras_api_names',\n",
              " '_keras_api_names_v1',\n",
              " '_keras_tensor_symbolic_call',\n",
              " '_layers',\n",
              " '_list_extra_dependencies_for_serialization',\n",
              " '_list_functions_for_serialization',\n",
              " '_lookup_dependency',\n",
              " '_losses',\n",
              " '_map_resources',\n",
              " '_maybe_build',\n",
              " '_maybe_cast_inputs',\n",
              " '_maybe_create_attribute',\n",
              " '_maybe_initialize_trackable',\n",
              " '_metrics',\n",
              " '_metrics_lock',\n",
              " '_must_restore_from_config',\n",
              " '_name',\n",
              " '_name_based_attribute_restore',\n",
              " '_name_based_restores',\n",
              " '_name_scope',\n",
              " '_no_dependency',\n",
              " '_non_trainable_weights',\n",
              " '_obj_reference_counts',\n",
              " '_obj_reference_counts_dict',\n",
              " '_object_identifier',\n",
              " '_outbound_nodes',\n",
              " '_preload_simple_restoration',\n",
              " '_restore_from_checkpoint_position',\n",
              " '_saved_model_inputs_spec',\n",
              " '_self_setattr_tracking',\n",
              " '_set_call_arg_value',\n",
              " '_set_connectivity_metadata',\n",
              " '_set_dtype_policy',\n",
              " '_set_mask_keras_history_checked',\n",
              " '_set_mask_metadata',\n",
              " '_set_save_spec',\n",
              " '_set_trainable_state',\n",
              " '_set_training_mode',\n",
              " '_setattr_tracking',\n",
              " '_should_cast_single_input',\n",
              " '_single_restoration_from_checkpoint_position',\n",
              " '_split_out_first_arg',\n",
              " '_stateful',\n",
              " '_supports_masking',\n",
              " '_symbolic_call',\n",
              " '_tf_api_names',\n",
              " '_tf_api_names_v1',\n",
              " '_thread_local',\n",
              " '_track_trackable',\n",
              " '_trackable_saved_model_saver',\n",
              " '_tracking_metadata',\n",
              " '_trainable',\n",
              " '_trainable_weights',\n",
              " '_type_spec',\n",
              " '_unconditional_checkpoint_dependencies',\n",
              " '_unconditional_dependency_names',\n",
              " '_update_uid',\n",
              " '_updates',\n",
              " '_warn_about_input_casting',\n",
              " 'activity_regularizer',\n",
              " 'add_loss',\n",
              " 'add_metric',\n",
              " 'add_update',\n",
              " 'add_variable',\n",
              " 'add_weight',\n",
              " 'apply',\n",
              " 'batch_size',\n",
              " 'build',\n",
              " 'built',\n",
              " 'call',\n",
              " 'compute_mask',\n",
              " 'compute_output_shape',\n",
              " 'compute_output_signature',\n",
              " 'count_params',\n",
              " 'dtype',\n",
              " 'dynamic',\n",
              " 'from_config',\n",
              " 'get_config',\n",
              " 'get_input_at',\n",
              " 'get_input_mask_at',\n",
              " 'get_input_shape_at',\n",
              " 'get_losses_for',\n",
              " 'get_output_at',\n",
              " 'get_output_mask_at',\n",
              " 'get_output_shape_at',\n",
              " 'get_updates_for',\n",
              " 'get_weights',\n",
              " 'inbound_nodes',\n",
              " 'input',\n",
              " 'input_mask',\n",
              " 'input_shape',\n",
              " 'input_spec',\n",
              " 'is_placeholder',\n",
              " 'losses',\n",
              " 'metrics',\n",
              " 'name',\n",
              " 'name_scope',\n",
              " 'non_trainable_variables',\n",
              " 'non_trainable_weights',\n",
              " 'outbound_nodes',\n",
              " 'output',\n",
              " 'output_mask',\n",
              " 'output_shape',\n",
              " 'ragged',\n",
              " 'set_weights',\n",
              " 'sparse',\n",
              " 'stateful',\n",
              " 'submodules',\n",
              " 'supports_masking',\n",
              " 'trainable',\n",
              " 'trainable_variables',\n",
              " 'trainable_weights',\n",
              " 'updates',\n",
              " 'variables',\n",
              " 'weights',\n",
              " 'with_name_scope']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3hLRlSbUMmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,l in enumerate(encoder.layers):\n",
        "  print(i,l.name,encoder_weights[i].name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REX4yLooKfuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(siam_model.get_layer(index=3).weights)\n",
        "def assign_encoder_weights(siam_model,encoder):\n",
        "  encoder_weights= siam_model.get_layer(index=3).weights\n",
        "  j=0\n",
        "  for i,l in enumerate(encoder.layers[1:-2]):\n",
        "    # print(l)\n",
        "    # if i > 0 and i< 150:\n",
        "    if 'activation' in l.name or 'pool' in l.name or 'add' in l.name or 'gap' in l.name or 'dropout' in l.name:\n",
        "      # j+=1\n",
        "      continue\n",
        "    \n",
        "    elif 'batch' in l.name:\n",
        "      print(f'I,J={i,j}')\n",
        "      print(encoder.layers[i+1].name,encoder_weights[j].name)\n",
        "\n",
        "      beta=encoder_weights[j].numpy()\n",
        "      mean=encoder_weights[j+1].numpy()\n",
        "      var=encoder_weights[j+2].numpy()\n",
        "\n",
        "      encoder.layers[i+1].set_weights([beta,mean,var])\n",
        "      j+=3\n",
        "      # encoder.layers[i].set_weights(encoder_weights[i-1].numpy())\n",
        "    elif 'dense' in l.name:\n",
        "      print(f'I,J={i,j}')\n",
        "      print(encoder.layers[i+1].name,encoder_weights[j].name)\n",
        "      w=encoder_weights[j].numpy()\n",
        "      b=encoder_weights[j+1].numpy()\n",
        "      encoder.layers[i+1].set_weights([w,b])\n",
        "      j+=2\n",
        "    else:\n",
        "      print(f'I,J={i,j}')\n",
        "      print(encoder.layers[i+1].name,encoder_weights[j].name)\n",
        "      # print(encoder_weights[i-j].name,encoder_weights[i-j])\n",
        "      encoder.layers[i+1].set_weights([encoder_weights[j].numpy()])\n",
        "      j+=1\n",
        "\n",
        "  return encoder\n",
        "  # print(l.weights)\n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgZ8fKDmEE2e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "ae334441-73fd-44e6-9558-c0ae4d727447"
      },
      "source": [
        "encoder(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
              "array([[0.18906678, 0.00930911, 0.        , 0.06218396, 0.        ,\n",
              "        0.11539859, 0.11956043, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.04007291, 0.17171042, 0.        ,\n",
              "        0.20768355, 0.        , 0.        , 0.        , 0.05784858,\n",
              "        0.33397382, 0.        , 0.        , 0.16350539, 0.        ,\n",
              "        0.18881287, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.02250721, 0.        , 0.05153485, 0.00054224, 0.02433058,\n",
              "        0.        , 0.22382772, 0.        , 0.10851827, 0.        ,\n",
              "        0.        , 0.06670155, 0.        , 0.        , 0.        ,\n",
              "        0.17206584, 0.        , 0.2771803 , 0.        , 0.        ,\n",
              "        0.        , 0.00431712, 0.        , 0.        , 0.02155943,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.18763652,\n",
              "        0.01777927, 0.04555228, 0.        , 0.        , 0.04520997,\n",
              "        0.        , 0.11980598, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.02654593, 0.00886873, 0.06314278, 0.04817495,\n",
              "        0.        , 0.        , 0.06573731, 0.00701502, 0.07342012,\n",
              "        0.        , 0.        , 0.13916135, 0.        , 0.04293318,\n",
              "        0.08051125, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.02196453, 0.        , 0.06796326, 0.34208447, 0.18900344,\n",
              "        0.04993038, 0.00163369, 0.03906316, 0.00663583, 0.        ,\n",
              "        0.        , 0.        , 0.15829253, 0.14286204, 0.        ,\n",
              "        0.00431164, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.11541646, 0.03927426, 0.22944196, 0.10050518,\n",
              "        0.16054523, 0.        , 0.        , 0.14338228, 0.01336302,\n",
              "        0.24143489, 0.        , 0.11920886, 0.04051847, 0.        ,\n",
              "        0.        , 0.        , 0.        ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr0RktnxbKJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "outputId": "fbd8ef90-467d-4578-8eb8-61f0ae835c16"
      },
      "source": [
        "encoder_weights[i-j],i-j,i,j,encoder_weights[5],encoder_weights[i-j].name#j+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Variable 'batch_normalization_1/beta:0' shape=(64,) dtype=float32, numpy=\n",
              " array([ 2.6301818e-04, -1.5682480e-03, -1.1785930e-03,  4.8760889e-04,\n",
              "         9.0179441e-04,  5.0183845e-04, -1.7869343e-04, -5.2055065e-04,\n",
              "        -3.9270293e-04,  9.8180189e-04, -7.9793518e-04,  4.0053850e-04,\n",
              "         4.6665088e-04,  1.4396056e-03, -1.1357679e-03, -5.9476743e-06,\n",
              "         3.4405963e-04,  6.0348772e-04, -1.5745842e-03,  1.6629930e-03,\n",
              "         1.1003368e-03, -3.6193393e-04, -1.1501609e-04,  1.2516575e-03,\n",
              "         2.2348842e-04,  1.4472057e-03,  1.4095052e-04, -2.3056769e-04,\n",
              "         7.7227276e-04, -3.3670143e-04,  1.7754354e-03,  7.2171923e-04,\n",
              "        -2.6932708e-03, -6.6069028e-05, -6.2659767e-04,  2.8872519e-04,\n",
              "         3.3457874e-04,  6.9299433e-04, -5.7828677e-04,  8.0979214e-04,\n",
              "        -1.7628502e-03, -6.7123701e-04,  9.7334204e-04, -1.5045362e-04,\n",
              "        -1.4920064e-03,  4.0226965e-05, -1.3800770e-03, -1.1351012e-03,\n",
              "         8.0423849e-04,  7.9688034e-04,  1.1967545e-03, -1.4971538e-03,\n",
              "         3.6537799e-04,  6.3189676e-05,  9.5391285e-04,  4.8308691e-04,\n",
              "         1.2049419e-03,  4.7064244e-04,  2.5244965e-04,  2.5075561e-04,\n",
              "         4.2941494e-04, -1.4026554e-03,  3.1565884e-05, -1.6170867e-03],\n",
              "       dtype=float32)>,\n",
              " 5,\n",
              " 5,\n",
              " 0,\n",
              " <tf.Variable 'batch_normalization_1/beta:0' shape=(64,) dtype=float32, numpy=\n",
              " array([ 2.6301818e-04, -1.5682480e-03, -1.1785930e-03,  4.8760889e-04,\n",
              "         9.0179441e-04,  5.0183845e-04, -1.7869343e-04, -5.2055065e-04,\n",
              "        -3.9270293e-04,  9.8180189e-04, -7.9793518e-04,  4.0053850e-04,\n",
              "         4.6665088e-04,  1.4396056e-03, -1.1357679e-03, -5.9476743e-06,\n",
              "         3.4405963e-04,  6.0348772e-04, -1.5745842e-03,  1.6629930e-03,\n",
              "         1.1003368e-03, -3.6193393e-04, -1.1501609e-04,  1.2516575e-03,\n",
              "         2.2348842e-04,  1.4472057e-03,  1.4095052e-04, -2.3056769e-04,\n",
              "         7.7227276e-04, -3.3670143e-04,  1.7754354e-03,  7.2171923e-04,\n",
              "        -2.6932708e-03, -6.6069028e-05, -6.2659767e-04,  2.8872519e-04,\n",
              "         3.3457874e-04,  6.9299433e-04, -5.7828677e-04,  8.0979214e-04,\n",
              "        -1.7628502e-03, -6.7123701e-04,  9.7334204e-04, -1.5045362e-04,\n",
              "        -1.4920064e-03,  4.0226965e-05, -1.3800770e-03, -1.1351012e-03,\n",
              "         8.0423849e-04,  7.9688034e-04,  1.1967545e-03, -1.4971538e-03,\n",
              "         3.6537799e-04,  6.3189676e-05,  9.5391285e-04,  4.8308691e-04,\n",
              "         1.2049419e-03,  4.7064244e-04,  2.5244965e-04,  2.5075561e-04,\n",
              "         4.2941494e-04, -1.4026554e-03,  3.1565884e-05, -1.6170867e-03],\n",
              "       dtype=float32)>,\n",
              " 'batch_normalization_1/beta:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93LmRI65anGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i,e in enumerate(encoder_weights):\n",
        "#   print(i,e)\n",
        "#   if i ==10:\n",
        "#     break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giuELWyLPvQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bde356a4-db33-41ed-88b5-4a63dbc18d6d"
      },
      "source": [
        "encoder_weights[0].numpy().shape,encoder.layers[1].weights[0].numpy().shape,i,encoder.layers[2].weights[0].shape,encoder_weights[1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7, 7, 1, 64), (7, 7, 1, 64), 2, TensorShape([64]), TensorShape([64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v49gHi39LVVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(siam_model.get_layer(index=3).weights)\n",
        "# encoder(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBcnJ1ZFJ50s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder.load_weights(siam_model.get_layer(index=3).weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xNYZgZN3CH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model =  tf.python.keras.models.load_model(cf['Pretrained_Model']['path'],compile=False)\n",
        "# test_tf_model=tf.keras.models.Model(model.inputs,model.get_layer(index=5).output)\n",
        "dir(siam_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RHun8FgBoaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir(siam_model.get_layer(index=5))#.get_layer(index=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbK3V9LUrt4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53480d3f-4e4f-44bf-c5d5-97e359fc1f0b"
      },
      "source": [
        "siam_model.get_layer(index=5)._layers #print([var.name for var in layer.trainable_variables])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[DictWrapper({})]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQsTs0qcIhcP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39bd023d-60ac-42c2-c211-893ff51a6555"
      },
      "source": [
        "(siam_model.layers[3].outputs)\n",
        "# .weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'lambda/l2_normalize_1:0' shape=(None, 128) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6E2ni-grYmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "b2f1e89c-3244-4524-a038-3bf2a6497d8d"
      },
      "source": [
        "layers=[layer.name for layer in encoder.layers]\n",
        "outputs=[]\n",
        "j=0\n",
        "for i,l in enumerate(layers):\n",
        "  if 'activation' in l:\n",
        "    val=encoder.get_layer(name=l).output\n",
        "    print(i,j,l,val.shape)\n",
        "    j+=1\n",
        "    outputs.append(val) \n",
        "outputs.append(encoder.output) \n",
        "test_tf_model=tf.keras.models.Model(inputs=encoder.inputs,outputs= outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 0 activation_110 (None, 61, 73, 64)\n",
            "7 1 activation_111 (None, 31, 37, 64)\n",
            "10 2 activation_112 (None, 31, 37, 64)\n",
            "12 3 activation_113 (None, 31, 37, 64)\n",
            "15 4 activation_114 (None, 31, 37, 64)\n",
            "18 5 activation_115 (None, 31, 37, 64)\n",
            "20 6 activation_116 (None, 31, 37, 64)\n",
            "23 7 activation_117 (None, 31, 37, 64)\n",
            "26 8 activation_118 (None, 31, 37, 64)\n",
            "28 9 activation_119 (None, 31, 37, 64)\n",
            "31 10 activation_120 (None, 16, 19, 128)\n",
            "36 11 activation_121 (None, 16, 19, 128)\n",
            "37 12 activation_122 (None, 16, 19, 128)\n",
            "39 13 activation_123 (None, 16, 19, 128)\n",
            "42 14 activation_124 (None, 16, 19, 128)\n",
            "45 15 activation_125 (None, 16, 19, 128)\n",
            "47 16 activation_126 (None, 16, 19, 128)\n",
            "50 17 activation_127 (None, 16, 19, 128)\n",
            "53 18 activation_128 (None, 16, 19, 128)\n",
            "55 19 activation_129 (None, 16, 19, 128)\n",
            "58 20 activation_130 (None, 16, 19, 128)\n",
            "61 21 activation_131 (None, 16, 19, 128)\n",
            "63 22 activation_132 (None, 16, 19, 128)\n",
            "66 23 activation_133 (None, 8, 10, 256)\n",
            "71 24 activation_134 (None, 8, 10, 256)\n",
            "72 25 activation_135 (None, 8, 10, 256)\n",
            "74 26 activation_136 (None, 8, 10, 256)\n",
            "77 27 activation_137 (None, 8, 10, 256)\n",
            "80 28 activation_138 (None, 8, 10, 256)\n",
            "82 29 activation_139 (None, 8, 10, 256)\n",
            "85 30 activation_140 (None, 8, 10, 256)\n",
            "88 31 activation_141 (None, 8, 10, 256)\n",
            "90 32 activation_142 (None, 8, 10, 256)\n",
            "93 33 activation_143 (None, 8, 10, 256)\n",
            "96 34 activation_144 (None, 8, 10, 256)\n",
            "98 35 activation_145 (None, 8, 10, 256)\n",
            "101 36 activation_146 (None, 8, 10, 256)\n",
            "104 37 activation_147 (None, 8, 10, 256)\n",
            "106 38 activation_148 (None, 8, 10, 256)\n",
            "109 39 activation_149 (None, 8, 10, 256)\n",
            "112 40 activation_150 (None, 8, 10, 256)\n",
            "114 41 activation_151 (None, 8, 10, 256)\n",
            "117 42 activation_152 (None, 4, 5, 512)\n",
            "122 43 activation_153 (None, 4, 5, 512)\n",
            "123 44 activation_154 (None, 4, 5, 512)\n",
            "125 45 activation_155 (None, 4, 5, 512)\n",
            "128 46 activation_156 (None, 4, 5, 512)\n",
            "131 47 activation_157 (None, 4, 5, 512)\n",
            "133 48 activation_158 (None, 4, 5, 512)\n",
            "136 49 activation_159 (None, 4, 5, 512)\n",
            "139 50 activation_160 (None, 4, 5, 512)\n",
            "141 51 activation_161 (None, 4, 5, 512)\n",
            "144 52 activation_162 (None, 512)\n",
            "147 53 activation_163 (None, 256)\n",
            "150 54 activation_164 (None, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFWFN6TtInn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions= test_tf_model(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkg6gp8vIxGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20788e0a-3f62-429a-ffa8-41865b4db9c0"
      },
      "source": [
        "len(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRsAe21-Ij8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7a9fae7-dfee-4b92-b053-4ef00270535e"
      },
      "source": [
        "encoder.output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'lambda_2/l2_normalize:0' shape=(None, 128) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7rJNm3fuwKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # predictions=test_tf_model([img,img,img])\n",
        "# # for p in predictions:\n",
        "# #   print(p.shape)\n",
        "# #for triplet loss\n",
        "# import tensorflow.keras.backend as K\n",
        "# positive_input = tf.keras.Input(shape=(121, 145, 1), name=\"positive_input\")\n",
        "# anchor_input = tf.keras.Input(shape=(121, 145, 1), name=\"anchor_input\")\n",
        "# negative_input = tf.keras.Input(shape=(121, 145, 1), name=\"negative_input\")\n",
        "\n",
        "# encoded_p = encoder(positive_input)\n",
        "# encoded_a = encoder(anchor_input)\n",
        "# encoded_n = encoder(negative_input)\n",
        "# pos_distance = K.sqrt(K.maximum(K.sum(K.square(encoded_p - encoded_a), axis=1, keepdims=True), K.epsilon())) #K.sum(K.square(encoded_p - encoded_a), axis=1)\n",
        "# neg_distance = K.sqrt(K.maximum(K.sum(K.square(encoded_a - encoded_n), axis=1, keepdims=True), K.epsilon())) #K.sum(K.square(encoded_a - encoded_n),axis=1)\n",
        "\n",
        "# # This lambda layer simply stacks outputs so both distances are available to the objective\n",
        "# stacked_dists = tf.keras.layers.Lambda(lambda vects: K.stack(vects, axis=1), name='stacked_dists')([pos_distance, neg_distance])\n",
        "\n",
        "# siamese = tf.keras.models.Model(inputs=[positive_input, anchor_input, negative_input], outputs=stacked_dists,\n",
        "#                 name='triplet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tgA9HkB2FNF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfa57ba5-f111-4a6b-da8a-1a6f124df6e7"
      },
      "source": [
        "siam_model.inputs[0]\n",
        "siam_model.get_layer(index=5).output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'tf_op_layer_sub_1/sub_1_2:0' shape=(None, 128) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYRJX3KQv_h7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "73e35554-3716-478e-f2e8-018d601bbae5"
      },
      "source": [
        "for o in outputs:\n",
        "  print(o)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"positive_input_2:0\", shape=(None, 121, 145, 1), dtype=float32)\n",
            "Tensor(\"anchor_input_2:0\", shape=(None, 121, 145, 1), dtype=float32)\n",
            "Tensor(\"negative_input_2:0\", shape=(None, 121, 145, 1), dtype=float32)\n",
            "Tensor(\"lambda/l2_normalize_5:0\", shape=(None, 128), dtype=float32)\n",
            "Tensor(\"tf_op_layer_sub/sub_2:0\", shape=(None, 128), dtype=float32)\n",
            "Tensor(\"tf_op_layer_sub_1/sub_1_2:0\", shape=(None, 128), dtype=float32)\n",
            "Tensor(\"tf_op_layer_Square/Square_2:0\", shape=(None, 128), dtype=float32)\n",
            "Tensor(\"tf_op_layer_Square_1/Square_1_2:0\", shape=(None, 128), dtype=float32)\n",
            "Tensor(\"tf_op_layer_Sum/Sum_2:0\", shape=(None, 1), dtype=float32)\n",
            "Tensor(\"tf_op_layer_Sum_1/Sum_1_2:0\", shape=(None, 1), dtype=float32)\n",
            "Tensor(\"tf_op_layer_Maximum/Maximum_2:0\", shape=(None, 1), dtype=float32)\n",
            "Tensor(\"tf_op_layer_Maximum_1/Maximum_1_2:0\", shape=(None, 1), dtype=float32)\n",
            "Tensor(\"tf_op_layer_clip_by_value/Minimum/clip_by_value/Minimum_2:0\", shape=(None, 1), dtype=float32)\n",
            "Tensor(\"tf_op_layer_clip_by_value_1/Minimum/clip_by_value_1/Minimum_2:0\", shape=(None, 1), dtype=float32)\n",
            "Tensor(\"tf_op_layer_clip_by_value/clip_by_value_2:0\", shape=(None, 1), dtype=float32)\n",
            "Tensor(\"tf_op_layer_clip_by_value_1/clip_by_value_1_2:0\", shape=(None, 1), dtype=float32)\n",
            "Tensor(\"tf_op_layer_Sqrt/Sqrt_2:0\", shape=(None, 1), dtype=float32)\n",
            "Tensor(\"tf_op_layer_Sqrt_1/Sqrt_1_2:0\", shape=(None, 1), dtype=float32)\n",
            "Tensor(\"stacked_dists/stack_2:0\", shape=(None, 2, 1), dtype=float32)\n",
            "Tensor(\"stacked_dists/stack_2:0\", shape=(None, 2, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGLXrg8LpPEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,im in enumerate(img_tf):\n",
        "  im=im.numpy()\n",
        "  # out=test_tf_model([im[:,:,48],im[:,:,48],im[:,:,48]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av6uuuZ2qcfh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51f0d3d0-f837-4403-8c23-b71711e26cf4"
      },
      "source": [
        "im.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(121, 145, 121)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGosD0T1mXgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# siam_model.keras_api#get_layer(index=1)siam_model.layer\n",
        "\n",
        "cf={'Pretrained_Model':{'path':'/content/drive/My Drive/BA_Estimation/models/exp_siam/Model'},'Paths':\\\n",
        "      {'labels':'/content/drive/My Drive/BA_Estimation/csv_data/oasis1_oasis3_labels.csv',\\\n",
        "       'test_tfrecord':'/content/drive/My Drive/BA_Estimation/tf_records_data/testing_exp2'}}#testing_all_cdr\n",
        "siam_model =  tf.python.keras.models.load_model(cf['Pretrained_Model']['path'],compile=False)\n",
        "encoder=model_create()\n",
        "tf_model = assign_encoder_weights(siam_model,encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU7EqIyJhfZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ca0cc10-da86-4122-b9cd-4c11c40dff04"
      },
      "source": [
        "for i,l in enumerate(tf_model.layers):\n",
        "  print(i,l.name)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_2\n",
            "1 conv2d_36\n",
            "2 batch_normalization_36\n",
            "3 activation_55\n",
            "4 max_pooling2d_1\n",
            "5 conv2d_37\n",
            "6 batch_normalization_37\n",
            "7 activation_56\n",
            "8 conv2d_38\n",
            "9 batch_normalization_38\n",
            "10 activation_57\n",
            "11 add_16\n",
            "12 activation_58\n",
            "13 conv2d_39\n",
            "14 batch_normalization_39\n",
            "15 activation_59\n",
            "16 conv2d_40\n",
            "17 batch_normalization_40\n",
            "18 activation_60\n",
            "19 add_17\n",
            "20 activation_61\n",
            "21 conv2d_41\n",
            "22 batch_normalization_41\n",
            "23 activation_62\n",
            "24 conv2d_42\n",
            "25 batch_normalization_42\n",
            "26 activation_63\n",
            "27 add_18\n",
            "28 activation_64\n",
            "29 conv2d_43\n",
            "30 batch_normalization_43\n",
            "31 activation_65\n",
            "32 conv2d_44\n",
            "33 conv2d_45\n",
            "34 batch_normalization_44\n",
            "35 batch_normalization_45\n",
            "36 activation_66\n",
            "37 activation_67\n",
            "38 add_19\n",
            "39 activation_68\n",
            "40 conv2d_46\n",
            "41 batch_normalization_46\n",
            "42 activation_69\n",
            "43 conv2d_47\n",
            "44 batch_normalization_47\n",
            "45 activation_70\n",
            "46 add_20\n",
            "47 activation_71\n",
            "48 conv2d_48\n",
            "49 batch_normalization_48\n",
            "50 activation_72\n",
            "51 conv2d_49\n",
            "52 batch_normalization_49\n",
            "53 activation_73\n",
            "54 add_21\n",
            "55 activation_74\n",
            "56 conv2d_50\n",
            "57 batch_normalization_50\n",
            "58 activation_75\n",
            "59 conv2d_51\n",
            "60 batch_normalization_51\n",
            "61 activation_76\n",
            "62 add_22\n",
            "63 activation_77\n",
            "64 conv2d_52\n",
            "65 batch_normalization_52\n",
            "66 activation_78\n",
            "67 conv2d_53\n",
            "68 conv2d_54\n",
            "69 batch_normalization_53\n",
            "70 batch_normalization_54\n",
            "71 activation_79\n",
            "72 activation_80\n",
            "73 add_23\n",
            "74 activation_81\n",
            "75 conv2d_55\n",
            "76 batch_normalization_55\n",
            "77 activation_82\n",
            "78 conv2d_56\n",
            "79 batch_normalization_56\n",
            "80 activation_83\n",
            "81 add_24\n",
            "82 activation_84\n",
            "83 conv2d_57\n",
            "84 batch_normalization_57\n",
            "85 activation_85\n",
            "86 conv2d_58\n",
            "87 batch_normalization_58\n",
            "88 activation_86\n",
            "89 add_25\n",
            "90 activation_87\n",
            "91 conv2d_59\n",
            "92 batch_normalization_59\n",
            "93 activation_88\n",
            "94 conv2d_60\n",
            "95 batch_normalization_60\n",
            "96 activation_89\n",
            "97 add_26\n",
            "98 activation_90\n",
            "99 conv2d_61\n",
            "100 batch_normalization_61\n",
            "101 activation_91\n",
            "102 conv2d_62\n",
            "103 batch_normalization_62\n",
            "104 activation_92\n",
            "105 add_27\n",
            "106 activation_93\n",
            "107 conv2d_63\n",
            "108 batch_normalization_63\n",
            "109 activation_94\n",
            "110 conv2d_64\n",
            "111 batch_normalization_64\n",
            "112 activation_95\n",
            "113 add_28\n",
            "114 activation_96\n",
            "115 conv2d_65\n",
            "116 batch_normalization_65\n",
            "117 activation_97\n",
            "118 conv2d_66\n",
            "119 conv2d_67\n",
            "120 batch_normalization_66\n",
            "121 batch_normalization_67\n",
            "122 activation_98\n",
            "123 activation_99\n",
            "124 add_29\n",
            "125 activation_100\n",
            "126 conv2d_68\n",
            "127 batch_normalization_68\n",
            "128 activation_101\n",
            "129 conv2d_69\n",
            "130 batch_normalization_69\n",
            "131 activation_102\n",
            "132 add_30\n",
            "133 activation_103\n",
            "134 conv2d_70\n",
            "135 batch_normalization_70\n",
            "136 activation_104\n",
            "137 conv2d_71\n",
            "138 batch_normalization_71\n",
            "139 activation_105\n",
            "140 add_31\n",
            "141 activation_106\n",
            "142 gap\n",
            "143 dense1\n",
            "144 activation_107\n",
            "145 dropout_2\n",
            "146 dense2\n",
            "147 activation_108\n",
            "148 dropout_3\n",
            "149 dense_1\n",
            "150 activation_109\n",
            "151 lambda_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXJ-XQYXq2B-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfa76f86-f620-4449-bb50-756ad3ee436f"
      },
      "source": [
        "import yaml\n",
        "if __name__=='__main__':\n",
        "  print(tf.__version__)\n",
        "  # total_gpus=tf.config.experimental.list_physical_devices('GPU')\n",
        "  # print(f'total_gpus={total_gpus}')\n",
        "  # gpu=total_gpus[2]\n",
        "  # tf.config.experimental.set_visible_devices(gpu,'GPU')\n",
        "  # tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "  # logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "  # print(f'GPUs used = {logical_gpus}')\n",
        "  # parser = argparse.ArgumentParser(description='BioAgeNet training')\n",
        "  # arguments = parser.parse_args()\n",
        "\n",
        "  # arguments.config_path = \"config/config_Age_3D.yml\"\n",
        "  # with open(arguments.config_path, 'r') as ymlfile:\n",
        "  #       cf = yaml.load(ymlfile)\n",
        "  \n",
        "  # if not os.path.exists(base_dir_mat):\n",
        "  #   os.makedirs(base_dir_mat)/model siam_model=tf.saved_model.load('/content/drive/My Drive/BA_Estimation/models/exp_siam/Model')\n",
        "  \n",
        "  # tfw=tf_model.weights\n",
        "\n",
        "  dt_string = datetime.now().strftime('%d-%m-%Y-%H-%M')+'_smoe_maps_blockend_scale_endlayers_equal_weights_exp2'\n",
        "  # dt_string= '14-08-2020-14-50_smoe_maps_blockend_scale_endlayers_equal_weights_exp2.2'\n",
        "  # dt_string = '07-08-2020-08-11'+'_smoe_maps_blockend_scale_endlayers_equal_weights'\n",
        "  # dt_string = '03-08-2020-16-49'#datetime.now().strftime('%d-%m-%Y-%H-%M')\n",
        "\n",
        "  label_path= cf['Paths']['labels'] #'/content/drive/My Drive/BA_Estimation/csv_data/oasis1_oasis3_labels.csv'#/media/shashanks/My Passport/documents/Master_Thesis_Backup/data/\n",
        "  data_path= cf['Paths']['test_tfrecord'] #'/home/raarmak1/.shashanks/BA_estimation/tfrecords_data/OASIS1_3_combined/tfrecords_data_exp3/testing/'\n",
        "  exp='exp2'\n",
        "  # debug_mode_subject= ['OAS31054']#\n",
        "  debug_mode_subject = ['OAS30884','OAS30720','OAS30344','OAS30541'] #OAS30911\n",
        "  # debug_mode_subject=None\n",
        "  test_patients,scan_ids, test_labels,test_gender,test_cdr = get_test_files(label_path,data_path,debug_mode_subject)\n",
        "  tfr=tf.data.TFRecordDataset(test_patients)\n",
        "  img_tf=tfr.map(map_func=lambda a:parse_function_image(a))\n",
        "  # final_model.eval()\n",
        "  # final_model.load_state_dict(inflated_model_dict)\n",
        "  gender_dict={0:'Female',1:'Male'}\n",
        "  for i,im in enumerate(img_tf): #OAS30686_d0030\n",
        "    print(type(im),im.shape)\n",
        "    img=im.numpy()\n",
        "    print(img.shape)\n",
        "    # if  'OAS30344'in scan_ids[i] or 'OAS30884' in scan_ids[i]  or 'OAS30911' in scan_ids[i]:\n",
        "    # if 'OAS30884' in scan_ids[i] :# OAS30720 or 'OAS30884' in scan_ids[i] : #  OAS30911_MR_d0099\n",
        "    max_intensity=0\n",
        "    csmap_list=[]\n",
        "    for chunk_id in [82,85,88,91,94]:#range(1,21):\n",
        "\n",
        "      start = (chunk_id)\n",
        "      end = start+1\n",
        "      \n",
        "      img_chunk=torch.tensor(img[:,:,start]).unsqueeze(0)\n",
        "      img_chunk = img_chunk.unsqueeze(-1)\n",
        "      \n",
        "      print(f'input shape={img_chunk.shape}')\n",
        "# predictions=siam_model([img,img,img])\n",
        "      #*********\n",
        "      # img_chunk = img_chunk.unsqueeze(0)\n",
        "      input_tensor = img_chunk#.unsqueeze(0)\n",
        "      #***********\n",
        "      # imshow(input_tensor[0,0,:,:,-1],cmap='jet')\n",
        "      in_height   = input_tensor.size()[1]\n",
        "      in_width    = input_tensor.size()[2]\n",
        "      print(test_gender[i],input_tensor.shape,scan_ids[i])\n",
        "      # break\n",
        "      a=106\n",
        "      n=71\n",
        "      # layer_name='conv3d_'+str(n-1)\n",
        "      layer_name='activation_'+str(a)\n",
        "      # conv_path ='conv1_'+str(n)+'_tfw'\n",
        "      # conv_path ='conv1_'+str(n)+'_blockend_scale_endlayers_equal_weights'\n",
        "      conv_path ='conv1_'+str(n)\n",
        "      \n",
        "\n",
        "      #axial\n",
        "      base_dir= '/content/drive/My Drive/BA_Estimation/results/sal_map_axial/'+dt_string\\\n",
        "      +'_'+exp+'/'+scan_ids[i]+'_cdr'+str(test_cdr[i])\n",
        "      path = base_dir+'/'+str(chunk_id)+'_'+conv_path+'/'\n",
        "      print(path)\n",
        "      if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "      base_path  = path+scan_ids[i]+'_chunk_'+str(chunk_id)\n",
        "      csmap_a=compute_saliency_tf(base_path,inputs=input_tensor,tf_model=tf_model)\n",
        "      image,gcam_img,gcam_pp_img = compute_gcam_and_gcam_pp(layer_name,tf_model,input_tensor)\n",
        "      combine_sal_gcam(path+scan_ids[i]+'_cdr'+str(test_cdr[i])+'_'+gender_dict[test_gender[i]],csmap_a,gcam_img,gcam_pp_img,image,layer_name=layer_name ) \n",
        "      \n",
        "\n",
        "      #sagittal\n",
        "      base_dir= '/content/drive/My Drive/BA_Estimation/results/sal_map_sagittal/'+dt_string\\\n",
        "      +'_'+exp+'/'+scan_ids[i]+'_cdr'+str(test_cdr[i])\n",
        "      path = base_dir+'/'+str(chunk_id)+'_'+conv_path+'/'\n",
        "      print(path)\n",
        "      if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "      base_path  = path+scan_ids[i]+'_chunk_'+str(chunk_id)\n",
        "      img_s= torch.from_numpy(img[start:end,:,:]).permute(2,1,0)\n",
        "      csmap_s=compute_saliency_tf(base_path,inputs=img_s,tf_model=tf_model)\n",
        "      image,gcam_img,gcam_pp_img = compute_gcam_and_gcam_pp(layer_name,tf_model,img_s)\n",
        "      combine_sal_gcam(path+scan_ids[i]+'_cdr'+str(test_cdr[i])+'_'+gender_dict[test_gender[i]],csmap_s,gcam_img,gcam_pp_img,image,layer_name=layer_name ) \n",
        "\n",
        "      #coronal\n",
        "      base_dir= '/content/drive/My Drive/BA_Estimation/results/sal_map_coronal/'+dt_string\\\n",
        "      +'_'+exp+'/'+scan_ids[i]+'_cdr'+str(test_cdr[i])\n",
        "      path = base_dir+'/'+str(chunk_id)+'_'+conv_path+'/'\n",
        "      print(path)\n",
        "      if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "      base_path  = path+scan_ids[i]+'_chunk_'+str(chunk_id)\n",
        "      img_c= torch.from_numpy(img[:,start:end,:]).permute(2,0,1)\n",
        "      img_c=img_c.unsqueeze(0)\n",
        "      img_c = torch.nn.functional.upsample(img_c.unsqueeze(0), size=(121,145,1), mode='nearest') \n",
        "      csmap_c=compute_saliency_tf(base_path,inputs=img_c,tf_model=tf_model)\n",
        "      image,gcam_img,gcam_pp_img = compute_gcam_and_gcam_pp(layer_name,tf_model,img_c)\n",
        "      combine_sal_gcam(path+scan_ids[i]+'_cdr'+str(test_cdr[i])+'_'+gender_dict[test_gender[i]],csmap_c,gcam_img,gcam_pp_img,image,layer_name=layer_name ) \n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'> (121, 145, 121)\n",
            "(121, 145, 121)\n",
            "input shape=torch.Size([1, 121, 145, 1])\n",
            "1 torch.Size([1, 121, 145, 1]) OAS30344_MR_d0049\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_axial/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30344_MR_d0049_cdr0.0/82_conv1_71/\n",
            "torch.Size([1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.429177, 0.12235768)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(4.431393, 0.66196334)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84985703 0.7527417  0.74610984 ... 0.7268853  0.7614306  0.75633585]\n",
            " [0.8163176  0.72150636 0.67588097 ... 0.6812857  0.7248418  0.77615654]\n",
            " [0.8295602  0.74800146 0.7293751  ... 0.7128768  0.7689446  0.7629746 ]\n",
            " ...\n",
            " [0.80941534 0.73410964 0.7136018  ... 0.71492827 0.7588714  0.7376537 ]\n",
            " [0.83640826 0.7985332  0.7725046  ... 0.78179705 0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.7957859  ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(3.7066448, 0.9709006)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.1996223  1.2430683  1.2017525  1.1974398  1.2533852  1.3119565\n",
            "  1.2702999  1.3423432  1.3687011  1.3364979  1.3725947  1.3507681\n",
            "  1.3410879  1.3055601  1.2360357  1.19928    1.2207917  1.2209934\n",
            "  1.1637677 ]\n",
            " [1.1658028  1.1483084  1.1626205  1.1983719  1.2827231  1.3482554\n",
            "  1.4844764  1.6075921  1.7821163  1.71881    1.6333815  1.5655212\n",
            "  1.6119682  1.304472   1.2004548  1.1413935  1.1057554  1.1470704\n",
            "  1.1467409 ]\n",
            " [1.0510584  1.1004442  1.082449   1.1602912  1.423927   1.5762159\n",
            "  2.0039306  1.9914341  2.0503182  2.1044755  1.8884495  1.8608142\n",
            "  1.7515216  1.6993787  1.4569215  1.2803189  1.0759447  1.0561212\n",
            "  1.1274784 ]\n",
            " [1.1023778  1.0792813  1.1334826  1.1796933  1.4404484  2.0678382\n",
            "  2.3226902  2.5031672  2.716079   2.434865   2.5407827  2.4026148\n",
            "  2.2394712  1.8932563  1.7305417  1.4982322  1.0850177  1.0352262\n",
            "  1.1390694 ]\n",
            " [1.0872594  1.08018    1.2501222  1.4810613  1.8364552  2.1093822\n",
            "  2.2809033  2.622807   2.4131894  2.5850222  2.739963   2.9753366\n",
            "  2.9219024  2.7863894  2.0318136  1.8806003  1.2693702  1.1286695\n",
            "  1.1290826 ]\n",
            " [1.0788296  1.0534939  1.1200218  1.4645723  1.9807924  2.0506117\n",
            "  2.3399596  2.4251325  2.393567   2.3723972  2.8645215  2.8385231\n",
            "  3.0125911  2.702538   2.7015216  2.0242777  1.6132829  1.2188464\n",
            "  1.1287819 ]\n",
            " [1.1165324  1.0296731  1.2449309  1.5859969  2.1028988  2.298946\n",
            "  2.556152   2.937333   2.7010288  2.5803885  3.0714338  3.1510777\n",
            "  3.0427995  3.3261852  3.059554   2.424158   1.8477234  1.3093399\n",
            "  1.1519018 ]\n",
            " [1.0648133  1.0748192  1.3289964  1.8452009  2.1983035  2.4832368\n",
            "  2.9168587  3.0154996  3.0142972  2.9483614  3.164107   3.3861108\n",
            "  3.2147875  3.2923913  3.4504263  2.5746703  1.9486217  1.4033371\n",
            "  1.1738139 ]\n",
            " [1.0420724  1.0085248  1.1991458  1.7935756  2.4413688  2.3355145\n",
            "  2.9145555  3.238075   2.993888   3.2176433  3.0263004  3.3658445\n",
            "  3.2035754  3.4576974  3.0337956  2.493918   1.6970571  1.2511948\n",
            "  1.1675421 ]\n",
            " [1.0911885  1.0379528  1.2351587  1.6428865  2.10464    2.602783\n",
            "  2.7889698  2.7762654  2.9789662  2.9697309  3.305081   3.1246014\n",
            "  3.1971169  3.1968973  2.6742992  2.228187   1.5837859  1.2523326\n",
            "  1.1708618 ]\n",
            " [1.0908779  1.0009419  1.0841836  1.2937536  1.7654219  2.0929465\n",
            "  2.037601   2.2527325  2.6016045  3.134544   3.7066448  3.2829213\n",
            "  3.178124   3.104805   2.572606   1.8295339  1.2444475  1.1102117\n",
            "  1.0997698 ]\n",
            " [1.0514023  1.0391551  1.0448838  1.184511   1.5765086  1.9761704\n",
            "  1.990835   2.020843   2.4367514  3.048202   3.3115535  3.2478147\n",
            "  3.3268585  2.8146486  1.9978005  1.5121853  1.1676166  1.0990251\n",
            "  1.1394114 ]\n",
            " [1.070195   1.0254549  1.0313209  1.1591147  1.3117112  1.5737475\n",
            "  1.8845984  2.1197534  2.2461753  2.3952608  2.8780484  3.0453422\n",
            "  2.4848647  2.0239398  1.7706081  1.3408642  1.061067   1.0540961\n",
            "  1.0939988 ]\n",
            " [1.0516139  1.0862917  1.0533651  1.0939671  1.1296061  1.4260699\n",
            "  1.5221006  1.7169071  1.9184624  1.9706681  2.021325   2.0276754\n",
            "  1.8658086  1.5496242  1.4423732  1.2433053  1.0116919  1.0603553\n",
            "  1.12367   ]\n",
            " [1.0863768  1.1040701  1.0336854  0.98906845 1.0728562  1.1071306\n",
            "  1.2112938  1.2728411  1.404699   1.425201   1.4805045  1.4507717\n",
            "  1.4469411  1.3552693  1.1955032  1.0926372  0.9709007  1.114762\n",
            "  1.1334349 ]\n",
            " [1.0132581  1.0778426  1.077487   1.0329654  1.0136455  1.08412\n",
            "  1.0866853  1.0948778  1.0966121  1.0882727  1.2491596  1.1796707\n",
            "  1.1269678  1.0363878  1.0631115  1.0207785  1.0170316  1.0734895\n",
            "  1.080649  ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(4.4105415, 1.6478765)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.8216909 1.9634911 1.8778554 2.1217823 2.027581  2.0029426 1.9241017\n",
            "  1.9812993 1.8835591 1.6478766]\n",
            " [1.9366431 2.289363  2.4891288 2.5960855 2.5975432 2.7608109 2.5837114\n",
            "  2.3822236 2.1111844 1.7448027]\n",
            " [2.0142128 2.3343925 2.586605  3.042605  3.238761  3.199285  3.3458738\n",
            "  2.6175127 2.3090317 1.7902398]\n",
            " [1.9835752 2.5293174 2.8337169 3.3440056 3.7005994 3.9965127 3.478891\n",
            "  3.2294674 2.5080385 1.9114524]\n",
            " [1.9735521 2.6739955 3.220685  3.9455311 4.4105415 4.285573  4.02297\n",
            "  3.278946  2.3884006 1.8449956]\n",
            " [1.9941518 2.3674297 2.5475783 3.0291145 3.7254198 4.2262335 3.9384496\n",
            "  3.090959  2.2861516 1.8739669]\n",
            " [1.9194838 2.3172479 2.4984422 2.6741471 3.0310018 3.4695048 3.1124341\n",
            "  2.7458131 2.331844  1.84667  ]\n",
            " [1.7399348 1.8383504 1.9782127 2.052077  2.3448243 2.4870448 2.3575184\n",
            "  2.051774  1.8884437 1.672534 ]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(2.2885573, 0.8377139)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.90583575 1.2104074  1.2677027  1.0778826  0.837714  ]\n",
            " [1.0359513  1.6670698  1.9496206  1.616548   0.96662754]\n",
            " [1.0242683  1.695597   2.2885573  1.768588   0.95128816]\n",
            " [0.8651938  1.1754569  1.4321842  1.2336627  0.9078754 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.16630892 0.         0.         0.05255869 0.0037975  0.11971844\n",
            "  0.11513667 0.         0.         0.         0.         0.\n",
            "  0.0296142  0.18647726 0.         0.19699693 0.         0.\n",
            "  0.         0.04780267 0.3324271  0.         0.         0.1778184\n",
            "  0.         0.19890766 0.01949806 0.         0.01564708 0.\n",
            "  0.0542827  0.00738225 0.04172713 0.00072268 0.03551232 0.\n",
            "  0.22229902 0.         0.12678356 0.         0.         0.07590044\n",
            "  0.         0.         0.         0.15961447 0.         0.30330053\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.02355179 0.         0.         0.         0.         0.18314287\n",
            "  0.01994432 0.05550279 0.         0.         0.04549787 0.\n",
            "  0.11926472 0.         0.         0.         0.         0.02274749\n",
            "  0.03075331 0.06116832 0.04494275 0.         0.         0.08094591\n",
            "  0.01448942 0.06464338 0.         0.00463488 0.12769869 0.\n",
            "  0.02364616 0.08577349 0.         0.         0.         0.\n",
            "  0.01761018 0.         0.07481016 0.34488508 0.19113453 0.03253397\n",
            "  0.         0.0390061  0.         0.         0.         0.\n",
            "  0.15319353 0.14730999 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.11706267 0.05213073 0.21010356\n",
            "  0.10132001 0.1448158  0.         0.         0.14359716 0.01347067\n",
            "  0.23174988 0.         0.11965028 0.04600361 0.00596312 0.\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_sagittal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30344_MR_d0049_cdr0.0/82_conv1_71/\n",
            "torch.Size([121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.7852304, 0.07235573)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(6.0476227, 0.6657671)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8508297  0.75664496 0.74884284 ... 0.7268853  0.7614306  0.75633585]\n",
            " [0.81598115 0.7249616  0.67098176 ... 0.6812875  0.7248421  0.77615607]\n",
            " [0.83239144 0.7399235  0.72256196 ... 0.71375144 0.76900107 0.7627036 ]\n",
            " ...\n",
            " [0.8094154  0.73411644 0.71357936 ... 0.7149297  0.75887144 0.73765373]\n",
            " [0.8364083  0.79853344 0.7725078  ... 0.7817972  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.795786   ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(4.892662, 0.97340536)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.2264287 1.3445781 1.8213122 2.1301563 2.5052671 2.4956675 2.5305228\n",
            "  2.5526342 2.5761662 2.6263056 2.6020465 2.3256607 2.0198648 1.9586486\n",
            "  1.6145552 1.3313285 1.2632786 1.2511952 1.197089 ]\n",
            " [1.1548264 1.537731  2.102014  2.570573  3.3046687 3.5157356 3.295384\n",
            "  3.6884294 3.4183478 3.493026  3.2389605 3.110331  2.815168  2.364009\n",
            "  2.0240223 1.5027143 1.2629279 1.2180165 1.1823968]\n",
            " [1.162719  1.5156271 2.2612815 3.1557627 3.5997846 3.6555967 3.7322621\n",
            "  4.1375628 4.1627383 4.639369  4.4056153 4.087579  3.8246832 2.7057295\n",
            "  2.2777143 1.7964077 1.3409361 1.3121753 1.2630514]\n",
            " [1.3662874 1.6954662 2.660038  3.4842663 3.7902324 4.1782365 4.3561974\n",
            "  4.340788  4.568941  4.552689  4.189268  4.574189  3.997849  3.6914623\n",
            "  2.804894  2.4479442 1.9854288 1.6706969 1.4259034]\n",
            " [1.3755968 1.922132  2.740101  3.3634076 3.720945  4.3424096 4.373865\n",
            "  4.748398  4.7455897 4.44402   4.581827  4.239917  4.892662  4.0682354\n",
            "  3.3700595 3.091096  2.393791  2.1504762 1.6112328]\n",
            " [1.4862572 1.9292632 2.793281  3.0079515 3.440281  4.107931  4.069001\n",
            "  4.674241  4.3566146 4.857787  4.101085  4.4841175 4.5460877 4.268818\n",
            "  3.826583  3.3846707 3.2266593 2.3012202 1.7986795]\n",
            " [1.3923389 1.9770666 2.67276   2.844911  2.8002336 2.9714718 3.8613396\n",
            "  3.4784732 4.0116186 4.0292354 3.9249783 3.9135952 4.220205  4.1752977\n",
            "  3.4634333 3.715403  3.2476945 2.4833322 1.6960908]\n",
            " [1.2901719 1.8790112 2.267217  2.9060893 2.5712967 2.2935863 2.5711806\n",
            "  2.6883934 3.2652254 3.1513433 3.4535723 3.4454155 3.3429687 3.2667346\n",
            "  2.959982  3.0743275 2.8530812 2.1778336 1.6392372]\n",
            " [1.1236926 1.5349449 2.074917  2.515626  2.503059  2.211219  2.246977\n",
            "  2.136173  2.3824823 2.7799234 2.847678  3.4019017 3.2099602 2.8688211\n",
            "  2.98272   2.56768   2.2145925 1.7652167 1.4368944]\n",
            " [1.041277  1.2707627 1.555782  2.180952  2.515106  2.2975473 2.2117617\n",
            "  2.3782504 2.3191633 2.8105135 2.8788042 3.0539503 3.1096132 3.0160708\n",
            "  2.6735623 2.3763561 1.6540943 1.384217  1.3027776]\n",
            " [1.0687684 1.2520279 1.4542129 1.8656527 2.3852398 2.2170792 2.0966017\n",
            "  2.1441603 2.7435732 2.9878495 3.5809443 2.9739485 3.2154648 2.8408465\n",
            "  2.507374  1.9571067 1.4807004 1.1745509 1.1431779]\n",
            " [1.0674678 1.0981814 1.2247304 1.3757814 1.6547443 2.012638  1.8793398\n",
            "  1.9865205 2.0557914 2.9673016 2.9978762 3.2225306 3.278237  2.4692543\n",
            "  2.0059094 1.6215392 1.2916546 1.1705003 1.0905885]\n",
            " [1.0543988 1.0846735 1.1749533 1.1589009 1.2688118 1.4240394 1.6666946\n",
            "  1.7126273 2.1330473 2.197939  2.3289828 2.375022  2.351818  1.8998346\n",
            "  1.6646515 1.3275613 1.1671709 1.0294212 1.1160558]\n",
            " [1.0360063 1.0664474 1.0867301 1.0880871 1.1169037 1.2210788 1.3578998\n",
            "  1.4606787 1.5344608 1.7148298 1.8213602 1.8156794 1.8659021 1.4451033\n",
            "  1.466734  1.2560524 1.039187  1.0824947 1.1248704]\n",
            " [1.079898  1.0930635 1.0569826 1.0170047 1.0819565 1.0666071 1.1305395\n",
            "  1.1646941 1.2802666 1.314268  1.3316942 1.2720317 1.3168617 1.1994768\n",
            "  1.172694  1.0763527 0.9734055 1.0820626 1.1204056]\n",
            " [1.01588   1.0715088 1.0740236 1.0406038 1.0510974 1.029934  1.0475641\n",
            "  1.0709647 1.1015266 1.1444389 1.1175009 1.0940822 1.0085363 1.0591141\n",
            "  1.0536937 1.0113255 1.0225135 1.0613445 1.0744382]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(5.6688323, 1.6839666)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[2.1529975 2.7639709 3.4069412 3.5907845 3.7311964 3.5435104 3.1410432\n",
            "  2.7202902 2.1631799 1.8842406]\n",
            " [2.3786345 3.3319368 4.3228245 4.499106  4.805603  5.0136013 4.3218718\n",
            "  3.5838199 2.921218  2.059833 ]\n",
            " [2.4334989 3.6468925 4.351093  5.625828  5.6688323 5.4231167 4.9666166\n",
            "  4.394378  3.2249851 2.2541227]\n",
            " [2.5089588 3.267961  3.7952888 4.5627565 4.8157644 5.0901403 4.9550805\n",
            "  4.269058  3.3732784 2.3549905]\n",
            " [2.2706194 2.7889843 2.9498854 3.7058046 3.9009469 4.4595833 4.2005053\n",
            "  3.8364406 2.8260827 2.091769 ]\n",
            " [2.0440826 2.4744534 2.6661642 3.1261628 3.607825  3.9972954 3.7179937\n",
            "  2.9764652 2.4789064 1.8228698]\n",
            " [1.9022595 2.426818  2.3213263 2.583783  2.867421  2.9975398 3.0156903\n",
            "  2.6165533 2.2113156 1.7800789]\n",
            " [1.7164676 1.9136455 1.8920718 2.1306694 2.247037  2.20197   2.141205\n",
            "  1.9109501 1.8295954 1.6839668]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.964851, 0.8284346)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.88675666 1.2473835  1.3820436  1.25748    0.8688196 ]\n",
            " [1.0659002  1.7109784  1.9648511  1.6360482  1.0896559 ]\n",
            " [0.95249015 1.4241118  1.5867168  1.4441003  0.97692084]\n",
            " [0.8462769  0.9860603  1.1301149  1.1441973  0.8284347 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[2.05177307e-01 7.47503787e-02 5.27110286e-02 6.40076771e-02\n",
            "  0.00000000e+00 2.04431005e-02 9.53289047e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  2.40861848e-02 1.49585217e-01 0.00000000e+00 2.19057739e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.68893680e-02\n",
            "  3.41638952e-01 0.00000000e+00 0.00000000e+00 1.53389871e-01\n",
            "  0.00000000e+00 1.82053953e-01 0.00000000e+00 0.00000000e+00\n",
            "  8.20594374e-03 0.00000000e+00 5.23264445e-02 0.00000000e+00\n",
            "  7.46224523e-02 3.90875451e-02 3.64235193e-02 0.00000000e+00\n",
            "  2.17217714e-01 0.00000000e+00 9.12986174e-02 0.00000000e+00\n",
            "  0.00000000e+00 7.28439689e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.74235851e-01 0.00000000e+00 2.62247771e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.51531154e-02\n",
            "  0.00000000e+00 0.00000000e+00 6.12543151e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.96612403e-01\n",
            "  6.47120327e-02 2.39608102e-02 0.00000000e+00 0.00000000e+00\n",
            "  5.75161912e-02 8.21169000e-03 1.49260223e-01 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.12007990e-01 4.70953770e-02 0.00000000e+00\n",
            "  0.00000000e+00 9.79730710e-02 0.00000000e+00 5.82454689e-02\n",
            "  0.00000000e+00 0.00000000e+00 9.81640145e-02 0.00000000e+00\n",
            "  4.54445928e-02 8.25399458e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.87917426e-02 6.31031860e-03 0.00000000e+00\n",
            "  8.83863866e-02 2.65630096e-01 1.49153024e-01 1.01377860e-01\n",
            "  0.00000000e+00 3.08613200e-02 1.11731775e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.07956991e-01 1.93071261e-01\n",
            "  0.00000000e+00 6.10942915e-02 2.83747911e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.37769222e-01\n",
            "  4.64693718e-02 2.38932148e-01 1.05470672e-01 1.74317107e-01\n",
            "  0.00000000e+00 0.00000000e+00 1.64995328e-01 3.13931727e-04\n",
            "  2.38158569e-01 0.00000000e+00 1.28140494e-01 1.01387784e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_coronal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30344_MR_d0049_cdr0.0/82_conv1_71/\n",
            "torch.Size([1, 1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2941: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x range=(1.8928111, 0.07547406)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(6.419285, 0.6479926)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8511195  0.7516406  0.7456311  ... 0.7272134  0.760854   0.75604767]\n",
            " [0.8177352  0.7164284  0.67512864 ... 0.69237566 0.7322564  0.7766063 ]\n",
            " [0.8354918  0.73481005 0.74341565 ... 0.7236352  0.77797383 0.7688509 ]\n",
            " ...\n",
            " [0.80940896 0.7341906  0.7134329  ... 0.71470296 0.75870657 0.7376535 ]\n",
            " [0.83640826 0.79852164 0.77249837 ... 0.7817853  0.8139446  0.7831061 ]\n",
            " [0.8018056  0.75318444 0.7957862  ... 0.7986541  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(5.3704586, 0.99633527)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.2890853  1.5204114  1.9792411  2.314163   2.5945702  2.7862415\n",
            "  2.4490833  2.1569536  2.0606139  1.9787416  2.0586483  1.9987375\n",
            "  2.3059533  2.1754813  2.0891027  2.1336079  1.8034452  1.5718995\n",
            "  1.2917198 ]\n",
            " [1.3568053  1.7541925  2.4654617  3.4521453  3.790445   3.535771\n",
            "  3.54684    2.9902835  2.650682   2.5937984  2.5826788  2.9144871\n",
            "  3.0142317  3.2053037  3.3222175  2.6230936  2.5788605  1.7777766\n",
            "  1.4133877 ]\n",
            " [1.3843483  2.0363076  3.1621203  4.174075   4.850896   4.7754936\n",
            "  4.708261   4.3841496  3.5297904  3.5848942  3.4549117  3.7938795\n",
            "  4.183591   4.3421717  4.4212713  4.010415   2.811253   2.3141675\n",
            "  1.5378828 ]\n",
            " [1.5402348  2.3097951  3.2241924  4.520773   4.242742   5.276312\n",
            "  5.2814293  5.1830225  4.8910804  3.860552   4.072901   4.491644\n",
            "  4.2642365  4.496318   5.0562315  4.720319   3.775462   2.5594616\n",
            "  1.6271687 ]\n",
            " [1.6713991  2.3250697  3.664393   4.300924   4.020632   4.856361\n",
            "  5.3704586  5.114417   4.839774   4.024343   4.6912074  4.6140485\n",
            "  5.0372305  4.501114   4.121019   4.3538723  3.600585   2.650915\n",
            "  1.584602  ]\n",
            " [1.6257833  2.42172    3.5589237  4.2223735  4.122616   4.729354\n",
            "  4.456916   4.807038   4.188669   3.8153071  4.080052   4.2461967\n",
            "  5.209839   4.4336867  4.130862   3.6939073  2.9498918  2.5041454\n",
            "  1.464023  ]\n",
            " [1.5158236  2.1732268  3.4106305  3.690565   3.988426   4.438507\n",
            "  4.0478477  3.5805354  3.4171073  2.9194965  3.2400873  4.019328\n",
            "  4.5557175  4.6170087  3.8990562  3.299111   2.4869342  2.1168797\n",
            "  1.5115854 ]\n",
            " [1.4483365  1.9638866  2.7535164  2.9045215  3.3309202  3.8683004\n",
            "  3.5421677  3.3006592  2.9052563  2.6592696  3.0183372  3.3831122\n",
            "  4.2212014  3.6970134  3.2440653  2.7964556  2.1824074  1.7898856\n",
            "  1.334868  ]\n",
            " [1.2175967  1.7018713  2.186028   2.4036121  2.5968401  2.8414304\n",
            "  3.1281533  2.9530013  2.9639826  3.173659   2.8235333  3.0612307\n",
            "  3.0030713  3.1795726  2.8782885  2.2982702  2.1139698  1.545252\n",
            "  1.2703208 ]\n",
            " [1.0996715  1.3166374  1.7398906  2.0247555  2.129412   2.4377294\n",
            "  2.6746871  2.8448153  3.2815864  3.2353768  3.3617673  2.9776525\n",
            "  3.1079595  2.755932   2.591426   2.0390084  1.8578702  1.6013695\n",
            "  1.242082  ]\n",
            " [1.0628153  1.1697427  1.5457307  2.302218   2.3755217  2.3774543\n",
            "  2.6337972  3.1388865  3.5352771  3.4122548  3.706306   3.6055517\n",
            "  3.8095574  3.3179166  2.8206859  2.4066057  1.6437783  1.412304\n",
            "  1.243559  ]\n",
            " [1.0756046  1.1268297  1.3103496  1.7053663  2.2916422  2.6381752\n",
            "  2.7600513  3.2030926  3.4709084  3.4306412  3.190889   3.6925235\n",
            "  3.2817323  3.0762315  2.5846505  2.016643   1.49518    1.2846837\n",
            "  1.1227896 ]\n",
            " [1.0593178  1.0443135  1.1637774  1.4659243  1.6621023  2.0190518\n",
            "  2.6968126  2.9412608  3.252495   3.1099463  3.0731382  2.860149\n",
            "  2.6581442  2.321707   1.8995848  1.5899582  1.2626566  1.1637508\n",
            "  1.1014066 ]\n",
            " [1.0331005  1.1003703  1.0994632  1.2035054  1.230161   1.5184076\n",
            "  1.8353621  2.0858803  2.4607038  2.079986   1.9481683  2.0591264\n",
            "  1.6987599  1.746769   1.4484206  1.4179187  1.2187823  1.1586796\n",
            "  1.1517388 ]\n",
            " [1.0628022  1.0879848  0.9963354  1.0705087  1.0951318  1.1915474\n",
            "  1.3982159  1.656448   1.6568476  1.545061   1.5901033  1.589749\n",
            "  1.4042245  1.3033627  1.2361376  1.1231377  1.0145942  1.1404393\n",
            "  1.1419835 ]\n",
            " [1.0177653  1.080786   1.0675943  1.0517479  0.99879956 1.0601159\n",
            "  1.1392529  1.1403307  1.1824012  1.1632643  1.1706518  1.1903502\n",
            "  1.1848658  1.097034   1.040212   1.0156724  1.0308464  1.0537353\n",
            "  1.0678928 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(5.717836, 1.7221279)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[2.5895352 3.4231281 3.6932507 4.030735  3.4695463 3.530033  3.5061483\n",
            "  3.5466275 2.982624  2.1580756]\n",
            " [3.038869  4.3184114 5.186323  5.222167  4.8620534 4.8232813 5.122753\n",
            "  4.8820543 3.7049317 2.8060308]\n",
            " [3.25857   4.593335  5.1822095 5.717836  5.45779   5.406147  5.264177\n",
            "  4.817179  3.96999   2.6567228]\n",
            " [3.1060781 4.334907  4.5983686 5.158289  5.0611143 4.7923727 5.2032986\n",
            "  4.497979  3.7692559 2.4888701]\n",
            " [2.710378  3.6557481 4.0984416 4.391086  4.223321  4.3024955 4.180644\n",
            "  3.8035707 3.0494723 2.2483914]\n",
            " [2.2559392 2.958373  3.494906  3.9263682 4.151607  4.3087196 3.9698238\n",
            "  3.2617774 2.5996373 1.9848166]\n",
            " [2.0325747 2.5512037 2.7694614 3.4734628 3.6617188 3.6702065 3.3071947\n",
            "  2.7912602 2.3656678 1.8811034]\n",
            " [1.8162045 1.9926364 2.0811248 2.436754  2.511536  2.6223068 2.3553855\n",
            "  2.1424308 1.970877  1.722128 ]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(2.1141367, 0.8841186)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.0229241  1.3688424  1.3784045  1.292621   1.0184793 ]\n",
            " [1.2406355  1.8533441  2.1141367  2.0891953  1.3003782 ]\n",
            " [1.0829848  1.7569507  2.0288236  1.8381888  1.1530637 ]\n",
            " [0.88411874 1.1107391  1.4666907  1.3009409  0.9170371 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.19820948 0.03113227 0.0022898  0.03089073 0.         0.08779179\n",
            "  0.10023921 0.         0.         0.         0.         0.\n",
            "  0.06069695 0.17406362 0.         0.20486845 0.01803909 0.01532246\n",
            "  0.         0.06749082 0.36819428 0.         0.         0.12174127\n",
            "  0.         0.15871449 0.         0.         0.02755526 0.\n",
            "  0.02961053 0.         0.03825517 0.02442702 0.00476655 0.\n",
            "  0.18763413 0.         0.11145093 0.         0.         0.04904993\n",
            "  0.         0.         0.         0.17805682 0.         0.2810091\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.02856128 0.         0.         0.         0.         0.18501402\n",
            "  0.03480263 0.04807685 0.         0.         0.0644927  0.\n",
            "  0.13133219 0.         0.         0.         0.         0.\n",
            "  0.00043705 0.05718438 0.04626079 0.         0.         0.06749735\n",
            "  0.         0.12200186 0.         0.         0.14355993 0.\n",
            "  0.04731024 0.05294702 0.         0.         0.         0.00596164\n",
            "  0.02261404 0.         0.03383129 0.32905766 0.2150652  0.03205049\n",
            "  0.01234586 0.00332594 0.00797853 0.         0.         0.\n",
            "  0.15242226 0.16865727 0.         0.03006239 0.         0.\n",
            "  0.         0.         0.         0.11129535 0.05156341 0.21660402\n",
            "  0.0878624  0.15838386 0.         0.02157097 0.11614618 0.02441938\n",
            "  0.2613133  0.         0.14579007 0.02372898 0.         0.\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "input shape=torch.Size([1, 121, 145, 1])\n",
            "1 torch.Size([1, 121, 145, 1]) OAS30344_MR_d0049\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_axial/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30344_MR_d0049_cdr0.0/85_conv1_71/\n",
            "torch.Size([1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.388099, 0.11625062)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(4.554654, 0.66180325)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84985703 0.75274163 0.7461117  ... 0.7268853  0.7614306  0.75633585]\n",
            " [0.8163176  0.7215066  0.67592895 ... 0.6812856  0.7248418  0.77615654]\n",
            " [0.8295602  0.7480049  0.7294599  ... 0.7128681  0.7689445  0.7629746 ]\n",
            " ...\n",
            " [0.8094154  0.7341169  0.7135788  ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.8364083  0.79853344 0.7725079  ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.795786   ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(3.4464018, 0.93891555)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.2052779  1.2461866  1.2011304  1.1857845  1.2085018  1.2500156\n",
            "  1.2366474  1.203164   1.2868955  1.2450955  1.2786814  1.2726642\n",
            "  1.3019675  1.2362189  1.2182087  1.211407   1.1858146  1.2202977\n",
            "  1.165528  ]\n",
            " [1.1446675  1.1366644  1.1754335  1.1641079  1.1605107  1.1861153\n",
            "  1.259663   1.333909   1.3876243  1.3182753  1.3703912  1.3982455\n",
            "  1.2780812  1.2738997  1.1805366  1.1285714  1.0928519  1.1396061\n",
            "  1.1516267 ]\n",
            " [1.0589728  1.0909604  1.0124989  1.0345205  1.1855139  1.2331854\n",
            "  1.3682137  1.5507376  1.6797737  1.5509502  1.7418116  1.563881\n",
            "  1.5744447  1.5129417  1.2774541  1.161079   1.010775   1.0756029\n",
            "  1.1381489 ]\n",
            " [1.0712734  1.0445712  1.0366703  1.1268997  1.3871932  1.5298196\n",
            "  1.7209865  1.8843752  1.7713308  1.901341   2.0091028  1.8458534\n",
            "  1.9376537  1.8540666  1.7239467  1.2357749  1.0675005  1.0390753\n",
            "  1.1195039 ]\n",
            " [1.0915812  1.0638787  1.1627394  1.2649549  1.5661645  1.8182436\n",
            "  1.8469641  2.1131146  2.3253448  2.4867883  2.563674   2.5761814\n",
            "  2.7855082  2.348547   1.9314824  1.5621338  1.1739788  1.0996392\n",
            "  1.1068305 ]\n",
            " [1.0746858  1.0207777  1.1829401  1.2793416  1.7158546  2.108298\n",
            "  2.1536655  2.1532087  2.2549543  2.439178   2.9610581  2.783165\n",
            "  2.925157   2.6892414  2.2122374  1.8725182  1.276173   1.0960938\n",
            "  1.0898284 ]\n",
            " [1.110164   1.0318469  1.1398782  1.4815627  2.0395834  2.085794\n",
            "  2.469522   2.5051517  2.429067   2.8952668  3.131106   3.2899194\n",
            "  3.0129626  3.0983658  2.7504885  2.106408   1.3789512  1.1322414\n",
            "  1.1269845 ]\n",
            " [1.0845484  1.0519508  1.3090411  1.6841279  2.0683439  2.4384873\n",
            "  2.6970901  2.4049296  2.816794   3.0131187  2.9097624  3.1439002\n",
            "  3.0714037  3.2024326  2.9733253  1.9998236  1.4253358  1.1855522\n",
            "  1.1183954 ]\n",
            " [1.0594348  1.0136408  1.2131928  1.5408529  2.1201189  2.2457972\n",
            "  2.512658   2.8222723  2.8098974  3.0080962  2.8793411  3.2174249\n",
            "  3.0974684  3.130152   2.7398376  2.2030706  1.3756609  1.1239439\n",
            "  1.1173579 ]\n",
            " [1.0805465  1.0150784  1.2159122  1.4479151  1.9728197  2.204941\n",
            "  2.2536156  2.3549533  2.724617   3.0346875  2.8843813  2.9265013\n",
            "  2.898673   2.707037   2.0555608  1.6146761  1.2798848  1.1462996\n",
            "  1.1361859 ]\n",
            " [1.0785574  1.0525056  1.0182732  1.2325388  1.692919   2.1580029\n",
            "  2.0075572  2.1330843  2.5682487  3.0908685  3.4464018  3.2221859\n",
            "  2.8894682  2.5937088  1.8153055  1.4533826  1.1005721  1.0697258\n",
            "  1.099195  ]\n",
            " [1.0772538  1.0677851  1.0366261  1.1282624  1.410854   1.6444695\n",
            "  1.9103785  1.9007922  2.3678842  2.4124825  2.7422457  2.9093184\n",
            "  2.896406   1.9229254  1.6749266  1.3654467  1.0939385  1.0732015\n",
            "  1.1036959 ]\n",
            " [1.0685089  1.0221969  1.0756422  1.0650147  1.1480564  1.2719511\n",
            "  1.480636   1.6020769  2.090835   2.118403   2.0308971  2.0515094\n",
            "  2.006439   1.6329643  1.4780649  1.2036477  0.93891567 1.0538559\n",
            "  1.0815395 ]\n",
            " [1.0600108  1.057352   1.0612284  1.0190703  1.1272179  1.1443473\n",
            "  1.246817   1.3316014  1.5815784  1.635942   1.5664883  1.4852813\n",
            "  1.5559103  1.4051684  1.3132375  1.1178453  0.96962476 1.0703396\n",
            "  1.105567  ]\n",
            " [1.0759307  1.0977842  1.0829142  1.0329229  1.0064255  1.0525252\n",
            "  1.069355   1.1187583  1.2285556  1.1739149  1.1572385  1.2343508\n",
            "  1.1832265  1.1523176  1.0852097  1.005172   0.98609287 1.1124495\n",
            "  1.1353968 ]\n",
            " [1.0128224  1.0722438  1.0671984  1.0441152  1.0275782  1.0298609\n",
            "  1.0518231  1.0838809  1.0810211  1.1122093  1.1367472  1.1082554\n",
            "  1.0393847  1.0341028  1.0428342  1.0117172  1.0191209  1.0708781\n",
            "  1.0755368 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(4.1912155, 1.6520494)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.7921854 1.8691655 1.8819995 1.8802605 1.9462461 1.8681484 1.9055442\n",
            "  1.8412217 1.8278674 1.6520495]\n",
            " [1.9823126 2.36483   2.312964  2.4148262 2.5179791 2.6014862 2.3822153\n",
            "  2.332107  2.0575495 1.7664908]\n",
            " [1.997187  2.253986  2.606399  2.870595  3.245256  3.0157115 3.0242\n",
            "  2.5337672 2.2260551 1.8244406]\n",
            " [2.030024  2.497348  2.710532  3.0688603 3.4570913 3.7500982 3.520563\n",
            "  2.8571181 2.291171  1.8466531]\n",
            " [2.0605056 2.4708705 2.9704337 3.5880444 4.1591325 4.110422  3.7171707\n",
            "  3.0085804 2.3126662 1.8143712]\n",
            " [1.8750645 2.3775516 2.5190897 2.9930286 3.4741192 4.1912155 3.5103436\n",
            "  2.8189087 2.21131   1.789559 ]\n",
            " [1.9247866 2.2148461 2.3864353 2.7987342 3.0281994 3.001667  2.988235\n",
            "  2.5400937 2.2034674 1.8051876]\n",
            " [1.7702488 1.8975616 1.9274299 2.0297987 2.219036  2.170825  2.3070748\n",
            "  2.0205016 1.8188795 1.6820489]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.8473864, 0.84575367)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8576364  1.0820712  1.0537596  0.9751022  0.8457538 ]\n",
            " [0.9797429  1.4055325  1.6878909  1.3590914  0.88040125]\n",
            " [0.96376437 1.4436172  1.8473865  1.5315261  0.93499607]\n",
            " [0.8563508  1.0589029  1.2028075  1.1737852  0.8870073 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:259: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predictions=[[1.97602123e-01 2.56280368e-03 6.86276797e-03 6.49605393e-02\n",
            "  2.69212760e-04 1.05474405e-01 8.43925327e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 7.68566271e-03 0.00000000e+00\n",
            "  1.45857288e-02 1.69927970e-01 0.00000000e+00 1.98553935e-01\n",
            "  1.40614407e-02 0.00000000e+00 0.00000000e+00 6.33572116e-02\n",
            "  3.45100135e-01 0.00000000e+00 0.00000000e+00 1.82579771e-01\n",
            "  0.00000000e+00 1.99276343e-01 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 3.90189551e-02 8.32769088e-04\n",
            "  7.62178227e-02 5.22373104e-03 7.01648518e-02 0.00000000e+00\n",
            "  2.12246582e-01 1.86615400e-02 9.87766683e-02 0.00000000e+00\n",
            "  0.00000000e+00 6.63428009e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.49404630e-01 0.00000000e+00 2.33870611e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 3.29571031e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.88946620e-01\n",
            "  4.26549092e-02 1.03801182e-02 0.00000000e+00 0.00000000e+00\n",
            "  1.17735090e-02 0.00000000e+00 1.32638082e-01 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.85987630e-02\n",
            "  2.12445986e-02 9.96114463e-02 7.07017779e-02 0.00000000e+00\n",
            "  0.00000000e+00 6.66218996e-02 0.00000000e+00 2.27378383e-02\n",
            "  1.33761158e-02 0.00000000e+00 1.33072808e-01 0.00000000e+00\n",
            "  4.76957187e-02 1.27912238e-01 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.09881029e-01 2.39966080e-01 1.79106966e-01 1.09078385e-01\n",
            "  0.00000000e+00 5.42805344e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.54113635e-01 1.69627592e-01\n",
            "  0.00000000e+00 6.74505532e-02 3.51561010e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.32646963e-01\n",
            "  2.30341796e-02 2.38552958e-01 9.63806883e-02 1.72762722e-01\n",
            "  0.00000000e+00 0.00000000e+00 1.77092001e-01 0.00000000e+00\n",
            "  2.35341430e-01 0.00000000e+00 1.63550034e-01 8.93457457e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:557: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:570: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([145, 1, 121])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:605: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/BA_Estimation/results/sal_map_sagittal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30344_MR_d0049_cdr0.0/85_conv1_71/\n",
            "torch.Size([121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(2.085549, 0.08988717)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(6.275003, 0.6568867)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8502583  0.7498355  0.7541715  ... 0.7268853  0.7614306  0.7563359 ]\n",
            " [0.8165544  0.72393095 0.67174506 ... 0.68127084 0.7248419  0.7761559 ]\n",
            " [0.82933116 0.7473342  0.7222172  ... 0.713357   0.76862764 0.7627293 ]\n",
            " ...\n",
            " [0.80941546 0.73411703 0.71359324 ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.8364083  0.79853344 0.77250785 ... 0.78179705 0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.795786   ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(5.1143675, 0.984153)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.2250949 1.266279  1.702282  1.9152306 2.336163  2.2533174 2.392056\n",
            "  2.2070076 2.3385684 2.1658306 2.233362  2.1574192 1.865183  2.0070086\n",
            "  1.654684  1.35644   1.2802192 1.2484844 1.1933862]\n",
            " [1.1205641 1.4253842 1.7982613 2.5910993 2.937086  3.170434  3.1247182\n",
            "  3.2638597 2.8866026 2.833799  2.838485  2.615986  2.7071595 2.4463203\n",
            "  2.1324487 1.5419976 1.3172232 1.1837214 1.1864977]\n",
            " [1.0894461 1.3879246 2.0844073 2.6722198 3.4142897 3.0758505 3.3121133\n",
            "  3.7765574 3.7291229 3.9576602 3.9810443 3.4070234 3.7052479 2.8023572\n",
            "  2.3263848 1.775757  1.4198111 1.2831043 1.2310574]\n",
            " [1.2867564 1.4612367 2.2152462 2.7103152 3.5626717 3.4601188 3.652856\n",
            "  3.7737913 4.0938616 4.190584  3.9324434 4.195475  4.2063637 3.963522\n",
            "  2.9013734 2.2481704 1.8136691 1.554725  1.3927654]\n",
            " [1.2945263 1.5794822 2.1831307 2.9700885 3.1975586 3.794749  3.489524\n",
            "  3.8352995 4.359572  4.595681  4.481456  4.1771655 5.1143675 4.2437887\n",
            "  3.0171063 2.9526436 2.190808  2.073503  1.6084737]\n",
            " [1.1972582 1.6373693 2.2047486 2.867527  2.6983976 3.1673172 3.3210402\n",
            "  3.4282088 3.762487  4.794446  4.705717  5.076354  4.736265  4.203558\n",
            "  3.7395992 3.308432  3.0618763 2.2164693 1.5722228]\n",
            " [1.1107196 1.68495   2.2086058 2.4515886 2.5109968 2.6512134 2.3400524\n",
            "  2.9643497 3.7748828 4.16452   4.613743  4.7620263 4.969456  4.256856\n",
            "  3.6633766 3.0987456 2.8992472 2.2816095 1.5042636]\n",
            " [1.1179626 1.4680576 1.9187293 2.2430592 1.9775945 1.9330572 2.0694084\n",
            "  2.5914292 3.2918043 3.920081  4.697323  4.1122136 3.818932  4.001895\n",
            "  3.141608  2.771229  2.4275417 2.0450497 1.4385685]\n",
            " [1.0709635 1.422918  1.7634224 2.1568189 2.0596209 1.8611362 1.9041299\n",
            "  2.5677185 2.8319168 3.4241939 3.426108  3.9214606 3.593382  3.4354534\n",
            "  3.2246609 2.5425482 1.9265416 1.6843075 1.405217 ]\n",
            " [1.0550015 1.2636465 1.6482856 1.9210703 2.1398106 1.8636751 1.8535004\n",
            "  2.0244904 2.3509798 2.8535013 3.1091697 3.4259586 3.7631004 3.3603878\n",
            "  3.010027  2.131919  1.5781804 1.3737625 1.1740285]\n",
            " [1.0879792 1.2193056 1.3611104 1.5704103 1.9799005 1.8527194 1.8176157\n",
            "  2.1030838 2.1830235 2.5809772 3.1368954 3.2946978 3.3585176 3.2200532\n",
            "  2.4768    1.9897953 1.3444782 1.2553226 1.1080958]\n",
            " [1.0695406 1.0729145 1.1837616 1.2646317 1.5148858 1.5818661 1.8561777\n",
            "  2.0202053 1.9326135 2.3389883 2.5263848 2.997491  3.116271  2.3699808\n",
            "  2.0707302 1.5185968 1.2319127 1.1126884 1.1388673]\n",
            " [1.0368892 1.0780438 1.0947205 1.1472722 1.2239336 1.334991  1.4834518\n",
            "  1.6564149 1.9827013 1.8491042 2.1914287 2.048894  2.1027846 1.8655511\n",
            "  1.5909864 1.365799  1.0916654 1.0387347 1.0862592]\n",
            " [1.0501907 1.0404322 1.0674214 1.08949   1.0584681 1.1302711 1.3531915\n",
            "  1.3296231 1.4092971 1.5471127 1.4962147 1.6877915 1.6024956 1.4724394\n",
            "  1.3846029 1.1720494 1.0411558 1.0766147 1.1081656]\n",
            " [1.083528  1.1062716 1.069583  1.027892  1.0238148 1.0893673 1.1265475\n",
            "  1.1624601 1.2212483 1.1208593 1.2543746 1.2642511 1.2282063 1.1864954\n",
            "  1.0544777 1.039647  0.9841531 1.0935062 1.131552 ]\n",
            " [1.0156709 1.0746489 1.0801867 1.0486764 1.0513438 1.0074627 1.0468053\n",
            "  1.0712007 1.0625527 1.1689727 1.0847338 1.0699478 1.0183417 1.0384623\n",
            "  1.0395658 1.0205405 1.0232636 1.0767115 1.0777851]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(5.881318, 1.7485088)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[2.0108395 2.4904976 3.0758576 3.2916117 3.6277392 3.4128442 3.2275982\n",
            "  2.6090245 2.1639132 1.8747907]\n",
            " [2.3412209 3.232541  3.9300041 4.275961  4.558401  5.0162477 4.448488\n",
            "  3.5823076 2.7868338 2.0660903]\n",
            " [2.3809786 3.3553467 3.9726186 4.5364714 5.5603914 5.881318  5.430057\n",
            "  4.302534  3.1165628 2.3417974]\n",
            " [2.3925915 3.1005726 3.5044537 3.8755004 4.5806446 5.8248777 5.6428847\n",
            "  4.6403265 3.376264  2.281822 ]\n",
            " [2.1803546 2.7040586 3.1315608 3.4576633 4.190324  4.5448103 4.6272497\n",
            "  3.993993  2.951528  2.0775375]\n",
            " [2.00402   2.3351052 2.6631813 2.8248606 3.3944325 4.1296406 4.080748\n",
            "  3.3820887 2.5718725 1.9333683]\n",
            " [1.9160323 2.2587023 2.246481  2.4515953 2.8296764 3.1866045 3.1357841\n",
            "  2.795533  2.3427775 1.8033835]\n",
            " [1.7607597 1.8759931 1.7906268 1.973627  2.169619  2.1355307 2.1811225\n",
            "  2.0261695 1.8592607 1.7485089]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(2.3836913, 0.835719)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.9232123  1.3555804  1.6327422  1.3785111  0.90459746]\n",
            " [1.0883597  1.8982406  2.3836913  2.0504272  1.1828691 ]\n",
            " [0.9814985  1.5652243  2.012424   1.7858318  1.018979  ]\n",
            " [0.8357191  1.0298792  1.1419505  1.1351238  0.89830124]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:232: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:242: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(4.2200174, 0.66180766)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84985703 0.7527417  0.74611175 ... 0.7268853  0.7614306  0.75633585]\n",
            " [0.81631756 0.7215063  0.6759294  ... 0.6812856  0.7248418  0.77615654]\n",
            " [0.82956004 0.7480056  0.7294707  ... 0.7128678  0.7689445  0.7629746 ]\n",
            " ...\n",
            " [0.80940366 0.7340684  0.71358883 ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.8363992  0.7984865  0.7724765  ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.8018041  0.7531828  0.7957865  ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(2.759985, 0.90344286)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.1972375  1.2269019  1.2044592  1.2032336  1.177805   1.1875099\n",
            "  1.1760408  1.202068   1.2409405  1.2442566  1.2106823  1.2102057\n",
            "  1.1899999  1.1907712  1.1820878  1.1813039  1.1825693  1.2216579\n",
            "  1.1597017 ]\n",
            " [1.1340356  1.1266482  1.152364   1.1402609  1.1151121  1.1445911\n",
            "  1.1502641  1.2452314  1.3090345  1.3373555  1.1897475  1.1638099\n",
            "  1.1895798  1.1411449  1.136895   1.1216072  1.069428   1.1416415\n",
            "  1.1549908 ]\n",
            " [1.0460106  1.0536255  1.0194073  0.99802494 0.98660547 0.99387044\n",
            "  1.0613791  1.2085618  1.3197042  1.2709156  1.2590238  1.1785294\n",
            "  1.0636811  1.0936859  1.0238911  1.0190924  0.9793217  1.0670056\n",
            "  1.137214  ]\n",
            " [1.0580435  1.0218056  1.0182136  0.968336   0.9391955  0.9258641\n",
            "  1.0843807  1.3368136  1.54225    1.334459   1.419864   1.2954353\n",
            "  1.2651054  1.0573385  1.0356451  0.9709562  0.91590035 1.020305\n",
            "  1.1258572 ]\n",
            " [1.059914   1.0373263  1.0451312  1.0049719  0.9761876  0.95606685\n",
            "  1.1322404  1.2243699  1.427611   1.6499627  1.529704   1.6495688\n",
            "  1.3888977  1.2076576  1.0036116  0.99276483 0.9426728  1.0486139\n",
            "  1.1208011 ]\n",
            " [1.0621389  1.029542   1.0564429  0.9997872  0.987572   1.0810837\n",
            "  1.0802243  1.1562234  1.4399306  1.6284827  2.1118357  1.738225\n",
            "  1.4284962  1.174207   1.0285162  0.9811468  0.9434601  1.0463704\n",
            "  1.1143433 ]\n",
            " [1.063166   1.0184938  1.0487267  1.0120755  1.0459245  1.1268092\n",
            "  1.3011168  1.4453245  1.6194189  1.6787008  1.809794   1.7498382\n",
            "  1.3748153  1.162707   1.0089222  0.98230493 0.9213982  1.0520035\n",
            "  1.1133934 ]\n",
            " [1.0634179  1.0275844  1.0359516  1.0134263  1.0183796  1.3509251\n",
            "  1.4672333  1.7979877  1.8937875  1.8268379  1.8777555  1.5522676\n",
            "  1.257582   1.1126109  1.0085473  0.9760761  0.9273567  1.0579947\n",
            "  1.1120288 ]\n",
            " [1.0647029  1.0180689  1.0328656  1.0014919  1.0020268  1.5019677\n",
            "  2.1103914  2.7019331  2.4670763  1.8744375  1.5689195  1.1147518\n",
            "  1.1281835  1.0633911  0.998904   0.9829881  0.9309976  1.0564739\n",
            "  1.1112849 ]\n",
            " [1.0625132  1.012683   1.0369455  0.9630818  1.044416   1.3361334\n",
            "  2.0209153  2.759985   2.5708694  1.8105823  1.4616231  1.0901744\n",
            "  1.028418   1.0196688  0.9739349  0.9800538  0.942026   1.0546764\n",
            "  1.1108592 ]\n",
            " [1.0573701  1.0191735  1.0381848  0.96590674 0.903443   1.1914104\n",
            "  1.6585411  1.8238426  1.9970509  1.7222351  1.3199927  1.0748885\n",
            "  1.0117286  0.98134685 0.9951716  1.0030899  0.94626415 1.0526447\n",
            "  1.1123873 ]\n",
            " [1.0671246  1.013481   1.0280696  0.9659353  0.93609875 1.1398345\n",
            "  1.2168521  1.5059239  1.4785371  1.3883687  1.1673667  0.99388784\n",
            "  0.9901444  0.98430115 0.9765483  0.9871503  0.94472444 1.0640664\n",
            "  1.0983113 ]\n",
            " [1.0622896  1.0273527  1.0263352  0.98369586 0.9980848  0.9876363\n",
            "  1.1012877  1.1677892  1.1031848  1.2040516  1.0445007  1.02645\n",
            "  0.99993813 0.99112487 0.97681576 0.99305373 0.9410093  1.0490785\n",
            "  1.0904665 ]\n",
            " [1.0509528  1.0546666  1.054937   1.0479851  1.0257965  1.0556234\n",
            "  1.038386   1.0685862  1.1018006  1.0790061  1.0629504  1.0368034\n",
            "  1.0300719  1.021165   1.0202404  1.0132182  0.98669004 1.0717351\n",
            "  1.1076981 ]\n",
            " [1.0718111  1.0804061  1.0633553  1.0399796  1.0329012  1.0373083\n",
            "  1.0097613  1.0156912  1.0377628  1.0329616  1.033729   1.0254627\n",
            "  1.0288846  1.0235201  1.0362744  1.0341402  0.9918849  1.1034203\n",
            "  1.137874  ]\n",
            " [1.0148174  1.0675508  1.0677785  1.04204    1.0392619  1.0454135\n",
            "  1.051597   1.0543509  1.0446633  1.0516776  1.0490794  1.044002\n",
            "  1.0481173  1.0456237  1.0435966  1.025283   1.0248383  1.069787\n",
            "  1.0765055 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(2.758133, 1.6380708)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.7748437 1.879654  1.8325251 1.9128926 1.8748633 1.8091294 1.7315208\n",
            "  1.810553  1.7961602 1.6478096]\n",
            " [1.9236451 2.2328856 2.1586008 2.240618  2.182355  2.1535964 2.1806264\n",
            "  2.0539255 2.0661008 1.764687 ]\n",
            " [1.9582607 2.1599345 2.2083817 2.3221126 2.3588715 2.2871232 2.1247196\n",
            "  2.03027   2.0968845 1.7582995]\n",
            " [1.9251894 2.1633062 2.24407   2.5565681 2.4954917 2.5352    2.2056036\n",
            "  2.0938034 2.0948768 1.8175335]\n",
            " [1.9649397 2.2237415 2.3844514 2.758133  2.6691396 2.4024875 2.1753778\n",
            "  2.155868  2.1419725 1.8226185]\n",
            " [1.9116896 2.1626043 2.217562  2.5686712 2.5479808 2.1911354 2.1557984\n",
            "  2.049632  2.0522757 1.7438842]\n",
            " [1.9379836 2.1467748 2.220603  2.1747265 2.3129988 2.1830325 2.1887522\n",
            "  2.0637827 2.117206  1.7832974]\n",
            " [1.7695282 1.8830925 1.8625199 1.836884  1.8220235 1.8407061 1.8061358\n",
            "  1.7347559 1.8424993 1.638071 ]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.3148057, 0.83262956)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.86149746 0.932461   0.93308675 0.95947903 0.8326297 ]\n",
            " [0.94760895 1.2065572  1.2463684  1.2147595  0.968632  ]\n",
            " [0.9533044  1.2200938  1.3148059  1.228289   0.9802842 ]\n",
            " [0.8498322  1.0071008  0.9846077  0.9904337  0.907534  ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.0813129  0.03898782 0.         0.01086727 0.         0.\n",
            "  0.00185023 0.         0.         0.0019493  0.0972995  0.\n",
            "  0.11394744 0.12975946 0.07718289 0.15877682 0.12157986 0.\n",
            "  0.         0.08499701 0.30773938 0.         0.         0.05123377\n",
            "  0.         0.08765128 0.         0.         0.         0.01818281\n",
            "  0.01750045 0.         0.         0.         0.10328078 0.04577824\n",
            "  0.12244986 0.04585252 0.05212491 0.06144608 0.         0.00544491\n",
            "  0.05683224 0.06778061 0.         0.104325   0.         0.10278903\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.02339835 0.         0.         0.         0.         0.21003163\n",
            "  0.05138781 0.03372351 0.07358053 0.         0.         0.\n",
            "  0.08729615 0.         0.         0.05140397 0.         0.\n",
            "  0.         0.         0.09669925 0.         0.04009998 0.04446002\n",
            "  0.0772485  0.0752707  0.         0.03479565 0.06668776 0.\n",
            "  0.0241498  0.21241635 0.         0.03472299 0.         0.01442956\n",
            "  0.         0.         0.         0.09732469 0.28120396 0.07914377\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.19883865 0.19312534 0.10370106 0.20820726 0.08228888 0.\n",
            "  0.00354428 0.         0.         0.02298052 0.         0.19090374\n",
            "  0.01828633 0.23243809 0.20893426 0.         0.1040378  0.\n",
            "  0.35247567 0.         0.2897126  0.         0.         0.\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_sagittal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/82_conv1_71/\n",
            "torch.Size([121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(0.68711925, 0.07573916)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(3.3588552, 0.64874995)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8498268  0.7525685  0.7461792  ... 0.7269085  0.76144195 0.7563404 ]\n",
            " [0.81669533 0.7226926  0.6748214  ... 0.6813036  0.7248712  0.77614623]\n",
            " [0.83011705 0.74618256 0.7318221  ... 0.7126488  0.76885575 0.76288617]\n",
            " ...\n",
            " [0.80941534 0.73411703 0.7135827  ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.83640826 0.79853344 0.7725078  ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.795786   ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(2.5421162, 0.9131218)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.200194   1.2257394  1.2375678  1.2321237  1.2064503  1.2660905\n",
            "  1.2514836  1.1940144  1.1824743  1.1977245  1.1806041  1.1661247\n",
            "  1.1746703  1.1723977  1.1772375  1.1828479  1.1856301  1.221202\n",
            "  1.161024  ]\n",
            " [1.1567074  1.1559678  1.177904   1.273131   1.2213845  1.4291062\n",
            "  1.2097895  1.216005   1.1403135  1.1334049  1.1231302  1.1208471\n",
            "  1.1171768  1.1240603  1.1253663  1.1093625  1.0795505  1.140879\n",
            "  1.1563387 ]\n",
            " [1.0643348  1.133808   1.1552101  1.4435908  1.5452002  1.3924966\n",
            "  1.2843518  1.2172859  1.0756576  1.0343791  1.0325383  1.02751\n",
            "  0.9975362  1.0053102  0.9965292  1.0183363  0.9817358  1.0668098\n",
            "  1.1383965 ]\n",
            " [1.0406803  1.0678431  1.3012425  1.5343424  1.5813756  1.4694043\n",
            "  1.4916898  1.3416846  1.3573052  1.0905262  1.016527   0.96979266\n",
            "  0.9716723  0.95043695 0.9493202  0.957904   0.91312194 1.0204643\n",
            "  1.1250354 ]\n",
            " [1.0852705  1.1390713  1.2572136  1.6952771  1.9322114  1.7213556\n",
            "  1.5599024  1.6215365  1.3604289  1.2913543  1.0945894  1.0253061\n",
            "  0.9860606  1.0186644  0.99097687 0.99765426 0.95533013 1.0511616\n",
            "  1.1212252 ]\n",
            " [1.0536797  1.0612336  1.1020063  1.3240035  1.4469073  1.6056461\n",
            "  1.7365929  1.6883196  1.4766718  1.2906145  1.1767929  1.0307366\n",
            "  1.0197875  1.0145781  0.98367757 1.0027819  0.94938874 1.0572293\n",
            "  1.1129342 ]\n",
            " [1.0489289  1.0514914  1.0843183  1.1188093  1.2897607  1.3921168\n",
            "  1.4168748  1.5205718  1.3786261  1.2528265  1.207095   1.2587336\n",
            "  1.1522402  1.1017939  1.0334707  1.0049747  0.941068   1.0561082\n",
            "  1.1126119 ]\n",
            " [1.0384306  1.0496678  1.0179099  1.1355287  1.0538851  1.1845168\n",
            "  1.1744198  1.3518144  1.3998272  1.3120401  1.4383116  1.4689456\n",
            "  1.5990157  1.2126734  1.0746014  1.0252852  0.95263886 1.0534035\n",
            "  1.1124986 ]\n",
            " [1.0502734  1.0072454  0.9847503  1.0683341  1.1139215  1.1404581\n",
            "  1.11629    1.085223   1.2810767  1.5073706  1.6736816  1.9736956\n",
            "  1.5987092  1.5367967  1.1753374  1.0281905  0.9358789  1.0520142\n",
            "  1.1105895 ]\n",
            " [1.0858968  1.0392452  1.0312415  1.1502805  1.2734616  1.188434\n",
            "  1.0978183  1.0858661  1.2657839  1.6097978  2.1326818  2.5421162\n",
            "  1.9658407  1.5187573  1.1374248  1.0125352  0.92339766 1.0521129\n",
            "  1.1090287 ]\n",
            " [1.0936772  1.0346264  0.9776964  1.084781   1.142035   1.173617\n",
            "  1.0514603  1.055797   1.0713522  1.3354949  1.7274119  1.9772369\n",
            "  1.714924   1.3684167  1.0848217  1.0185052  0.9273143  1.053224\n",
            "  1.1126354 ]\n",
            " [1.0541312  1.0367771  1.0782741  0.94704187 0.9923084  1.1190681\n",
            "  1.0907022  1.0092231  0.9763756  1.1950084  1.2958006  1.3095787\n",
            "  1.3401555  1.1465784  0.99886465 0.9967987  0.93712974 1.0641066\n",
            "  1.1004922 ]\n",
            " [1.0549173  1.0597227  1.031829   0.9555142  0.9733006  1.0624782\n",
            "  1.0437566  1.0407463  1.0685078  1.0307268  1.1075934  1.1286472\n",
            "  1.0846359  1.0456644  1.002409   0.9799429  0.93017626 1.0515203\n",
            "  1.0902569 ]\n",
            " [1.051124   1.0573679  1.0808907  1.0293964  1.029703   1.0682936\n",
            "  1.0571228  1.096885   1.0541822  1.0676293  1.1084331  1.0409478\n",
            "  1.0777558  1.0604323  1.0404631  1.0044962  0.98424745 1.0714371\n",
            "  1.1073695 ]\n",
            " [1.0763484  1.0880524  1.0534717  1.0360763  1.0530013  1.0698243\n",
            "  1.0257936  1.1098571  1.0663439  1.0096213  1.0018286  1.0091842\n",
            "  1.0476805  1.0286993  1.0330102  1.0430093  0.98988235 1.1042186\n",
            "  1.1375912 ]\n",
            " [1.0127888  1.0670606  1.0760494  1.0506626  1.0367101  1.0345591\n",
            "  1.027551   1.0451245  1.0434474  1.0508364  1.0387347  1.0529727\n",
            "  1.0444099  1.0497487  1.043176   1.0264803  1.0268818  1.0676378\n",
            "  1.0761858 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(2.633603, 1.6509912)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.8288997 1.9697475 1.8262993 1.8154246 1.8595701 1.8059877 1.7864155\n",
            "  1.779925  1.7784439 1.6509913]\n",
            " [1.9577664 2.306188  2.4055994 2.3482413 2.160897  2.12099   2.0793371\n",
            "  2.1211429 2.081915  1.7451192]\n",
            " [2.040736  2.3890095 2.633603  2.562861  2.3971558 2.162788  2.0781338\n",
            "  2.103173  2.0654907 1.7551149]\n",
            " [2.0033383 2.2895608 2.3119    2.4376583 2.3490624 2.4061937 2.2194166\n",
            "  2.1228373 2.071604  1.7792072]\n",
            " [2.044982  2.2281098 2.3303084 2.4008915 2.6263366 2.4876919 2.2666593\n",
            "  2.1710234 2.151977  1.7696644]\n",
            " [1.9570771 2.2226179 2.1311297 2.2591474 2.3771963 2.4417453 2.2843585\n",
            "  2.1061177 2.070934  1.8054851]\n",
            " [1.9854748 2.158797  2.1450958 2.2210789 2.2088594 2.249956  2.2278728\n",
            "  2.1736228 2.1725845 1.7814091]\n",
            " [1.7912945 1.8562665 1.8438382 1.8436369 1.8306141 1.872322  1.8350663\n",
            "  1.7554191 1.8139254 1.653097 ]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.3268402, 0.8339588)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8339589  0.9445471  0.9843535  0.9587418  0.83466154]\n",
            " [0.96316504 1.2532595  1.284501   1.162826   0.92330587]\n",
            " [0.95288944 1.2111326  1.3268403  1.2589666  0.9522557 ]\n",
            " [0.8593819  0.95411664 0.9867796  0.9813005  0.91379726]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.09453895 0.0263     0.         0.02113253 0.         0.\n",
            "  0.01816195 0.         0.         0.         0.08023029 0.\n",
            "  0.1037617  0.1510259  0.06924941 0.14904274 0.12257535 0.\n",
            "  0.         0.07505336 0.316339   0.         0.         0.07750116\n",
            "  0.         0.1037178  0.         0.         0.         0.00525085\n",
            "  0.01816772 0.         0.         0.         0.1100135  0.03399449\n",
            "  0.14582908 0.05546275 0.0615702  0.05405006 0.         0.01398078\n",
            "  0.04196834 0.0290589  0.         0.11362462 0.         0.123351\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.02786161 0.         0.         0.         0.         0.22599229\n",
            "  0.06447913 0.04668962 0.07249217 0.         0.         0.\n",
            "  0.08973679 0.         0.         0.05183004 0.         0.\n",
            "  0.         0.00474889 0.08337781 0.         0.04267664 0.06061496\n",
            "  0.08245964 0.05653447 0.         0.01046623 0.07817184 0.\n",
            "  0.04327773 0.2026946  0.         0.0236633  0.         0.01598697\n",
            "  0.         0.         0.         0.10682213 0.2617579  0.09214958\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.20012155 0.19031058 0.08912995 0.18894282 0.08680525 0.\n",
            "  0.         0.         0.         0.03159449 0.         0.17926848\n",
            "  0.01083212 0.24755253 0.18441328 0.         0.11994705 0.\n",
            "  0.3410132  0.         0.29623866 0.         0.         0.\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_coronal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/82_conv1_71/\n",
            "torch.Size([1, 1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.542325, 0.08698582)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(5.5592375, 0.6625625)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8497896  0.75268835 0.74612516 ... 0.7268833  0.76142734 0.7563356 ]\n",
            " [0.81630814 0.72188663 0.676381   ... 0.6812713  0.7248375  0.77615505]\n",
            " [0.82916224 0.74709964 0.7300214  ... 0.71282876 0.7689203  0.7629799 ]\n",
            " ...\n",
            " [0.8094154  0.73411596 0.7137184  ... 0.7150029  0.7588718  0.7376537 ]\n",
            " [0.83640826 0.7985296  0.77250224 ... 0.7817968  0.8139498  0.7831061 ]\n",
            " [0.8018056  0.7531841  0.7957833  ... 0.79865575 0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(4.3659673, 0.9143449)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.1985213  1.2240623  1.201466   1.1876932  1.1700162  1.1775795\n",
            "  1.1703864  1.1715832  1.174214   1.1703826  1.1731048  1.1717356\n",
            "  1.1743242  1.1729177  1.1771041  1.1834407  1.1850334  1.2207142\n",
            "  1.1605093 ]\n",
            " [1.1296824  1.1261896  1.1367401  1.1242892  1.1172503  1.1180669\n",
            "  1.1126989  1.1119596  1.1200453  1.1154069  1.1202542  1.1179106\n",
            "  1.1224242  1.122147   1.1254715  1.1103965  1.080716   1.1409863\n",
            "  1.1564127 ]\n",
            " [1.0495185  1.0379424  1.0022248  1.0157545  1.0136503  1.0035747\n",
            "  0.9776205  0.99642694 1.0084668  1.0157037  1.0018947  1.0062772\n",
            "  1.0056548  0.9978277  0.9935833  1.0190729  0.9797437  1.0675017\n",
            "  1.1381978 ]\n",
            " [1.0658597  1.0573634  1.017532   1.0359914  0.98676044 1.016427\n",
            "  1.0335884  1.0244864  1.0354543  1.0267007  1.003461   0.9911955\n",
            "  0.97236645 0.9566474  0.9566592  0.9586675  0.914345   1.0193176\n",
            "  1.1260852 ]\n",
            " [1.1222652  1.151586   1.2044827  1.2433834  1.2005446  1.2670916\n",
            "  1.1974657  1.2305171  1.2075454  1.1737652  1.0485916  1.0773286\n",
            "  1.0160085  1.0002562  1.0428295  1.0139685  0.9659058  1.0507225\n",
            "  1.127841  ]\n",
            " [1.1549715  1.3366107  1.6142033  1.820819   1.7272238  1.5859178\n",
            "  1.7025257  1.6130562  1.6169958  1.4280363  1.3843926  1.2264954\n",
            "  1.0488634  0.98253524 1.0034524  1.0387477  0.9421347  1.064631\n",
            "  1.1062979 ]\n",
            " [1.2744287  1.7747532  2.1558628  2.6073685  2.566159   2.5497203\n",
            "  2.2964811  2.2658575  2.1141567  1.9426265  1.4654527  1.3144385\n",
            "  1.1165035  1.0994859  1.1490254  1.0800551  1.0084809  1.0638653\n",
            "  1.1023375 ]\n",
            " [1.3727381  2.1410055  2.9573011  3.6083574  3.634843   3.0876117\n",
            "  2.7721832  3.1109416  2.666318   2.452546   1.8640281  1.5720354\n",
            "  1.329697   1.3440381  1.2541405  1.2510852  1.0245239  1.0753038\n",
            "  1.0929183 ]\n",
            " [1.3225148  1.8957814  3.218388   4.2851095  3.9910269  3.139414\n",
            "  3.082033   2.9018054  2.8718433  2.4014657  2.166835   1.7484773\n",
            "  1.5340172  1.4100243  1.4080766  1.3962675  1.0730673  1.1000704\n",
            "  1.125291  ]\n",
            " [1.213657   1.786189   2.6497836  3.3223262  3.6300693  3.7165165\n",
            "  2.9485216  2.7139316  2.9540691  2.773305   2.135724   1.649928\n",
            "  1.7254287  1.7933475  1.4884008  1.3637661  1.092089   1.0878166\n",
            "  1.0773013 ]\n",
            " [1.1378347  1.3995334  1.983085   2.499021   3.2782178  3.6461968\n",
            "  3.9189794  3.0306373  2.821663   2.2253215  1.7151455  1.464127\n",
            "  1.3420997  1.428553   1.2368373  1.1609315  0.954839   1.091875\n",
            "  1.0942922 ]\n",
            " [1.1143066  1.2671155  1.618213   2.018625   3.1897821  4.3659673\n",
            "  4.058297   3.3506646  2.4507856  2.0649014  1.5078723  1.2510113\n",
            "  1.1178834  1.138129   1.1343085  1.0424032  0.995067   1.0908321\n",
            "  1.0721961 ]\n",
            " [1.0480424  1.1365869  1.2184511  1.7083532  2.522572   3.1156201\n",
            "  2.957258   2.7031682  2.2280047  1.672851   1.2853316  1.1347874\n",
            "  1.0534877  1.0243071  1.0216883  0.9830891  0.9228975  1.0469426\n",
            "  1.0752753 ]\n",
            " [1.0618829  1.0619091  1.1458155  1.4769233  1.6990353  2.1315694\n",
            "  2.0616665  2.06039    1.6207263  1.3229059  1.1914833  1.1525474\n",
            "  1.0199906  1.0257119  1.0416158  1.0386976  0.9990304  1.0632288\n",
            "  1.1111637 ]\n",
            " [1.0697846  1.0853791  1.1110948  1.1463983  1.3272904  1.3998698\n",
            "  1.4547223  1.3858216  1.3302172  1.132557   1.1174845  1.0297526\n",
            "  1.0290631  1.0234622  1.0286483  1.0458876  0.9849303  1.105493\n",
            "  1.1395158 ]\n",
            " [1.0070063  1.0733978  1.054926   1.0435092  1.0946994  1.1531157\n",
            "  1.1372129  1.0939381  1.0450433  1.0653058  1.0212157  1.0500954\n",
            "  1.0516529  1.0516711  1.0580758  1.0249802  1.022484   1.0689713\n",
            "  1.0760559 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(4.175993, 1.6149707)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.8257145 1.8626113 1.7759799 1.8528221 1.8122985 1.8410206 1.7885301\n",
            "  1.7774206 1.7707748 1.666757 ]\n",
            " [2.0936446 2.3183165 2.200934  2.2790918 2.2140188 2.1961107 2.084849\n",
            "  2.1368306 2.1016843 1.7229081]\n",
            " [2.2417934 2.6604605 2.709455  2.537896  2.3717241 2.2265797 2.0361226\n",
            "  2.056785  2.105959  1.7932456]\n",
            " [2.6794271 3.3999696 3.2996895 3.327856  2.995969  2.5566208 2.2454944\n",
            "  2.1734843 2.1093276 1.7436594]\n",
            " [2.6946378 3.7427785 3.9920537 3.6845524 3.349994  2.6554382 2.3123674\n",
            "  1.9965225 2.090111  1.8073813]\n",
            " [2.4495096 3.3280702 4.03296   4.175993  3.5540907 2.7387981 2.2403703\n",
            "  2.1151803 2.09672   1.8122084]\n",
            " [2.2731652 2.8532732 3.2469766 3.6329422 3.084567  2.3425846 2.1745453\n",
            "  2.2012444 2.1296215 1.8133968]\n",
            " [1.8916026 2.1684887 2.3076773 2.4761612 2.2427807 2.012608  1.9004618\n",
            "  1.7996582 1.8012763 1.6149708]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.4219339, 0.8413121)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.88397217 0.97600543 0.9327971  0.9555794  0.8413122 ]\n",
            " [0.9407998  1.1099895  1.1068245  1.1769     0.9464402 ]\n",
            " [0.9785985  1.421934   1.338214   1.2794942  1.0242834 ]\n",
            " [0.87458736 1.2040979  1.1560415  0.9708194  0.90850765]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[1.25868499e-01 2.29733493e-02 0.00000000e+00 8.60965252e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 2.81990580e-02 1.16475426e-01 0.00000000e+00\n",
            "  8.35861042e-02 1.74774468e-01 5.95539249e-02 1.46878645e-01\n",
            "  1.17823854e-01 0.00000000e+00 0.00000000e+00 1.08562879e-01\n",
            "  3.22392523e-01 0.00000000e+00 0.00000000e+00 1.00566581e-01\n",
            "  0.00000000e+00 1.08779788e-01 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.85083970e-02 2.90081277e-02 9.62733757e-03\n",
            "  1.14120701e-02 9.61275597e-04 1.27864167e-01 5.01857065e-02\n",
            "  1.03577830e-01 3.52066197e-02 4.75211926e-02 3.93145680e-02\n",
            "  0.00000000e+00 4.29787412e-02 2.25130748e-02 6.45737499e-02\n",
            "  0.00000000e+00 1.01718269e-01 0.00000000e+00 1.05234683e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 3.48911397e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.27900192e-01\n",
            "  7.62430653e-02 1.52444933e-02 2.50548180e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 9.51593220e-02 0.00000000e+00\n",
            "  0.00000000e+00 2.80060377e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 9.22545977e-03 9.15536806e-02 0.00000000e+00\n",
            "  2.58685518e-02 5.87568246e-02 6.11412078e-02 8.42567384e-02\n",
            "  0.00000000e+00 0.00000000e+00 8.75585601e-02 0.00000000e+00\n",
            "  8.36828500e-02 1.91110596e-01 0.00000000e+00 2.06282306e-02\n",
            "  0.00000000e+00 2.32825819e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 6.97573945e-02 2.51195699e-01 1.06262013e-01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.55575991e-01 2.18157560e-01\n",
            "  1.04104653e-01 2.18213588e-01 7.97914341e-02 0.00000000e+00\n",
            "  3.26478891e-02 0.00000000e+00 0.00000000e+00 2.88748145e-02\n",
            "  0.00000000e+00 1.75271317e-01 2.43218674e-04 2.22357109e-01\n",
            "  1.90161809e-01 1.36711489e-04 1.04912795e-01 0.00000000e+00\n",
            "  3.14324379e-01 0.00000000e+00 3.18651319e-01 1.72352772e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "input shape=torch.Size([1, 121, 145, 1])\n",
            "1 torch.Size([1, 121, 145, 1]) OAS30720_MR_d0566\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_axial/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/85_conv1_71/\n",
            "torch.Size([1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.0336784, 0.11140381)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(4.318981, 0.6537103)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84985703 0.7527417  0.74611175 ... 0.7268853  0.7614306  0.75633585]\n",
            " [0.8163176  0.7215066  0.67593    ... 0.6812856  0.7248418  0.77615654]\n",
            " [0.8295602  0.74800485 0.72947025 ... 0.71286774 0.7689445  0.7629746 ]\n",
            " ...\n",
            " [0.80941457 0.73411226 0.71357703 ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.836408   0.7985303  0.7725011  ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.80180544 0.7531842  0.7957856  ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(3.2043233, 0.90619797)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.1972024  1.2260619  1.2053791  1.198151   1.1707255  1.1675193\n",
            "  1.1684035  1.16598    1.2117085  1.1861333  1.2182287  1.2078394\n",
            "  1.2101259  1.239967   1.2135402  1.208341   1.1870967  1.2198268\n",
            "  1.1622744 ]\n",
            " [1.1328202  1.1263182  1.1468667  1.1296847  1.1350652  1.1290869\n",
            "  1.1348386  1.1478139  1.1348671  1.2125934  1.3143476  1.3336862\n",
            "  1.3250006  1.2867192  1.2038239  1.1591953  1.0996511  1.1420438\n",
            "  1.1564238 ]\n",
            " [1.0455424  1.053748   1.0275189  1.0101174  1.0069205  1.020799\n",
            "  1.0638639  1.129651   1.2501096  1.4656501  1.6170243  1.6648406\n",
            "  1.6306688  1.6262516  1.2040573  1.1520743  1.0003145  1.0582258\n",
            "  1.1320341 ]\n",
            " [1.0586237  1.0205382  1.0246316  0.97177243 0.94393826 0.94576293\n",
            "  1.0000936  1.1246748  1.4944724  1.7821383  2.3883398  2.355186\n",
            "  2.4527645  1.7725505  1.4892668  1.1736636  0.956519   1.0206417\n",
            "  1.126921  ]\n",
            " [1.0604613  1.0310057  1.0542945  1.0037926  0.9688568  0.96524966\n",
            "  0.98099077 1.1560878  1.7425567  2.4536047  2.7063842  3.2043233\n",
            "  2.6349008  2.071519   1.4656657  1.1330028  0.9691913  1.0317733\n",
            "  1.114118  ]\n",
            " [1.0617698  1.0296324  1.0556631  1.0000571  0.98413306 0.9813531\n",
            "  1.01941    1.0957807  1.8218712  2.5024376  3.1910007  2.88087\n",
            "  2.457093   1.7369919  1.3477043  1.045822   0.9554977  1.0453043\n",
            "  1.105819  ]\n",
            " [1.0630329  1.0165797  1.0491253  1.0122591  1.0107402  1.035216\n",
            "  1.0369185  1.1179538  1.4872988  2.0027792  2.782516   2.3095887\n",
            "  2.0038533  1.5398194  1.2814797  1.063579   0.95673275 1.0444611\n",
            "  1.1054121 ]\n",
            " [1.0633091  1.0228876  1.0401849  0.99282604 0.9985802  1.1313618\n",
            "  1.2065467  1.2603776  1.669552   1.8276821  2.424319   2.090934\n",
            "  1.6775771  1.3039206  1.1451455  1.0007933  0.942976   1.0544701\n",
            "  1.109363  ]\n",
            " [1.062914   1.0273674  1.0453686  1.0129637  0.9799059  1.115173\n",
            "  1.4258527  1.5775359  1.3779675  1.3858763  1.6188039  1.5051371\n",
            "  1.3833872  1.1075581  1.039142   1.0003688  0.94113034 1.0516902\n",
            "  1.1121386 ]\n",
            " [1.0646476  1.023442   1.0079957  0.99036276 1.0273265  1.1875209\n",
            "  1.5614587  1.8269632  1.6662403  1.4683493  1.3477873  1.1567247\n",
            "  1.1354413  1.054871   0.9964082  0.9927726  0.93424624 1.0531663\n",
            "  1.1120007 ]\n",
            " [1.0621588  1.0277334  1.0483676  0.96578324 0.94886374 1.0705916\n",
            "  1.4649346  1.6529746  1.6326028  1.5634845  1.2312781  1.1937276\n",
            "  1.0235434  0.9975022  1.0014094  1.0051827  0.94575036 1.0524276\n",
            "  1.1111711 ]\n",
            " [1.0715724  1.0120448  1.0519441  0.96212125 0.9061981  1.1106676\n",
            "  1.175203   1.2979912  1.5214379  1.4569403  1.3634628  1.0756036\n",
            "  0.9756565  0.9840545  0.9652997  0.9869862  0.9455854  1.0631816\n",
            "  1.0991706 ]\n",
            " [1.0624408  1.0308715  1.0356385  1.0122824  0.9940069  1.0083137\n",
            "  1.1075904  1.1258923  1.2062545  1.2680589  1.1004378  1.0722417\n",
            "  0.97495925 1.001489   0.9852967  0.99514586 0.9400322  1.0489149\n",
            "  1.0906845 ]\n",
            " [1.0500067  1.0508487  1.0484303  1.0275847  0.9852313  1.0409014\n",
            "  1.0964233  1.2167507  1.1116742  1.1320105  1.1098684  1.0816003\n",
            "  1.043009   1.0229222  1.0132331  1.0083932  0.98757136 1.0721819\n",
            "  1.1073807 ]\n",
            " [1.0721623  1.085333   1.05819    1.0305564  1.0289602  1.0247813\n",
            "  1.0241886  1.0450453  1.0383102  1.0187716  1.0435523  1.0378985\n",
            "  1.026491   1.0322506  1.0378593  1.0324336  0.990744   1.1037242\n",
            "  1.137546  ]\n",
            " [1.0140742  1.0676385  1.0705048  1.0417092  1.0453414  1.028926\n",
            "  1.0332319  1.0478966  1.0388426  1.0437119  1.0370723  1.0424\n",
            "  1.0385841  1.0402738  1.0461155  1.0256196  1.0245582  1.0699049\n",
            "  1.0766865 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(2.9128256, 1.6320179)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.7577566 1.8742481 1.8646315 2.0050666 1.9959325 1.8968914 1.9897336\n",
            "  1.9157969 1.7626082 1.632018 ]\n",
            " [1.9198477 2.1702237 2.2094388 2.2420082 2.321875  2.5415626 2.376119\n",
            "  2.2820365 2.105906  1.819729 ]\n",
            " [1.9094776 2.2083058 2.1452408 2.4834707 2.756733  2.9128256 2.756998\n",
            "  2.1500409 2.0459754 1.7630489]\n",
            " [1.9260019 2.1313548 2.1482606 2.2661881 2.4835277 2.6497817 2.6965778\n",
            "  2.276061  2.1002092 1.8369685]\n",
            " [1.9424809 2.1619644 2.2146778 2.3462577 2.4914582 2.371521  2.3714178\n",
            "  2.2594848 2.1538773 1.788622 ]\n",
            " [1.9308909 2.0700288 2.2061844 2.3145964 2.3268657 2.2432098 2.0362775\n",
            "  2.0788112 2.0207486 1.7652191]\n",
            " [1.9496814 2.124174  2.1319208 2.1901362 2.3454652 2.237174  2.1342874\n",
            "  2.0919034 2.0989099 1.7909522]\n",
            " [1.7792622 1.8504055 1.8183943 1.8689591 1.8595318 1.8061117 1.7879704\n",
            "  1.7588631 1.8268391 1.6320504]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.2754205, 0.8363894)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8669603  0.9279082  0.92261624 0.9259098  0.83638954]\n",
            " [0.9728724  1.1770885  1.2385367  1.2556971  0.97077096]\n",
            " [0.94051075 1.1797926  1.2754207  1.2491808  1.0115739 ]\n",
            " [0.8485431  0.97451067 0.9633964  0.98458016 0.9105871 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.09703249 0.04191367 0.         0.02175793 0.         0.\n",
            "  0.01607765 0.         0.         0.         0.08361742 0.\n",
            "  0.1260888  0.15826768 0.0681463  0.16268821 0.11001267 0.\n",
            "  0.         0.07936549 0.31490532 0.         0.         0.05395253\n",
            "  0.         0.09216259 0.         0.         0.         0.00321393\n",
            "  0.01374977 0.         0.         0.         0.10479319 0.04324248\n",
            "  0.11555714 0.04334068 0.06375176 0.07703353 0.         0.03127411\n",
            "  0.06079936 0.0532323  0.         0.12126852 0.         0.10895627\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.02307694 0.         0.         0.         0.         0.19296654\n",
            "  0.05341744 0.02246291 0.06530777 0.         0.         0.\n",
            "  0.10173143 0.         0.         0.03702723 0.         0.\n",
            "  0.         0.         0.09035434 0.         0.03289216 0.05839747\n",
            "  0.06357659 0.08270027 0.         0.02621417 0.07331859 0.\n",
            "  0.03734786 0.20427766 0.         0.03855526 0.         0.\n",
            "  0.         0.         0.         0.1293361  0.2633875  0.08796629\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.20090212 0.1887057  0.10699406 0.20160618 0.06374975 0.\n",
            "  0.         0.         0.00853977 0.03746707 0.         0.2000026\n",
            "  0.0114668  0.23630467 0.19473822 0.00201735 0.11761274 0.\n",
            "  0.35249388 0.         0.27740902 0.         0.         0.\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_sagittal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/85_conv1_71/\n",
            "torch.Size([121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.0902761, 0.055962063)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(5.3774123, 0.65211606)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84988654 0.75263375 0.7457157  ... 0.7268969  0.76143646 0.75633776]\n",
            " [0.8165519  0.7228403  0.6741271  ... 0.681291   0.7248586  0.77615356]\n",
            " [0.8300017  0.74675345 0.73077667 ... 0.71276426 0.76888704 0.7629334 ]\n",
            " ...\n",
            " [0.80941504 0.7341158  0.7135726  ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.8364083  0.79853344 0.7725074  ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.795786   ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(3.837738, 0.9115936)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.2119898  1.2278355  1.2162732  1.2293206  1.2190276  1.3145093\n",
            "  1.2397616  1.2233062  1.2174411  1.2057551  1.1791652  1.1654259\n",
            "  1.1787322  1.1712712  1.1772255  1.1829021  1.1855235  1.2211351\n",
            "  1.1607057 ]\n",
            " [1.1842345  1.1213769  1.1940104  1.3303461  1.2889508  1.4447273\n",
            "  1.3306714  1.2937075  1.1792942  1.1179622  1.1209778  1.1255616\n",
            "  1.1167288  1.1226099  1.1261789  1.1088011  1.0791799  1.1408913\n",
            "  1.1557416 ]\n",
            " [1.0801102  1.0937859  1.1453468  1.4580513  1.6250926  1.6415867\n",
            "  1.6497219  1.3963095  1.312005   1.0818015  1.0316826  1.0066683\n",
            "  0.9909252  1.0100108  0.9923191  1.0190631  0.98112583 1.0664552\n",
            "  1.1388175 ]\n",
            " [1.093178   1.0571328  1.2770364  1.5577422  2.0112205  2.0398493\n",
            "  1.7695746  1.7664617  1.590474   1.1479135  1.0970488  0.99137604\n",
            "  0.97476363 0.948393   0.9512676  0.9590845  0.91159374 1.0201994\n",
            "  1.1247908 ]\n",
            " [1.0948539  1.1184665  1.2531333  1.8405896  2.0247035  2.689374\n",
            "  2.40783    2.130513   1.493761   1.2914914  1.156248   1.1187825\n",
            "  1.0146886  1.0413076  0.99433064 0.9987557  0.9622953  1.0504285\n",
            "  1.1216717 ]\n",
            " [1.0582823  1.1038057  1.1228929  1.463247   1.7458619  2.162707\n",
            "  2.0387845  2.1086483  1.7383444  1.4218065  1.4505543  1.2347261\n",
            "  1.1617445  1.0526149  1.0232118  0.9918228  0.9550582  1.0560015\n",
            "  1.1120611 ]\n",
            " [1.0510432  1.0182313  1.0240942  1.2166033  1.3989636  1.6706738\n",
            "  1.6647607  1.9029733  1.7768515  1.9103409  1.6883475  1.7600614\n",
            "  1.467805   1.3328768  1.1437699  1.0243921  0.9468397  1.0581821\n",
            "  1.1110255 ]\n",
            " [1.0483015  1.0267576  1.000338   1.0443305  1.1732707  1.3457353\n",
            "  1.4836701  1.5122386  1.9572428  2.2015343  2.4998918  2.333762\n",
            "  2.363164   1.6166979  1.3184539  1.0165524  0.9480716  1.0529307\n",
            "  1.1115435 ]\n",
            " [1.0680875  1.030722   1.0365751  1.0023743  1.0758272  1.2190696\n",
            "  1.3539723  1.2702249  1.9991916  2.8014622  3.1263068  3.3816056\n",
            "  2.5857089  1.997887   1.2880183  1.0437113  0.92852956 1.0477427\n",
            "  1.1124043 ]\n",
            " [1.0648525  1.0346764  1.009506   0.99521196 1.0521053  0.99848396\n",
            "  1.1396952  1.3434442  1.6041622  2.4877822  3.5927486  3.837738\n",
            "  2.9828744  1.9009143  1.3169402  1.0598735  0.93107677 1.0473274\n",
            "  1.1057085 ]\n",
            " [1.0640265  1.0306659  1.0165308  0.97032344 0.9888243  0.9992336\n",
            "  1.0536274  1.0783955  1.5088285  2.0731456  2.6129239  2.4502182\n",
            "  2.3689613  1.744361   1.1506408  1.0483168  0.9336     1.0463094\n",
            "  1.1088011 ]\n",
            " [1.0683421  1.0125643  1.0467294  0.96911895 0.96651673 0.95967007\n",
            "  0.970989   1.029547   1.184187   1.6236917  1.7590026  1.8290917\n",
            "  1.5916969  1.3706788  1.0838722  1.04649    0.9444626  1.0670729\n",
            "  1.09994   ]\n",
            " [1.0614406  1.0238189  1.0409275  0.9876351  0.98488766 0.97478056\n",
            "  0.99038994 1.0002061  1.1355546  1.2884537  1.2990226  1.2758001\n",
            "  1.1787721  1.1134435  1.0575386  1.0198661  0.93778867 1.0545832\n",
            "  1.0921764 ]\n",
            " [1.0520134  1.0497698  1.0465254  1.0479122  1.0273615  1.0384223\n",
            "  1.0342087  1.0597322  1.0475444  1.1324779  1.1483438  1.1369296\n",
            "  1.1024381  1.0727174  1.0207525  1.0035398  0.98366743 1.0711762\n",
            "  1.1074277 ]\n",
            " [1.0713071  1.0858312  1.06249    1.0324504  1.0222307  1.017094\n",
            "  1.0288848  1.056892   1.044237   0.99570566 1.0400399  1.0224988\n",
            "  1.0673167  1.0256385  1.034427   1.0406036  0.98630774 1.1032857\n",
            "  1.1369296 ]\n",
            " [1.014466   1.0682205  1.0666026  1.0452353  1.0474263  1.0444351\n",
            "  1.0395215  1.0399413  1.046477   1.0605589  1.042992   1.0500172\n",
            "  1.0496091  1.0519341  1.0429813  1.0277263  1.0264751  1.0686928\n",
            "  1.0756058 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(3.0926619, 1.6498656)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.8611966 2.029739  1.9013968 1.9548848 1.955666  1.8367115 1.7634233\n",
            "  1.761279  1.7763294 1.6498657]\n",
            " [1.9591006 2.3116374 2.4100387 2.4258838 2.3039083 2.189015  2.0770464\n",
            "  2.1735551 2.1661768 1.7381238]\n",
            " [1.9403399 2.481     2.6865058 2.7943757 2.4668715 2.350194  2.1450357\n",
            "  2.1721153 2.0210326 1.8132931]\n",
            " [1.9617816 2.3100874 2.50779   2.664988  2.620008  2.7081747 2.461491\n",
            "  2.2013462 2.1043386 1.845785 ]\n",
            " [1.955643  2.158461  2.270505  2.355813  2.8764424 3.0926619 2.6878402\n",
            "  2.326929  2.1053934 1.8084576]\n",
            " [1.9424342 2.0756683 2.0338194 2.2829084 2.5090203 2.9056315 2.6387482\n",
            "  2.2835262 2.0831141 1.8205411]\n",
            " [1.9623411 2.1338503 2.1288266 2.2573724 2.1848605 2.4387314 2.368232\n",
            "  2.2954574 2.1861773 1.8125228]\n",
            " [1.7706765 1.8182163 1.8173224 1.8324567 1.8851594 1.9228759 1.9415865\n",
            "  1.8155695 1.8209807 1.6599456]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.3560663, 0.8378246)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8378247  0.9388876  0.9644585  0.98042005 0.84498084]\n",
            " [0.953017   1.187236   1.2743338  1.200759   0.9837129 ]\n",
            " [0.97914404 1.2306968  1.3331381  1.3560665  0.97425336]\n",
            " [0.8565903  0.9828425  1.0170915  1.0098925  0.92785406]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.         0.00543284 0.         0.         0.         0.\n",
            "  0.         0.14125143 0.00441114 0.07652839 0.28750226 0.\n",
            "  0.         0.04375698 0.1259858  0.07730473 0.11051296 0.\n",
            "  0.01082031 0.11539742 0.15247852 0.         0.         0.\n",
            "  0.         0.         0.         0.05248671 0.         0.05742816\n",
            "  0.04488398 0.         0.         0.06310145 0.03060387 0.14077076\n",
            "  0.05006148 0.         0.04267997 0.08360607 0.         0.\n",
            "  0.04746029 0.26363412 0.         0.03533883 0.         0.03360565\n",
            "  0.02027299 0.         0.04356689 0.         0.04570433 0.\n",
            "  0.04322654 0.         0.         0.         0.         0.10258169\n",
            "  0.04186006 0.01186628 0.13519332 0.03014475 0.         0.\n",
            "  0.03166858 0.         0.         0.05555388 0.         0.\n",
            "  0.         0.         0.15225604 0.         0.0743285  0.02970438\n",
            "  0.11709276 0.12891227 0.         0.13249128 0.01226738 0.\n",
            "  0.04974239 0.21711853 0.         0.07325292 0.01136213 0.\n",
            "  0.         0.         0.         0.         0.22282857 0.08968205\n",
            "  0.         0.06473022 0.         0.         0.         0.\n",
            "  0.08859464 0.13944642 0.11155979 0.2720348  0.14694802 0.\n",
            "  0.14984018 0.         0.00967101 0.         0.         0.06786849\n",
            "  0.01233813 0.22061937 0.29762244 0.         0.05532943 0.\n",
            "  0.2562582  0.         0.30672732 0.01355422 0.         0.03330068\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_coronal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/85_conv1_71/\n",
            "torch.Size([1, 1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(2.019126, 0.08694858)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(7.1362476, 0.6617715)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84979737 0.7526888  0.74612427 ... 0.72687113 0.76138055 0.75632787]\n",
            " [0.81638527 0.72160316 0.6762201  ... 0.6811132  0.7248088  0.7761191 ]\n",
            " [0.8295173  0.7476053  0.72965854 ... 0.7127009  0.76889    0.762935  ]\n",
            " ...\n",
            " [0.8094154  0.73411715 0.71357626 ... 0.71494687 0.75887144 0.73765373]\n",
            " [0.8364083  0.7985333  0.77250004 ... 0.78179693 0.81395    0.7831061 ]\n",
            " [0.8018056  0.75318444 0.79578596 ... 0.79865414 0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(5.789525, 0.9185776)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.1978016  1.2206503  1.200562   1.1912454  1.1703416  1.1732628\n",
            "  1.1758904  1.173974   1.1722698  1.1732914  1.1743526  1.1728703\n",
            "  1.1734095  1.1721443  1.1775111  1.183349   1.1859018  1.2208073\n",
            "  1.1609648 ]\n",
            " [1.1325315  1.1191456  1.1279818  1.1237649  1.128261   1.124653\n",
            "  1.1069609  1.1117228  1.1264304  1.1162125  1.1166912  1.1179432\n",
            "  1.1198425  1.1219519  1.126299   1.1115922  1.0820612  1.1417679\n",
            "  1.1572168 ]\n",
            " [1.0581383  1.0674807  1.0286561  1.0078983  1.0096084  0.99371386\n",
            "  0.99749947 1.0143157  0.9945917  1.0090468  1.0104469  0.99844533\n",
            "  0.9973255  0.9970151  0.99661726 1.0174726  0.9804654  1.0678185\n",
            "  1.1381487 ]\n",
            " [1.0684978  1.0693396  1.0537548  1.0617348  1.074567   1.0367614\n",
            "  1.0377423  0.98655105 0.98550713 0.97098875 0.9561963  0.9546737\n",
            "  0.96175975 0.9540757  0.9581497  0.9656013  0.91857773 1.0140885\n",
            "  1.1221514 ]\n",
            " [1.1814936  1.2667812  1.3523325  1.4630443  1.4468102  1.3474096\n",
            "  1.2214717  1.1625985  1.0640934  1.0572406  1.0162761  1.0588502\n",
            "  1.0057306  1.012693   0.9945437  1.0064919  0.9790391  1.0602773\n",
            "  1.1334275 ]\n",
            " [1.3603493  1.776034   2.1502457  2.2147079  2.214112   2.0590744\n",
            "  1.668363   1.4254624  1.2639991  1.1487035  1.0931004  1.1486516\n",
            "  1.1267059  1.164492   1.0964738  1.074066   0.9451308  1.0801195\n",
            "  1.10503   ]\n",
            " [1.7616903  2.5580041  3.1120381  3.5910277  3.078933   2.9727547\n",
            "  2.28266    1.7907004  1.5931584  1.4930154  1.3962227  1.5228055\n",
            "  1.606751   1.5353631  1.5092758  1.3070393  1.1078777  1.0803\n",
            "  1.1270728 ]\n",
            " [2.0021684  3.2057238  3.9949756  4.9784927  4.5160866  3.4438415\n",
            "  2.8402438  2.1056812  1.6952441  1.749189   1.6987432  2.0532966\n",
            "  2.3778212  2.403754   2.0308022  1.6828535  1.2578567  1.1788946\n",
            "  1.1363367 ]\n",
            " [1.9938823  3.2037826  5.1821012  5.789525   4.997313   3.9711432\n",
            "  2.8389132  2.3738976  1.916015   2.0069122  2.144689   2.7249336\n",
            "  3.1986923  3.4222078  2.382771   1.9768785  1.378964   1.2808805\n",
            "  1.214589  ]\n",
            " [1.6422974  2.802566   4.089443   5.1870537  4.843611   3.698217\n",
            "  2.7498364  2.0594673  2.131138   2.0805151  2.2205253  2.5960217\n",
            "  3.8455153  3.9691715  3.0180707  2.2806098  1.4059138  1.1929902\n",
            "  1.1010574 ]\n",
            " [1.3269042  2.1118436  2.9110174  3.6270542  3.926489   3.455208\n",
            "  2.881216   2.0168626  1.8401964  1.5145398  1.8582007  2.2002401\n",
            "  2.6965308  3.0190368  2.5337164  1.9084959  1.3540128  1.1975363\n",
            "  1.0849516 ]\n",
            " [1.2182273  1.5037094  2.1578758  2.3872676  2.9199219  3.8549376\n",
            "  3.4374905  2.2698374  1.7730371  1.3155562  1.4404665  1.5710446\n",
            "  1.8642006  2.053865   1.8687191  1.6272614  1.1383487  1.1854261\n",
            "  1.0735496 ]\n",
            " [1.0326375  1.1923112  1.4308289  1.7140192  2.015666   2.3626165\n",
            "  2.4852877  1.9107757  1.399268   1.1534996  1.1368906  1.2076929\n",
            "  1.4277543  1.3310639  1.3344954  1.1804639  1.06196    1.0569998\n",
            "  1.0961012 ]\n",
            " [1.031988   1.0665715  1.1490372  1.292398   1.5275255  1.7810897\n",
            "  1.7105577  1.4751593  1.3436438  1.1937528  1.0458403  1.0539361\n",
            "  1.0867568  1.0933985  1.0954144  1.0535661  1.0193139  1.0543218\n",
            "  1.1318696 ]\n",
            " [1.0685712  1.0931417  1.0920674  1.0777138  1.1513964  1.2706114\n",
            "  1.2305402  1.2748064  1.1060817  1.0306958  1.0279094  1.0545198\n",
            "  1.0043733  1.0274934  1.039486   1.0500304  0.99985194 1.1169435\n",
            "  1.1370058 ]\n",
            " [1.0141999  1.070757   1.0436774  1.0419673  1.0584384  1.0870827\n",
            "  1.0944709  1.088869   1.0910338  1.0490977  1.0527946  1.0470837\n",
            "  1.0578884  1.0535933  1.0618169  1.0326813  1.0232784  1.066085\n",
            "  1.0747474 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(4.957235, 1.6317672)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.8306168 1.8862084 1.8347768 1.8364967 1.8518938 1.7956692 1.7799369\n",
            "  1.7275676 1.783908  1.6317673]\n",
            " [2.3811839 2.554484  2.5631187 2.3601987 2.2675335 2.23599   2.060908\n",
            "  2.1691024 2.0514872 1.7486554]\n",
            " [2.8368912 3.1685266 3.0078142 2.8286502 2.3907278 2.326602  2.2669861\n",
            "  2.1938205 2.078588  1.8036718]\n",
            " [3.4612389 4.452493  3.9196997 3.4333658 2.8197455 2.7773347 2.6758485\n",
            "  2.5246615 2.1423664 1.7956015]\n",
            " [3.6374383 4.957235  4.7627635 3.8509583 3.1364713 3.0343957 3.2716458\n",
            "  2.725565  2.3662758 1.8574207]\n",
            " [2.9420238 4.3686895 4.17672   3.7981396 3.1437044 2.8986278 3.0853953\n",
            "  2.6795545 2.1238618 1.8147904]\n",
            " [2.4910667 3.0100904 3.144763  3.1190479 2.7542129 2.500227  2.3808584\n",
            "  2.280789  2.199899  1.7966548]\n",
            " [2.0244198 2.2272768 2.2052546 2.270996  1.9980607 1.9315358 1.9892945\n",
            "  1.9163098 1.8074728 1.653195 ]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.5603666, 0.8583391)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.87800777 0.976576   0.9771499  0.96199507 0.8583392 ]\n",
            " [0.9849194  1.2772442  1.242095   1.2583586  0.9340682 ]\n",
            " [1.2164549  1.5603667  1.3312436  1.3943144  0.97897667]\n",
            " [0.970284   1.136658   1.0836515  0.99660206 0.8977639 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.22223727 0.         0.         0.0566635  0.         0.0473427\n",
            "  0.05978377 0.         0.         0.         0.0290077  0.\n",
            "  0.05985281 0.21395846 0.00040214 0.19054845 0.07821874 0.\n",
            "  0.         0.08461186 0.33563986 0.         0.         0.1595714\n",
            "  0.         0.17049256 0.         0.         0.         0.\n",
            "  0.02599101 0.02017208 0.05793681 0.         0.10274021 0.\n",
            "  0.17946164 0.04970603 0.08008983 0.         0.         0.05820419\n",
            "  0.         0.         0.         0.15218355 0.         0.2004187\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.03866544 0.         0.         0.         0.         0.21578224\n",
            "  0.07065607 0.00926822 0.         0.         0.02792259 0.\n",
            "  0.1252918  0.         0.         0.         0.         0.\n",
            "  0.         0.08838221 0.05643393 0.         0.         0.05892032\n",
            "  0.         0.06904717 0.         0.         0.13134427 0.\n",
            "  0.06396221 0.13761772 0.         0.         0.         0.01315348\n",
            "  0.         0.         0.04577548 0.20242754 0.19232714 0.11650668\n",
            "  0.         0.0059658  0.         0.         0.         0.\n",
            "  0.19566949 0.18300089 0.02210226 0.10623056 0.045304   0.\n",
            "  0.         0.         0.         0.11299548 0.02323836 0.24184345\n",
            "  0.03238899 0.17853835 0.         0.         0.15508027 0.\n",
            "  0.27530098 0.         0.19276507 0.03891292 0.         0.\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "input shape=torch.Size([1, 121, 145, 1])\n",
            "1 torch.Size([1, 121, 145, 1]) OAS30720_MR_d0566\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_axial/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/88_conv1_71/\n",
            "torch.Size([1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.3872812, 0.0935139)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(4.914713, 0.66180766)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84985703 0.7527417  0.74611175 ... 0.7268853  0.7614306  0.75633585]\n",
            " [0.8163176  0.7215066  0.67593    ... 0.6812856  0.7248418  0.77615654]\n",
            " [0.8295602  0.74800485 0.72947025 ... 0.71286774 0.7689445  0.7629746 ]\n",
            " ...\n",
            " [0.8094154  0.73411703 0.71358347 ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.8364084  0.7985334  0.7725074  ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.7957858  ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(3.5931149, 0.9247719)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.196966   1.2262058  1.2045838  1.1941327  1.1711744  1.1882112\n",
            "  1.1738299  1.1875736  1.1791738  1.2139641  1.232868   1.2186913\n",
            "  1.208186   1.2097067  1.2063311  1.1821908  1.1804587  1.2240272\n",
            "  1.1612259 ]\n",
            " [1.1333375  1.1252239  1.1400117  1.1295698  1.1393028  1.1568476\n",
            "  1.1460372  1.1245935  1.225376   1.3077961  1.283804   1.3276274\n",
            "  1.213063   1.2021898  1.1308193  1.1556693  1.0964812  1.1469294\n",
            "  1.1554835 ]\n",
            " [1.0456275  1.0540125  1.0249673  1.0120589  0.9989513  1.0300692\n",
            "  1.0885875  1.0963783  1.3173887  1.6758982  1.6736189  1.821967\n",
            "  1.5111908  1.301819   1.0887104  1.0842565  0.98325723 1.0536385\n",
            "  1.1364843 ]\n",
            " [1.0590686  1.0192463  1.0288554  0.9828743  0.9519129  0.9539951\n",
            "  1.0101069  1.3058168  1.7118685  2.1397257  2.4077067  2.136548\n",
            "  2.0794322  1.7459956  1.3213183  1.0320749  0.924772   1.0196348\n",
            "  1.1269194 ]\n",
            " [1.0607574  1.0341543  1.0580012  1.0186706  1.0004553  0.98936236\n",
            "  1.0573362  1.2701979  2.1088674  2.9202228  3.3725004  2.9957364\n",
            "  2.4599166  1.6665314  1.2853451  1.0135478  0.9280132  1.0480024\n",
            "  1.1183499 ]\n",
            " [1.0647362  1.0244849  1.0673234  0.9987117  0.99999595 0.9897667\n",
            "  1.025825   1.1403219  1.8864323  2.8095512  3.5931149  3.0931273\n",
            "  2.5025854  1.6991928  1.3971249  1.0666386  0.9423586  1.0431024\n",
            "  1.1018926 ]\n",
            " [1.0633854  1.0196983  1.045792   0.9987318  1.0551025  1.052264\n",
            "  1.2589488  1.3791469  1.6543751  2.3256717  2.9482844  2.4284604\n",
            "  2.0052125  1.5292542  1.2201346  0.9801789  0.9526187  1.040763\n",
            "  1.1042767 ]\n",
            " [1.0645107  1.0279379  1.0479056  0.9780209  1.0606767  1.2045102\n",
            "  1.4328924  1.5581328  1.9060922  1.9611108  2.2466216  1.7785397\n",
            "  1.559019   1.3182157  1.1235136  0.9974325  0.9386629  1.0550611\n",
            "  1.1088634 ]\n",
            " [1.0645123  1.0238783  1.0517528  0.95318097 1.0216583  1.4359444\n",
            "  1.8332714  2.2200522  1.9089711  1.9374002  1.6269869  1.3532051\n",
            "  1.2707871  1.0696105  1.0572602  0.9885457  0.940254   1.0540015\n",
            "  1.110847  ]\n",
            " [1.0589842  1.0303029  1.0200663  0.97816354 1.0859137  1.4822825\n",
            "  2.2420335  2.327746   2.0510159  1.6741008  1.2854818  1.1438097\n",
            "  1.0653191  1.0795672  0.96788895 0.9908175  0.93371344 1.0512925\n",
            "  1.1129776 ]\n",
            " [1.0647272  1.0292081  1.0243213  0.96036065 0.9942784  1.2846752\n",
            "  1.7701095  2.2323875  1.9434979  1.7086654  1.2814695  1.1302708\n",
            "  1.0585426  1.0370704  1.0020812  0.99294776 0.93293107 1.0487041\n",
            "  1.1123773 ]\n",
            " [1.0686634  1.0198172  1.0614116  0.9583357  0.9922948  1.2006713\n",
            "  1.4149848  1.5858425  1.5844245  1.4745334  1.3027778  1.1243132\n",
            "  1.0072261  1.0257231  0.98103553 0.9803725  0.9357952  1.0611732\n",
            "  1.0991513 ]\n",
            " [1.0604293  1.0359505  1.0520691  0.9933377  0.98501694 1.0522476\n",
            "  1.15637    1.1685783  1.3485681  1.2482284  1.1421233  1.0638986\n",
            "  1.0010979  0.99725384 0.9912808  0.9917269  0.9442773  1.0483845\n",
            "  1.0910113 ]\n",
            " [1.0517801  1.0504788  1.0481278  1.047669   1.0092748  1.0206169\n",
            "  1.0718479  1.1672288  1.1538216  1.1556835  1.0820509  1.1066762\n",
            "  1.0835391  1.0476202  0.9967485  0.99822307 0.9888778  1.0732726\n",
            "  1.106697  ]\n",
            " [1.07226    1.0813895  1.0582877  1.0263251  1.0272461  1.0465187\n",
            "  1.0240654  1.0375912  1.0491879  0.97971565 1.0734159  1.0240654\n",
            "  1.0245296  1.0169663  1.0278417  1.0328332  0.99109745 1.1050276\n",
            "  1.1378057 ]\n",
            " [1.0140429  1.0691653  1.0667127  1.0362982  1.0389405  1.0327502\n",
            "  1.0323187  1.0674795  1.0517098  1.0254678  1.0485588  1.0509921\n",
            "  1.0372974  1.0471529  1.0467186  1.0270667  1.0244231  1.0698001\n",
            "  1.0769438 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(3.238124, 1.6401869)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.7584652 1.8571566 1.910209  1.9769222 2.0398633 1.8441631 1.9568272\n",
            "  1.7952629 1.7565396 1.6624565]\n",
            " [1.9112016 2.1738749 2.1724086 2.4138398 2.5613527 2.7077613 2.3389187\n",
            "  2.1331425 2.185173  1.7996918]\n",
            " [1.9323591 2.2202096 2.2627423 2.4843893 2.885147  3.238124  2.4711633\n",
            "  2.2578804 2.1091063 1.7882224]\n",
            " [1.9511887 2.1374106 2.2894106 2.4491816 2.767771  3.0147212 2.6188424\n",
            "  2.247405  2.0788398 1.8311713]\n",
            " [1.9140432 2.2032254 2.2860284 2.5929615 2.5653203 2.507887  2.3251424\n",
            "  2.1834316 2.1201234 1.8017939]\n",
            " [1.905287  2.1549919 2.1816967 2.570365  2.4401822 2.111886  1.994073\n",
            "  1.9831669 2.0594094 1.8102211]\n",
            " [1.9594109 2.149691  2.2052526 2.2564268 2.3208323 2.2750788 2.174205\n",
            "  2.1675138 2.136868  1.8098449]\n",
            " [1.7941571 1.8869616 1.8205237 1.8890702 1.9108778 1.8227092 1.8608993\n",
            "  1.7575828 1.8488451 1.640187 ]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.3219775, 0.83625233)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8490073  0.91461074 0.9269265  0.9562162  0.83625245]\n",
            " [0.9647294  1.1477495  1.2271051  1.1608077  0.9710908 ]\n",
            " [0.94971156 1.1962734  1.3219776  1.2432984  1.0079829 ]\n",
            " [0.84400517 1.0404366  0.9640312  0.98186433 0.9109621 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.11426789 0.04291895 0.         0.02123752 0.         0.\n",
            "  0.03057523 0.         0.         0.         0.09465595 0.\n",
            "  0.12852299 0.14893363 0.0641177  0.16045317 0.12154973 0.\n",
            "  0.         0.06920201 0.32717645 0.         0.         0.06160027\n",
            "  0.         0.0939373  0.         0.         0.         0.00980582\n",
            "  0.01049516 0.0016775  0.         0.         0.10419738 0.03892674\n",
            "  0.12464858 0.04362665 0.06083557 0.06386417 0.         0.02445173\n",
            "  0.05277391 0.04767036 0.         0.11508895 0.         0.09044203\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.00867348 0.         0.         0.         0.         0.19604442\n",
            "  0.05905096 0.01308589 0.0509568  0.         0.         0.\n",
            "  0.10223833 0.         0.         0.02569404 0.         0.\n",
            "  0.         0.         0.09431349 0.         0.02921338 0.06126514\n",
            "  0.06015464 0.08725924 0.         0.01513855 0.09083762 0.\n",
            "  0.02998954 0.20377351 0.         0.02592033 0.         0.\n",
            "  0.         0.         0.         0.12446183 0.2669951  0.07837094\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.20584083 0.17921457 0.09470671 0.206272   0.07219354 0.\n",
            "  0.         0.         0.         0.03088909 0.         0.21294998\n",
            "  0.00314378 0.2343256  0.19601472 0.01176113 0.10414784 0.\n",
            "  0.34251952 0.         0.27896366 0.         0.         0.\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_sagittal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/88_conv1_71/\n",
            "torch.Size([121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.0111325, 0.054043654)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(3.7251396, 0.6582149)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8499019  0.7532073  0.74396044 ... 0.7268858  0.7614308  0.7563359 ]\n",
            " [0.8166277  0.72236776 0.6726904  ... 0.6812868  0.7248435  0.7761563 ]\n",
            " [0.82959425 0.7467183  0.72935617 ... 0.71286714 0.7689369  0.7629706 ]\n",
            " ...\n",
            " [0.8094154  0.7341168  0.71358323 ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.83640826 0.79853344 0.7725078  ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.795786   ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(2.817891, 0.91161245)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.2125303  1.2338432  1.2492868  1.2622703  1.2835954  1.3022565\n",
            "  1.3008126  1.3012291  1.222668   1.1953235  1.1863431  1.1678067\n",
            "  1.175275   1.1712569  1.1776187  1.1831865  1.1855251  1.2212298\n",
            "  1.160498  ]\n",
            " [1.1679484  1.1321203  1.238449   1.4632593  1.6213142  1.5540644\n",
            "  1.4026263  1.2906039  1.2397805  1.1270899  1.1365806  1.11249\n",
            "  1.1182278  1.1189786  1.1251348  1.1088586  1.0809478  1.1414634\n",
            "  1.1556028 ]\n",
            " [1.0453237  1.1405793  1.2666935  1.5282658  1.8292831  1.9769763\n",
            "  1.9336166  1.5256556  1.3629072  1.1660413  1.0462818  1.0340371\n",
            "  1.0108463  1.0076476  0.99893    1.016945   0.9825313  1.067502\n",
            "  1.1389551 ]\n",
            " [1.0754496  1.045937   1.3180606  1.8719565  2.5541804  2.2472062\n",
            "  2.1530585  1.9578013  1.5801295  1.2558017  1.056824   1.0079626\n",
            "  0.993819   0.95746714 0.9644505  0.959074   0.9116126  1.0233175\n",
            "  1.1248986 ]\n",
            " [1.0795423  1.1320723  1.3509957  1.8694838  2.4973004  2.817891\n",
            "  2.6691873  2.4258504  1.7758785  1.4753271  1.2912115  1.1678039\n",
            "  1.0519698  1.013139   0.9965378  0.9918697  0.9543917  1.0533046\n",
            "  1.1212144 ]\n",
            " [1.0211029  1.0903108  1.1807638  1.4571873  1.9259596  2.2097826\n",
            "  2.3522668  1.9745075  1.8494743  1.7872204  1.7545389  1.3177096\n",
            "  1.143457   1.0087589  0.9910867  0.98405653 0.94656307 1.057868\n",
            "  1.1139661 ]\n",
            " [1.0454152  1.0752239  0.9958266  1.2245175  1.4713527  1.7967972\n",
            "  1.9538532  2.194354   2.2981987  2.39639    1.7695855  1.669814\n",
            "  1.1813605  1.0440481  1.0058792  0.9894766  0.93962276 1.0539955\n",
            "  1.1133367 ]\n",
            " [1.0512404  1.0477034  1.0010736  1.0279943  1.3339871  1.6116604\n",
            "  1.8410515  1.9871036  2.672336   2.5097315  2.0680583  1.4949509\n",
            "  1.2164886  1.073929   0.9927976  0.996783   0.94409233 1.0547189\n",
            "  1.1127053 ]\n",
            " [1.0722752  1.0489734  1.0697368  0.97720206 1.0629139  1.3392304\n",
            "  1.6864835  1.7819749  1.8631135  2.1253183  1.8256732  1.5485352\n",
            "  1.2284548  1.0915036  1.0124288  0.97355276 0.9451827  1.0533254\n",
            "  1.1123152 ]\n",
            " [1.0640279  1.0410541  1.0285395  0.9856656  1.0754303  1.2223243\n",
            "  1.3587649  1.4118079  1.5837017  1.7015424  1.5856199  1.4688224\n",
            "  1.208351   1.0491818  0.9893694  0.97124195 0.94022393 1.0581545\n",
            "  1.1125661 ]\n",
            " [1.0646108  1.0321965  1.0528233  1.0104007  1.0321511  1.0075152\n",
            "  1.073366   1.167626   1.3539529  1.3467637  1.3726219  1.2912556\n",
            "  1.1916121  1.0595323  1.0021096  0.9981967  0.93795896 1.049986\n",
            "  1.1127105 ]\n",
            " [1.0719482  1.0190474  1.0303384  0.96943057 0.99999243 1.0363711\n",
            "  1.0020002  1.0958763  1.1982112  1.2961161  1.2115866  1.1096\n",
            "  1.0825831  0.9839226  0.9736177  0.9866085  0.9339441  1.0652696\n",
            "  1.1002994 ]\n",
            " [1.059117   1.0257976  1.0424179  0.9881011  0.9787476  0.95806205\n",
            "  1.0025556  1.0174075  1.1248059  1.116253   1.1414342  1.0249959\n",
            "  1.0530455  0.9869201  0.98107296 0.9779669  0.9350021  1.0507325\n",
            "  1.0903772 ]\n",
            " [1.0524577  1.0524915  1.0429312  1.0223237  1.0220231  1.0288893\n",
            "  1.0276471  1.0587671  1.1032526  1.1179243  1.1147918  1.0449438\n",
            "  1.0897105  1.0090698  1.0261267  1.0057014  0.98623323 1.072126\n",
            "  1.1073083 ]\n",
            " [1.0727298  1.0820059  1.0586255  1.0379022  1.0274338  1.0291921\n",
            "  1.0315468  1.0741676  1.0493208  1.0454104  1.0434463  1.0422326\n",
            "  1.0344659  1.0228952  1.030004   1.0354565  0.9931331  1.1030993\n",
            "  1.1374668 ]\n",
            " [1.0147842  1.0663573  1.0685925  1.041021   1.0482943  1.0387756\n",
            "  1.040598   1.0469786  1.0599598  1.052401   1.0515821  1.0540525\n",
            "  1.054407   1.0301493  1.042953   1.0262927  1.0257225  1.0695623\n",
            "  1.0766944 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(2.8744059, 1.6456352)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.9155269 2.1130905 2.049828  2.0373983 1.9222578 1.8103851 1.753049\n",
            "  1.7663985 1.8016075 1.6573766]\n",
            " [2.0809288 2.49227   2.636764  2.605699  2.2744637 2.121578  1.9873614\n",
            "  2.155145  2.0997581 1.7515182]\n",
            " [2.021824  2.6132612 2.7916    2.8744059 2.5873983 2.2698703 2.0493226\n",
            "  2.0954022 2.0722904 1.7934746]\n",
            " [1.9542053 2.38442   2.4506102 2.6235166 2.6320589 2.302485  2.0354676\n",
            "  2.103919  2.0765507 1.8019023]\n",
            " [2.0247774 2.2685714 2.4228525 2.5573075 2.6896725 2.4630992 2.1008399\n",
            "  2.186604  2.107749  1.8022679]\n",
            " [1.9316369 2.068573  2.107048  2.219013  2.3648946 2.315556  2.1572921\n",
            "  2.0309417 2.053298  1.7793635]\n",
            " [1.9784108 2.1660104 2.150628  2.2012181 2.235413  2.2822957 2.1354063\n",
            "  2.1501482 2.142201  1.7856942]\n",
            " [1.7698709 1.8594356 1.8209891 1.8100132 1.8636945 1.8045708 1.8170289\n",
            "  1.7880679 1.8446162 1.6456354]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.4217675, 0.8356956)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.85359895 0.96436423 0.96130455 0.91954714 0.83569574]\n",
            " [0.9722681  1.3006691  1.3269717  1.1734481  0.9521351 ]\n",
            " [0.9830632  1.2594507  1.4217676  1.2711414  0.98284626]\n",
            " [0.8486669  1.0185925  1.0197903  0.98807347 0.9189507 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.         0.01268227 0.         0.         0.         0.\n",
            "  0.         0.14475186 0.         0.06899793 0.2994097  0.\n",
            "  0.         0.03501798 0.13748173 0.06415501 0.10891201 0.\n",
            "  0.0033222  0.09629938 0.15652671 0.         0.         0.\n",
            "  0.         0.         0.         0.04132767 0.         0.04362325\n",
            "  0.03910071 0.         0.         0.05261704 0.05296636 0.13568379\n",
            "  0.06322293 0.00828434 0.04284547 0.09123611 0.         0.\n",
            "  0.04782629 0.27046153 0.         0.0430736  0.         0.04216327\n",
            "  0.00554279 0.         0.04526637 0.         0.05089111 0.\n",
            "  0.0562031  0.008664   0.         0.         0.         0.11362457\n",
            "  0.0351171  0.0163867  0.14789923 0.03328162 0.         0.\n",
            "  0.03711328 0.00042245 0.         0.05578297 0.         0.\n",
            "  0.         0.         0.14369707 0.         0.09601217 0.03243791\n",
            "  0.11541089 0.14565039 0.         0.12851019 0.         0.\n",
            "  0.03375444 0.22791147 0.         0.05621667 0.         0.\n",
            "  0.         0.         0.         0.         0.20976344 0.0922366\n",
            "  0.         0.06150416 0.         0.         0.         0.\n",
            "  0.08725686 0.11621754 0.10906954 0.26693815 0.13874955 0.\n",
            "  0.1415282  0.         0.00475444 0.         0.         0.07705955\n",
            "  0.01297014 0.2239664  0.30759907 0.         0.04230589 0.\n",
            "  0.25604987 0.         0.2913861  0.00748723 0.         0.03325114\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_coronal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/88_conv1_71/\n",
            "torch.Size([1, 1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(2.0212517, 0.085703075)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(6.8147416, 0.66230834)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84983444 0.75269175 0.7459904  ... 0.7269398  0.7612111  0.75624996]\n",
            " [0.8162945  0.7215698  0.6761626  ... 0.6794081  0.7242878  0.7755125 ]\n",
            " [0.8295816  0.74782306 0.7289232  ... 0.7107532  0.7682286  0.76239944]\n",
            " ...\n",
            " [0.8094154  0.73411715 0.7135783  ... 0.7149313  0.7588713  0.7376536 ]\n",
            " [0.8364083  0.79853344 0.7725086  ... 0.7817973  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.79578596 ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(5.482646, 0.9074628)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.1981745  1.2198647  1.1987807  1.191698   1.1718119  1.1732216\n",
            "  1.1746548  1.1738994  1.1739454  1.1743892  1.1744978  1.1737984\n",
            "  1.173309   1.1703548  1.1789109  1.1881095  1.1886649  1.2254602\n",
            "  1.161774  ]\n",
            " [1.1340528  1.1107539  1.1346202  1.1152271  1.1231254  1.1276462\n",
            "  1.1115729  1.1151882  1.1181877  1.1170185  1.1178538  1.1198335\n",
            "  1.1185815  1.1181263  1.1236734  1.1115679  1.0726875  1.1350511\n",
            "  1.1624913 ]\n",
            " [1.0634003  1.0671283  1.0065143  1.0078716  0.9837278  0.98001415\n",
            "  1.0062647  1.014689   1.0139347  1.0130097  1.0082546  1.0022676\n",
            "  0.99992853 0.9952555  0.9928302  1.0178225  0.9688175  1.0621904\n",
            "  1.1354762 ]\n",
            " [1.0717839  1.0597018  1.1073412  1.0373789  1.0689837  1.0376323\n",
            "  1.0698168  0.9849037  0.9810092  0.96987873 0.9660661  0.96043885\n",
            "  0.96766585 0.96424955 0.9609426  0.960685   0.9074629  1.0146091\n",
            "  1.1311575 ]\n",
            " [1.1422843  1.1332203  1.2669132  1.4257274  1.4232726  1.4279774\n",
            "  1.3126658  1.1235319  1.0702708  1.0283715  1.0040444  1.0404046\n",
            "  0.9962722  1.0067271  1.0018924  1.0006396  0.96623904 1.0521721\n",
            "  1.1207795 ]\n",
            " [1.2358993  1.5069681  2.0005062  2.1033552  2.2193854  2.0144796\n",
            "  1.6304215  1.3981961  1.2036573  1.0716692  1.0074179  1.0217053\n",
            "  1.0098144  1.0158454  1.0119821  1.0100434  0.9650241  1.0473158\n",
            "  1.1152503 ]\n",
            " [1.4583088  1.9632965  2.7031505  3.145369   3.1638505  2.5100925\n",
            "  2.3994505  1.6699551  1.4183882  1.2319121  1.1794068  1.1338711\n",
            "  1.077761   1.0968952  0.9701725  1.0224434  0.96501213 1.0633866\n",
            "  1.1194456 ]\n",
            " [1.4505733  2.284462   3.373095   4.5494947  4.4394608  3.6208282\n",
            "  2.8104317  1.7452852  1.4197723  1.3218937  1.3542778  1.3373847\n",
            "  1.2999853  1.2036996  1.1784551  1.0548428  0.9845901  1.0708073\n",
            "  1.0913707 ]\n",
            " [1.3223292  2.5558004  3.8456106  4.7031674  5.482646   4.2728252\n",
            "  3.1597917  2.1019974  1.5768346  1.6210877  1.6407684  1.560102\n",
            "  1.578172   1.5564829  1.3259242  1.2485555  1.0068892  1.1021711\n",
            "  1.1307131 ]\n",
            " [1.2676226  2.0071154  2.9670286  4.4295187  4.485012   3.9313064\n",
            "  3.1340144  1.9512328  1.6105311  1.4753288  1.6868205  1.4909174\n",
            "  1.7829798  1.6870955  1.3474694  1.099366   1.0671645  1.0976921\n",
            "  1.0753359 ]\n",
            " [1.1913811  1.7218548  2.3686404  2.8608768  3.4844174  3.3602824\n",
            "  2.7022274  2.0989804  1.6755924  1.1978384  1.28974    1.3252913\n",
            "  1.4292597  1.5635375  1.3069698  1.1949297  1.0091738  1.0965801\n",
            "  1.1174692 ]\n",
            " [1.1438142  1.3307964  1.7272826  2.0609348  2.6059237  3.5101557\n",
            "  3.029127   1.9400609  1.4380323  1.0371     1.1383182  1.184148\n",
            "  1.2640482  1.2024835  1.1809675  1.0790462  0.96177375 1.0807583\n",
            "  1.0872699 ]\n",
            " [1.0890213  1.0933574  1.2579074  1.5861055  1.9008988  2.1306043\n",
            "  2.1100202  1.7596686  1.2232518  1.0842932  0.95620346 1.0665447\n",
            "  1.0821487  1.047824   1.0589     1.0032034  0.9622647  1.026374\n",
            "  1.0773394 ]\n",
            " [1.0333759  1.0944666  1.1218477  1.281785   1.4644521  1.545803\n",
            "  1.4895645  1.3499941  1.2386109  1.0496246  1.0515589  1.0635463\n",
            "  1.0734619  1.0141     1.0128982  1.0351106  0.9886049  1.0769079\n",
            "  1.1132901 ]\n",
            " [1.0683719  1.097752   1.0644511  1.0467521  1.1736678  1.1730444\n",
            "  1.1394858  1.2096349  1.0595297  1.0315562  1.0343089  1.0155467\n",
            "  1.0294958  1.0117402  1.0276213  1.0410392  0.9898827  1.1059808\n",
            "  1.1361238 ]\n",
            " [1.0218182  1.0798193  1.0564804  1.0588126  1.0552402  1.0970193\n",
            "  1.1019205  1.08299    1.0830675  1.0403425  1.0450402  1.0465661\n",
            "  1.0435873  1.0507189  1.0460902  1.025565   1.0237007  1.0690681\n",
            "  1.0770786 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(4.679534, 1.6420524)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.8614768 1.927413  1.9115568 1.8896662 1.8050407 1.7341735 1.7634965\n",
            "  1.7665576 1.7581934 1.6420525]\n",
            " [2.2282014 2.5112844 2.4237545 2.331006  2.2764568 2.1010842 2.0895667\n",
            "  2.102909  2.0729027 1.757367 ]\n",
            " [2.3904321 3.0219975 3.0778646 2.8155265 2.3034992 1.9983908 2.1201062\n",
            "  2.0289814 2.046267  1.7326111]\n",
            " [2.8793266 3.7915967 3.8414173 3.4118118 2.6714048 2.0900006 2.0552552\n",
            "  2.1172297 2.0705428 1.7447971]\n",
            " [2.945346  4.0843716 4.679534  3.504522  2.7614913 2.4186502 2.3182664\n",
            "  2.1608796 2.1254818 1.8180084]\n",
            " [2.6405563 3.3528464 3.788927  3.392749  2.9662237 2.3241339 2.302054\n",
            "  2.1324234 2.1021986 1.8178676]\n",
            " [2.248747  2.8128474 2.927452  2.6105838 2.4001503 2.3031025 2.2276425\n",
            "  2.2174792 2.1873536 1.8497981]\n",
            " [1.9290811 1.9855    2.0460951 2.0056791 1.9944303 1.8906575 1.8207518\n",
            "  1.8069581 1.8341105 1.6568134]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.5317762, 0.85317934)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.88303924 0.9614459  0.929595   0.95556396 0.85317945]\n",
            " [0.9967135  1.2615211  1.2063503  1.1459208  0.95684767]\n",
            " [1.0554315  1.5317763  1.2191237  1.2038013  1.000487  ]\n",
            " [0.8716339  1.0684203  1.0033925  0.9740875  0.9185208 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[8.91420618e-02 3.84870134e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.45347221e-02\n",
            "  0.00000000e+00 4.30937409e-02 1.19802155e-01 0.00000000e+00\n",
            "  7.73128867e-02 1.31252110e-01 8.03237259e-02 1.57935306e-01\n",
            "  1.35181651e-01 0.00000000e+00 0.00000000e+00 9.07646641e-02\n",
            "  3.09475541e-01 0.00000000e+00 0.00000000e+00 4.83265668e-02\n",
            "  0.00000000e+00 6.44916594e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.61590874e-02 5.40721184e-03 4.02742880e-05\n",
            "  0.00000000e+00 0.00000000e+00 1.04014486e-01 6.90148994e-02\n",
            "  1.03359438e-01 4.56459932e-02 3.62565890e-02 5.51436990e-02\n",
            "  1.51545391e-03 0.00000000e+00 4.13459986e-02 9.45269018e-02\n",
            "  0.00000000e+00 1.16239853e-01 0.00000000e+00 8.53113458e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 5.39493486e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.05567375e-01\n",
            "  3.32487859e-02 1.26789417e-02 4.61659171e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 8.37463588e-02 0.00000000e+00\n",
            "  0.00000000e+00 4.49620001e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.11213073e-01 0.00000000e+00\n",
            "  5.44439815e-02 4.85868603e-02 8.35216790e-02 9.56651196e-02\n",
            "  0.00000000e+00 1.76110603e-02 6.38554692e-02 0.00000000e+00\n",
            "  4.04465087e-02 1.98126376e-01 0.00000000e+00 2.14735810e-02\n",
            "  0.00000000e+00 2.05760244e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 3.36028486e-02 2.78340042e-01 9.29262042e-02\n",
            "  0.00000000e+00 1.31044220e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.69063285e-01 2.08999962e-01\n",
            "  9.38231647e-02 2.42576286e-01 7.96680897e-02 0.00000000e+00\n",
            "  5.27679361e-02 0.00000000e+00 0.00000000e+00 2.31053345e-02\n",
            "  0.00000000e+00 1.92263171e-01 2.99070869e-03 2.38694772e-01\n",
            "  2.29859471e-01 0.00000000e+00 8.46167952e-02 0.00000000e+00\n",
            "  3.13304693e-01 0.00000000e+00 3.15585792e-01 8.44475161e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "input shape=torch.Size([1, 121, 145, 1])\n",
            "1 torch.Size([1, 121, 145, 1]) OAS30720_MR_d0566\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_axial/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/91_conv1_71/\n",
            "torch.Size([1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.664672, 0.109112695)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(6.097318, 0.66180766)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84985703 0.7527417  0.74611175 ... 0.7268853  0.7614306  0.75633585]\n",
            " [0.8163176  0.7215066  0.67593    ... 0.6812856  0.7248418  0.77615654]\n",
            " [0.8295602  0.74800485 0.72947025 ... 0.71286774 0.7689445  0.7629746 ]\n",
            " ...\n",
            " [0.80941546 0.73411715 0.71358395 ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.8364083  0.79853344 0.7725078  ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.795786   ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(4.571498, 0.9155224)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.1969072  1.2260255  1.2063472  1.1949127  1.1747967  1.2034947\n",
            "  1.2054816  1.2213072  1.2668616  1.2446282  1.2655599  1.3216743\n",
            "  1.2721063  1.215258   1.1942009  1.2016023  1.1900463  1.2186903\n",
            "  1.1602858 ]\n",
            " [1.1335622  1.1259868  1.1399373  1.1346017  1.13003    1.1324666\n",
            "  1.1659188  1.1841667  1.3387135  1.6042936  1.624174   1.568195\n",
            "  1.3926227  1.2606162  1.1920824  1.1209663  1.0826734  1.1450156\n",
            "  1.1544166 ]\n",
            " [1.0457417  1.053578   1.0249518  1.0100127  1.0100266  1.0675697\n",
            "  1.130565   1.3919235  1.8867592  2.229295   2.1610332  2.2689342\n",
            "  1.6353174  1.3877746  1.1108562  1.0113181  0.976935   1.062165\n",
            "  1.1376017 ]\n",
            " [1.0590422  1.0202948  1.0257325  0.9844874  0.9711613  0.99257916\n",
            "  1.1277696  1.6424093  2.4262924  3.1508803  3.4884996  2.6749775\n",
            "  2.3497167  1.5475138  1.125213   0.95212114 0.9155225  1.0189992\n",
            "  1.1232432 ]\n",
            " [1.0596973  1.0332196  1.0523981  1.004864   0.97441095 1.0191602\n",
            "  1.1665295  1.7805593  2.790997   4.050885   4.571498   3.4007459\n",
            "  2.5236058  1.6024803  1.1188773  0.96304846 0.95635813 1.0458821\n",
            "  1.1205753 ]\n",
            " [1.0653117  1.0232393  1.0509148  1.0029091  0.9887018  1.0663832\n",
            "  1.0206393  1.5882597  2.5498538  3.5032096  4.014167   3.332913\n",
            "  2.2958722  1.5101941  1.1636475  0.96814436 0.93928206 1.0520022\n",
            "  1.1134158 ]\n",
            " [1.0636314  1.0221277  1.0522343  1.0001655  1.0097791  1.0142226\n",
            "  1.064067   1.383613   1.9467833  2.5038383  2.7478344  2.6203308\n",
            "  1.8330272  1.3702818  1.0782043  1.0046959  0.9238975  1.0526469\n",
            "  1.1125451 ]\n",
            " [1.0651249  1.0181926  1.0521992  1.0192186  1.0929123  1.0965718\n",
            "  1.2329813  1.269997   1.5425329  1.794035   1.8118597  1.8296913\n",
            "  1.545082   1.175726   1.0551764  1.0178896  0.9352655  1.0524908\n",
            "  1.1107653 ]\n",
            " [1.0636053  1.0222222  1.0413615  1.002319   1.0438884  1.1441447\n",
            "  1.3583947  1.3805437  1.4685496  1.4885223  1.3053067  1.2772799\n",
            "  1.183006   1.0781788  0.9850543  0.9929868  0.9260341  1.0539976\n",
            "  1.1123208 ]\n",
            " [1.0580589  1.029648   1.016055   0.9503125  0.9794662  1.3476809\n",
            "  1.6624998  1.7284881  1.5695804  1.280157   1.1596844  1.110712\n",
            "  1.0045365  0.95977193 1.0002943  0.9924103  0.9338558  1.0554578\n",
            "  1.1115435 ]\n",
            " [1.0656732  1.0212154  1.071806   0.9882463  0.9876796  1.2528912\n",
            "  1.6591153  1.933129   1.487054   1.4026052  1.2718543  1.1454128\n",
            "  1.062653   1.0135131  1.0049654  1.0027522  0.9434814  1.0533857\n",
            "  1.1124153 ]\n",
            " [1.0710664  1.0224485  1.033136   0.96501625 0.9992027  1.1894819\n",
            "  1.2268833  1.3723692  1.3474561  1.2847158  1.3011533  1.1404408\n",
            "  1.0263892  0.96819067 0.9770427  0.9817822  0.9445472  1.0650579\n",
            "  1.1001413 ]\n",
            " [1.062862   1.0331163  1.0324341  0.99233145 0.96202093 1.0571223\n",
            "  1.1419487  1.2111807  1.1583961  1.1515381  1.0696033  1.0608622\n",
            "  1.0214051  0.9874859  0.9936368  0.9914447  0.9407674  1.0499116\n",
            "  1.0908777 ]\n",
            " [1.0540136  1.0507709  1.0462426  1.0350844  1.0345218  1.041061\n",
            "  1.0298523  1.1270406  1.1465135  1.1481298  1.0913713  1.0845383\n",
            "  1.080175   1.0538902  1.005981   1.0121282  0.98594904 1.0710999\n",
            "  1.1080891 ]\n",
            " [1.0730994  1.08362    1.0636244  1.0330049  1.0283805  1.029421\n",
            "  1.0156815  1.0416001  1.0326217  1.0334862  1.0452105  1.0443121\n",
            "  1.0187716  1.0157126  1.0259961  1.0347648  0.99236465 1.1026856\n",
            "  1.1379229 ]\n",
            " [1.0150622  1.0669893  1.0686878  1.040751   1.0375216  1.0501105\n",
            "  1.0378315  1.0491227  1.0495499  1.0471307  1.0495266  1.0536319\n",
            "  1.0431352  1.0404714  1.041695   1.0256277  1.0242792  1.0699304\n",
            "  1.0765216 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(3.6804857, 1.6240197)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.7624526 1.889753  1.9394705 2.1572483 2.3243763 2.2173653 2.1025705\n",
            "  1.8484815 1.8141974 1.6915967]\n",
            " [1.9494649 2.2181573 2.3340266 2.8136    3.092945  3.263483  2.571267\n",
            "  2.2064538 2.1358814 1.8174005]\n",
            " [1.9228904 2.2296176 2.303327  2.7105727 3.4444828 3.6804857 2.784236\n",
            "  2.2487512 2.0498664 1.7770015]\n",
            " [1.9434355 2.110139  2.3089542 2.507618  2.986252  3.033175  2.650023\n",
            "  2.224514  2.108583  1.8216141]\n",
            " [1.972824  2.2397738 2.2748632 2.4181788 2.4873054 2.4962366 2.457806\n",
            "  2.2590888 2.1468945 1.834362 ]\n",
            " [1.9040904 2.1786683 2.0865812 2.3855002 2.3379462 2.1473145 2.126473\n",
            "  2.0196807 2.01196   1.8006512]\n",
            " [1.9704465 2.2292984 2.2473125 2.1488326 2.227684  2.2204623 2.1819715\n",
            "  2.1546092 2.16926   1.7962356]\n",
            " [1.7773043 1.8912209 1.8602146 1.8984746 1.891168  1.8293892 1.7905143\n",
            "  1.7798055 1.8206738 1.6240199]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.3973366, 0.8281578)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.83055526 0.9115844  0.9328588  0.9096974  0.8281579 ]\n",
            " [0.98788416 1.1851333  1.3126453  1.2931546  0.9472839 ]\n",
            " [0.92261744 1.2510028  1.3973367  1.3371478  0.9599462 ]\n",
            " [0.84029126 1.002072   1.0431503  1.0350584  0.91825634]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.14601414 0.02582997 0.         0.05334154 0.         0.0214107\n",
            "  0.0775341  0.         0.         0.         0.04556587 0.\n",
            "  0.14587043 0.20632948 0.03617711 0.16467085 0.09748624 0.\n",
            "  0.         0.06662165 0.31993657 0.         0.         0.07457014\n",
            "  0.         0.14884223 0.         0.         0.         0.\n",
            "  0.03105634 0.01389795 0.         0.         0.09940498 0.\n",
            "  0.1423342  0.0368438  0.08103466 0.05051183 0.         0.06576696\n",
            "  0.0440705  0.         0.         0.14569984 0.         0.1475693\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.01494258 0.         0.         0.         0.         0.19221304\n",
            "  0.06880283 0.02918899 0.04017156 0.         0.00862785 0.\n",
            "  0.12319527 0.         0.         0.0056502  0.         0.\n",
            "  0.         0.0137171  0.06378778 0.         0.01729516 0.07318947\n",
            "  0.05021056 0.07972869 0.         0.00543898 0.08404111 0.\n",
            "  0.03450169 0.1568695  0.         0.00595393 0.         0.02172903\n",
            "  0.         0.         0.         0.21376893 0.24617079 0.08762564\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.22556151 0.16048038 0.07957266 0.14269565 0.0321862  0.\n",
            "  0.         0.         0.         0.08083815 0.         0.21152993\n",
            "  0.02988561 0.22469777 0.10747528 0.01661801 0.13832948 0.\n",
            "  0.3538079  0.         0.22394475 0.         0.         0.01223673\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_sagittal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/91_conv1_71/\n",
            "torch.Size([121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(0.7292545, 0.078346364)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(3.0079608, 0.65588754)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84987175 0.7529897  0.74513817 ... 0.7268875  0.7614306  0.7563359 ]\n",
            " [0.8164857  0.7224271  0.6742576  ... 0.6812806  0.72484297 0.77615535]\n",
            " [0.82986486 0.74514633 0.7293216  ... 0.7128686  0.76893204 0.76297456]\n",
            " ...\n",
            " [0.8094152  0.73411524 0.7135755  ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.83640826 0.79853344 0.7725078  ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.795786   ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(2.3624723, 0.9139143)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.1975644  1.2421906  1.2113134  1.2093524  1.2505268  1.2176379\n",
            "  1.3463244  1.2554314  1.2346209  1.2124804  1.1745635  1.1766124\n",
            "  1.1716331  1.1725551  1.1774007  1.183194   1.1852112  1.2211033\n",
            "  1.1605223 ]\n",
            " [1.1656455  1.1141422  1.1227183  1.3445433  1.3095766  1.369736\n",
            "  1.3855177  1.184848   1.2336246  1.1302627  1.115179   1.1223563\n",
            "  1.1173084  1.1192061  1.1257842  1.1095532  1.0806959  1.1413494\n",
            "  1.1556721 ]\n",
            " [1.0502679  1.1023039  1.0264177  1.394868   1.6103138  1.5688194\n",
            "  1.5957098  1.5257499  1.229647   1.0964764  1.0050123  1.014968\n",
            "  1.0030334  1.0055859  0.9966366  1.019592   0.9820003  1.068222\n",
            "  1.1383604 ]\n",
            " [1.0600864  1.0347373  0.99681854 1.4027663  1.8276066  1.9471005\n",
            "  1.8154277  1.6426882  1.3402184  1.0957385  0.9877421  0.99059474\n",
            "  0.96245974 0.9496242  0.95996577 0.95727515 0.91391444 1.0238663\n",
            "  1.1246802 ]\n",
            " [1.0665246  1.0346657  1.0381399  1.2358996  1.6498935  2.3624723\n",
            "  2.0441022  1.7760335  1.5592185  1.329502   1.0757607  1.0594501\n",
            "  1.0045186  0.99891734 0.99129194 0.99269176 0.9521382  1.0511695\n",
            "  1.1214936 ]\n",
            " [1.0690438  1.058684   1.0865649  1.1893395  1.4671808  2.0278172\n",
            "  2.1209285  1.8170425  1.7498215  1.6883643  1.3667655  1.1164684\n",
            "  1.0455343  1.0023822  0.9915267  0.9813608  0.9417956  1.0556501\n",
            "  1.1140634 ]\n",
            " [1.0524582  1.0472172  1.005761   1.0701481  1.422506   1.6140093\n",
            "  1.9683368  2.2868273  1.9007822  1.7977917  1.5007231  1.182309\n",
            "  1.020526   1.0057931  0.9860003  0.9961092  0.94358087 1.0533881\n",
            "  1.1126044 ]\n",
            " [1.0726804  1.0205265  1.0114436  0.9625378  1.2247964  1.4675936\n",
            "  1.8361572  1.972704   2.2132301  2.0554695  1.6485544  1.2623554\n",
            "  1.0765365  0.9988774  1.0017262  0.98789126 0.9414338  1.0533314\n",
            "  1.1113616 ]\n",
            " [1.0631728  1.0404434  1.0307816  0.9479188  1.0627041  1.3831637\n",
            "  1.542401   1.6880041  1.8264933  1.7251801  1.4366461  1.2344071\n",
            "  1.0877961  1.0458086  0.9701992  0.98139024 0.9442686  1.0567311\n",
            "  1.1119769 ]\n",
            " [1.0702229  1.0330243  0.99468935 0.9664526  1.0095243  1.1131874\n",
            "  1.3177391  1.3283397  1.5174708  1.4084638  1.4849255  1.2718729\n",
            "  1.0618051  1.0531737  1.0037793  0.9935921  0.94326293 1.0548337\n",
            "  1.1118639 ]\n",
            " [1.0683846  1.0291392  1.0153936  1.0645815  1.0024116  1.0738391\n",
            "  1.1215516  1.1912938  1.0922412  1.3122152  1.2588909  1.2263187\n",
            "  1.0199842  0.9925529  0.9902417  0.9746245  0.93342537 1.0573515\n",
            "  1.1140084 ]\n",
            " [1.069908   1.0253352  1.0307629  0.9792772  0.9693432  1.0072898\n",
            "  1.0166049  1.094689   1.0837091  1.037464   1.1507243  1.0902661\n",
            "  1.0428666  0.98094165 0.9869095  0.9926528  0.944571   1.0624436\n",
            "  1.1001947 ]\n",
            " [1.0615207  1.0221757  1.0317999  0.9946995  0.9850888  0.9714067\n",
            "  0.9892483  0.9964002  1.0730132  1.0720354  0.91568774 1.0128405\n",
            "  0.9644116  1.0026208  0.98273754 0.9835678  0.93698823 1.0498952\n",
            "  1.0897813 ]\n",
            " [1.0521688  1.0571009  1.0512091  1.0234956  1.0217813  1.0096263\n",
            "  1.039272   1.0660249  1.0239965  1.0531956  1.0508152  1.0489509\n",
            "  1.0340353  1.0430782  1.0217105  1.0113758  0.9879495  1.0736357\n",
            "  1.1074082 ]\n",
            " [1.0704708  1.0818402  1.060865   1.035757   1.0298226  1.0283046\n",
            "  1.0229458  1.0292307  1.0299134  1.0283194  1.0267566  1.0341815\n",
            "  1.0279914  1.02522    1.040845   1.0402431  0.9903017  1.1036648\n",
            "  1.1376258 ]\n",
            " [1.014766   1.0666517  1.0683988  1.04193    1.0444432  1.0425463\n",
            "  1.036843   1.0657884  1.0536228  1.0513026  1.0454305  1.0562073\n",
            "  1.0402782  1.0386451  1.0453928  1.0243801  1.0242515  1.0697421\n",
            "  1.0764333 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(2.7175348, 1.6406491)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.8510121 1.9847313 1.9220343 1.8191807 1.9377002 1.8064679 1.7929891\n",
            "  1.8149191 1.7919134 1.6449457]\n",
            " [2.0493813 2.2938712 2.3942523 2.3002486 2.316078  2.2299871 2.058766\n",
            "  2.1404486 2.0888348 1.7565037]\n",
            " [1.9081583 2.3464518 2.493004  2.6235127 2.4436362 2.236722  2.1961243\n",
            "  2.1577287 2.1113398 1.7730287]\n",
            " [1.9882956 2.144022  2.1969328 2.7175348 2.5640075 2.3718019 2.1733468\n",
            "  2.1209626 2.1342168 1.7780083]\n",
            " [1.9604486 2.200912  2.2457356 2.417554  2.6894038 2.3961112 2.2098284\n",
            "  2.1401172 2.1280692 1.8240277]\n",
            " [1.9450122 2.1005912 2.150034  2.173771  2.2966533 2.3223531 2.263637\n",
            "  2.114948  2.049302  1.7669774]\n",
            " [2.00179   2.1621006 2.1443615 2.220357  2.1871657 2.1943607 2.1845708\n",
            "  2.1218333 2.1745615 1.7875515]\n",
            " [1.7770488 1.8385986 1.8189795 1.7955123 1.8411863 1.7621228 1.8384686\n",
            "  1.7356883 1.83124   1.6406492]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.3276081, 0.8357718)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8357719  0.9468819  0.9939423  0.9537003  0.8460157 ]\n",
            " [0.96506345 1.2491869  1.2326658  1.1678325  0.96069086]\n",
            " [0.9773934  1.1913668  1.3276082  1.2686713  0.9734089 ]\n",
            " [0.83595145 1.0147072  1.0170726  0.99104524 0.9039291 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.         0.02732907 0.         0.         0.         0.\n",
            "  0.         0.0761892  0.         0.05482759 0.20536795 0.\n",
            "  0.04003088 0.06354234 0.1250425  0.11796013 0.12036117 0.\n",
            "  0.         0.09692001 0.22526962 0.         0.         0.00192942\n",
            "  0.         0.022342   0.         0.         0.         0.03881245\n",
            "  0.0291713  0.         0.         0.02130547 0.07206155 0.10712608\n",
            "  0.0917429  0.02784754 0.04201304 0.07809526 0.         0.\n",
            "  0.05538965 0.19585524 0.         0.0810172  0.         0.08571223\n",
            "  0.00852953 0.         0.00290804 0.         0.         0.\n",
            "  0.04334461 0.         0.         0.         0.         0.17768572\n",
            "  0.0397765  0.02093734 0.1205406  0.03212611 0.         0.\n",
            "  0.052864   0.         0.         0.05372739 0.         0.\n",
            "  0.         0.         0.12671967 0.         0.09122017 0.03479438\n",
            "  0.09282026 0.09946081 0.         0.09750699 0.01834383 0.\n",
            "  0.02213512 0.23095766 0.         0.06701727 0.         0.\n",
            "  0.         0.         0.         0.00909115 0.26624316 0.08519296\n",
            "  0.         0.02557603 0.         0.         0.         0.\n",
            "  0.13128984 0.16991241 0.13946995 0.24180374 0.10697665 0.\n",
            "  0.08396982 0.         0.01990489 0.         0.         0.12338204\n",
            "  0.01959226 0.24027237 0.27372482 0.         0.05978476 0.\n",
            "  0.3154779  0.         0.32224357 0.         0.         0.00280607\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_coronal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/91_conv1_71/\n",
            "torch.Size([1, 1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.8380613, 0.07629388)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(6.058159, 0.6621809)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8498466  0.7527888  0.7457387  ... 0.7269043  0.76118004 0.7562312 ]\n",
            " [0.816249   0.7216246  0.6760509  ... 0.67949593 0.7244964  0.7755874 ]\n",
            " [0.82935584 0.7474475  0.72886264 ... 0.7115351  0.76836824 0.76242924]\n",
            " ...\n",
            " [0.8094154  0.7341168  0.7135657  ... 0.7149321  0.7588714  0.7376537 ]\n",
            " [0.8364083  0.79853344 0.77250856 ... 0.7817973  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.79578596 ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(4.943122, 0.907121)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.1970727  1.2186428  1.1955203  1.187867   1.1712188  1.1787165\n",
            "  1.1738176  1.1745493  1.1737899  1.1747745  1.1748067  1.173832\n",
            "  1.1740466  1.1705223  1.1781133  1.1862701  1.1894665  1.2242695\n",
            "  1.1613286 ]\n",
            " [1.1366534  1.1110784  1.1258742  1.1188731  1.1156187  1.1171907\n",
            "  1.1148936  1.1125945  1.1170553  1.1226947  1.1175271  1.1197857\n",
            "  1.118865   1.1191173  1.1257659  1.1133848  1.0729374  1.1366767\n",
            "  1.1603913 ]\n",
            " [1.0608103  1.065267   1.0082214  1.0046514  0.99805593 0.97760355\n",
            "  1.0100596  1.0040158  1.0204259  1.0152904  1.0043175  1.000362\n",
            "  1.0042354  0.99888897 0.9951676  1.0137265  0.9699634  1.0649904\n",
            "  1.1352328 ]\n",
            " [1.0861609  1.043831   1.0862896  1.0395705  1.0845314  1.0241929\n",
            "  1.0490705  1.0071068  0.9789043  0.9712081  0.9577892  0.96195763\n",
            "  0.9623146  0.9529618  0.95568144 0.96270156 0.9071211  1.0173694\n",
            "  1.1275475 ]\n",
            " [1.1205329  1.1136694  1.1584102  1.2975343  1.4543027  1.3170112\n",
            "  1.345541   1.1665028  1.0917656  1.0333949  1.0286502  1.0459253\n",
            "  0.99468946 1.0074025  0.9928643  1.0000188  0.9561     1.0517141\n",
            "  1.1231691 ]\n",
            " [1.1912006  1.3617626  1.7402111  1.8564745  2.029039   1.920386\n",
            "  1.7469441  1.5449127  1.2547758  1.0805272  1.0554711  1.0091817\n",
            "  1.0108913  1.0061648  0.9945338  0.9902301  0.9431448  1.0521637\n",
            "  1.1146046 ]\n",
            " [1.3776487  1.7493392  2.4722795  2.7695858  3.145658   2.8628278\n",
            "  2.4131079  1.9596541  1.7099627  1.2412292  1.0882664  1.1485654\n",
            "  1.0532833  1.021239   0.9875951  0.9850807  0.9459678  1.0569426\n",
            "  1.1107664 ]\n",
            " [1.3484981  2.0546498  3.1802173  3.950832   4.1409383  3.5911946\n",
            "  3.3956232  2.4476376  2.0348814  1.2887553  1.2590071  1.1432685\n",
            "  1.116291   1.0888717  1.0136495  1.0127882  0.9639015  1.0690132\n",
            "  1.1086243 ]\n",
            " [1.2560408  2.031435   3.4493508  4.722789   4.943122   4.596283\n",
            "  4.370293   3.0031474  1.9176536  1.4516443  1.4245588  1.3637084\n",
            "  1.2274702  1.2649149  1.0738968  1.0262134  0.9571105  1.0638629\n",
            "  1.1148782 ]\n",
            " [1.2440747  1.765036   2.602902   3.6443415  4.454946   4.466127\n",
            "  3.986909   2.8301034  2.005252   1.5086477  1.3565298  1.4334165\n",
            "  1.2901881  1.1801429  1.110898   1.0104587  0.93953466 1.0515214\n",
            "  1.1013219 ]\n",
            " [1.1871675  1.5516237  2.1059728  2.5505104  3.3051176  4.1122556\n",
            "  3.3020396  2.636354   1.9634093  1.3704855  1.2379469  1.4309477\n",
            "  1.4401071  1.159084   1.019332   1.038967   0.937499   1.0410396\n",
            "  1.1155561 ]\n",
            " [1.110512   1.1381156  1.5259159  1.899734   2.655028   3.3939033\n",
            "  2.9376416  2.1255174  1.4762005  1.2070203  1.0982841  1.2149137\n",
            "  1.3304089  1.1812423  1.0406699  1.0167791  0.9663259  1.0824522\n",
            "  1.0973449 ]\n",
            " [1.0833892  1.0577891  1.215664   1.5471017  1.7191675  2.1003797\n",
            "  2.00809    1.7940478  1.3272685  1.1398956  1.1084806  1.0108813\n",
            "  1.0591327  1.0315117  0.9571465  1.0104975  0.94069195 1.0335206\n",
            "  1.0782839 ]\n",
            " [1.0398504  1.1032747  1.1125119  1.2623774  1.4316912  1.4525516\n",
            "  1.4169848  1.2636518  1.21672    1.0534836  1.0625857  1.0528718\n",
            "  1.064555   1.0591274  1.0661874  1.0253897  0.97751105 1.0711776\n",
            "  1.104273  ]\n",
            " [1.079562   1.0889562  1.0723337  1.0459408  1.1578473  1.162079\n",
            "  1.0774544  1.0986212  1.0593153  1.0285479  1.0432869  1.0025414\n",
            "  1.02416    1.0142938  1.0392509  1.0287246  0.9863738  1.1043034\n",
            "  1.1376781 ]\n",
            " [1.014655   1.0722151  1.0588907  1.0336734  1.0594572  1.0685626\n",
            "  1.0736058  1.0797838  1.0468034  1.054175   1.0527908  1.0565344\n",
            "  1.0517647  1.0489048  1.0508021  1.0219028  1.0279971  1.0703079\n",
            "  1.0749533 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(4.5519, 1.6116557)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.8774571 1.842925  1.8369123 1.8208988 1.8259219 1.7844863 1.7983851\n",
            "  1.7913046 1.7834901 1.6527096]\n",
            " [2.146274  2.4781997 2.306706  2.3763475 2.180332  2.1501276 2.097742\n",
            "  2.123462  2.1016545 1.7800846]\n",
            " [2.4132411 2.8697371 2.9249892 2.7454793 2.514822  2.1737912 2.1779852\n",
            "  2.063841  2.077032  1.7288176]\n",
            " [2.76756   3.693318  3.8295627 3.476775  2.8948054 2.2156665 2.0945046\n",
            "  2.0692837 2.1270318 1.806495 ]\n",
            " [2.7210147 3.9114697 4.5519    4.034297  3.2909336 2.4428234 2.2076788\n",
            "  2.1232402 2.123526  1.8151488]\n",
            " [2.4482503 3.2789059 3.8676682 3.4632893 2.9752207 2.5130887 2.1226802\n",
            "  2.11892   2.0542626 1.7605096]\n",
            " [2.1673088 2.7385466 2.9949741 2.8119411 2.498765  2.2508268 2.2290452\n",
            "  2.114694  2.1706204 1.7963585]\n",
            " [1.8912289 1.9557348 2.0770273 2.0676255 1.9894627 1.8345402 1.8666528\n",
            "  1.7455553 1.8212198 1.6116558]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.732188, 0.828003)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.86465734 1.0784382  0.9970934  0.9414052  0.8280031 ]\n",
            " [1.0489672  1.4565152  1.3749584  1.2052146  0.90963924]\n",
            " [1.1240207  1.7321881  1.5386766  1.2731766  0.9389826 ]\n",
            " [0.89722323 1.2159268  1.0686156  0.96212375 0.89172167]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.17099671 0.01327142 0.         0.0520104  0.         0.06507666\n",
            "  0.0782977  0.         0.         0.         0.         0.\n",
            "  0.10402685 0.21097697 0.         0.16523191 0.0709619  0.\n",
            "  0.         0.0558357  0.3464055  0.         0.         0.10807692\n",
            "  0.         0.17246366 0.         0.         0.         0.\n",
            "  0.0217505  0.03533692 0.01653221 0.         0.08191321 0.\n",
            "  0.18279034 0.03088808 0.08258124 0.         0.         0.06482068\n",
            "  0.         0.         0.         0.16794136 0.         0.23641063\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.03646147 0.         0.         0.         0.         0.19975229\n",
            "  0.05944112 0.04090378 0.00631647 0.         0.03857039 0.\n",
            "  0.12142489 0.         0.         0.         0.         0.\n",
            "  0.         0.0396707  0.05811964 0.         0.00577205 0.08886925\n",
            "  0.0256879  0.06821825 0.         0.         0.10769438 0.\n",
            "  0.04501097 0.12285867 0.         0.         0.         0.02420153\n",
            "  0.         0.         0.00987523 0.24821487 0.21721213 0.09314419\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.19240594 0.15599109 0.02703703 0.08247354 0.         0.\n",
            "  0.         0.         0.         0.11690981 0.         0.20983438\n",
            "  0.03868124 0.2099376  0.02015364 0.00313868 0.13725436 0.\n",
            "  0.3215448  0.         0.21100135 0.00101418 0.         0.00456281\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "input shape=torch.Size([1, 121, 145, 1])\n",
            "1 torch.Size([1, 121, 145, 1]) OAS30720_MR_d0566\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_axial/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/94_conv1_71/\n",
            "torch.Size([1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(0.8230864, 0.106817074)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(3.790206, 0.66180766)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84985703 0.7527417  0.74611175 ... 0.7268853  0.7614306  0.75633585]\n",
            " [0.8163176  0.7215066  0.67593    ... 0.6812856  0.7248418  0.77615654]\n",
            " [0.8295602  0.74800485 0.72947025 ... 0.71286774 0.7689445  0.7629746 ]\n",
            " ...\n",
            " [0.80941546 0.73411715 0.71358395 ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.8364083  0.79853344 0.7725078  ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.795786   ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(2.5712154, 0.9102644)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.196941   1.2260133  1.2042352  1.1932449  1.1696615  1.1865646\n",
            "  1.1760175  1.1631159  1.2042265  1.1890591  1.1948594  1.2148331\n",
            "  1.2222639  1.20687    1.1755592  1.1827449  1.1864427  1.2207624\n",
            "  1.1606544 ]\n",
            " [1.1332164  1.1259121  1.1403756  1.1283966  1.1292944  1.1493869\n",
            "  1.1538972  1.1740457  1.2236222  1.1415004  1.2625695  1.1737597\n",
            "  1.1416379  1.1484256  1.1440703  1.1110415  1.0802652  1.1419199\n",
            "  1.1550573 ]\n",
            " [1.045772   1.053271   1.0235466  1.0058086  1.0013916  1.0338279\n",
            "  1.0670141  1.1600099  1.266431   1.5421258  1.4186918  1.4318894\n",
            "  1.1665324  1.1066617  0.98590285 1.0170928  0.98078465 1.067825\n",
            "  1.1389124 ]\n",
            " [1.0589591  1.0202698  1.024727   0.97429466 0.96643794 1.0099121\n",
            "  0.9618238  1.2261953  1.5036825  1.6817082  2.0466235  1.6556417\n",
            "  1.4377695  1.0786881  0.9968053  0.9460666  0.9102645  1.0211996\n",
            "  1.1253387 ]\n",
            " [1.0604422  1.0324997  1.0501453  1.0044681  0.9724175  0.9889301\n",
            "  1.0227281  1.158967   1.6624969  2.2625585  2.5712154  1.8635049\n",
            "  1.5030485  1.1236255  1.0074488  0.99435115 0.9539595  1.0470754\n",
            "  1.1206548 ]\n",
            " [1.0662664  1.027331   1.056905   0.9883724  0.9758867  1.0478933\n",
            "  1.0098802  1.2300864  1.7478136  2.2193558  2.3350673  1.7022475\n",
            "  1.48642    1.0916947  1.015913   0.9667655  0.93812555 1.0562894\n",
            "  1.1134902 ]\n",
            " [1.0673257  1.0228376  1.0417134  1.0132064  1.0026004  1.0039529\n",
            "  0.98516834 1.1208768  1.4157778  1.6071182  1.7072588  1.4686335\n",
            "  1.1599678  1.0166436  0.9876568  0.98353994 0.93050385 1.0530826\n",
            "  1.1134577 ]\n",
            " [1.0664371  1.0254016  1.0381371  1.001625   1.0307106  1.0519724\n",
            "  1.0935172  1.1576971  1.1552024  1.2356783  1.2307814  1.227565\n",
            "  1.0462269  0.986342   1.0058129  0.99072593 0.93387234 1.0530595\n",
            "  1.1120297 ]\n",
            " [1.0646895  1.0243939  1.0426087  0.9607689  1.0355736  1.2508975\n",
            "  1.2536992  1.260069   1.2966652  1.1983103  1.0357494  1.0243069\n",
            "  0.99848783 0.9785741  0.98968244 0.98378205 0.9402491  1.0553955\n",
            "  1.1125141 ]\n",
            " [1.0643513  1.0223688  1.0257642  0.9449991  1.0141176  1.2736498\n",
            "  1.3682325  1.3922256  1.3743469  1.1523901  1.0598313  1.0026661\n",
            "  0.9735248  0.9822501  0.99468327 0.99326426 0.93675625 1.0559069\n",
            "  1.1112722 ]\n",
            " [1.0651911  1.0218778  1.0659549  0.96243817 0.95704925 1.3641344\n",
            "  1.7463367  1.7832443  1.262942   1.1132079  1.0251507  0.9980953\n",
            "  1.0023844  0.987517   0.994388   0.99567044 0.9458705  1.0538872\n",
            "  1.1122895 ]\n",
            " [1.0716621  1.017854   1.0521256  0.9968143  1.0379299  1.0701182\n",
            "  1.3040376  1.3095579  1.1792468  1.0378954  0.9861121  0.9934473\n",
            "  0.9880736  0.97927845 0.9892756  0.9845776  0.94681954 1.0645003\n",
            "  1.0997494 ]\n",
            " [1.0661304  1.0330755  1.0363208  0.9901532  1.0429592  1.0528591\n",
            "  1.0786821  1.1295217  1.0251567  0.95991576 0.99508727 0.9994154\n",
            "  1.0027593  0.98638064 0.9830769  0.9942826  0.9430945  1.0494535\n",
            "  1.0905489 ]\n",
            " [1.0503546  1.0621253  1.0475494  1.0279711  1.0221678  1.0626168\n",
            "  1.0215826  1.0758231  1.0573487  1.047088   1.0454439  1.0474979\n",
            "  1.0415775  1.0376173  1.0293595  1.0161829  0.98868895 1.072025\n",
            "  1.1078886 ]\n",
            " [1.0733361  1.0868176  1.0733793  1.0310816  1.0210057  1.01518\n",
            "  1.0359882  1.0204095  1.0432442  1.0438414  1.0423454  1.034088\n",
            "  1.0290914  1.0278507  1.0330354  1.0359105  0.99161136 1.1034166\n",
            "  1.1379778 ]\n",
            " [1.0139536  1.0652496  1.0674132  1.0436875  1.0366603  1.0421445\n",
            "  1.0438961  1.051309   1.0493078  1.0568935  1.046188   1.0461613\n",
            "  1.0420918  1.0429109  1.043504   1.025454   1.0251268  1.0697283\n",
            "  1.0765167 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(2.523292, 1.6338964)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.7667214 1.8698708 1.8445243 1.9069287 1.855955  1.8547301 1.8344868\n",
            "  1.7677364 1.8174568 1.662079 ]\n",
            " [1.9257032 2.2217379 2.1741366 2.3888345 2.448256  2.4289165 2.1403785\n",
            "  2.1475284 2.1360261 1.8099654]\n",
            " [1.9167274 2.150487  2.1225252 2.2937236 2.3801606 2.523292  2.2646708\n",
            "  2.1310024 2.1055143 1.7862762]\n",
            " [1.9551656 2.1148906 2.171625  2.1653073 2.3806958 2.4388914 2.1762445\n",
            "  2.2211413 2.1161752 1.8296617]\n",
            " [1.9857255 2.1986198 2.334907  2.445218  2.3010917 2.1946652 2.150115\n",
            "  2.164988  2.1388996 1.8317358]\n",
            " [1.9278315 2.1139627 2.173997  2.4340835 2.2936325 2.1069264 2.0740309\n",
            "  2.0183344 2.0448353 1.7615842]\n",
            " [1.9617355 2.1873994 2.2084384 2.1883187 2.324975  2.2266655 2.1363726\n",
            "  2.0582085 2.1088178 1.7843454]\n",
            " [1.7887645 1.8579365 1.8734888 1.8662325 1.8784136 1.8336056 1.7489885\n",
            "  1.7758381 1.820343  1.6338965]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.3360705, 0.8408151)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84081525 0.9103245  0.97335434 0.9405644  0.84293056]\n",
            " [0.9722454  1.2491775  1.2402649  1.1998693  0.967455  ]\n",
            " [0.9492924  1.3192487  1.3360707  1.1772299  0.9622079 ]\n",
            " [0.8624095  0.98993564 1.0055479  1.0092955  0.9034995 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.09874208 0.02905415 0.         0.01536769 0.         0.\n",
            "  0.02177188 0.         0.         0.         0.0771718  0.\n",
            "  0.13510467 0.1546017  0.07818306 0.15770996 0.12183287 0.\n",
            "  0.         0.0761869  0.30487165 0.         0.         0.0495864\n",
            "  0.         0.10301768 0.         0.         0.         0.01580809\n",
            "  0.00759975 0.         0.         0.         0.09635777 0.03261282\n",
            "  0.13769962 0.03553857 0.07395888 0.06017927 0.         0.03164397\n",
            "  0.05033523 0.02976472 0.         0.12514901 0.         0.14186594\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.01654816 0.         0.         0.         0.         0.21357729\n",
            "  0.06132644 0.04631652 0.07181769 0.00076345 0.         0.\n",
            "  0.10329865 0.         0.         0.04686399 0.         0.\n",
            "  0.         0.         0.07603791 0.         0.0416154  0.06143254\n",
            "  0.07040945 0.08482634 0.         0.04324365 0.07515242 0.\n",
            "  0.02490582 0.19477932 0.         0.01138023 0.         0.01598506\n",
            "  0.         0.         0.         0.14828113 0.26556548 0.06740859\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.19494614 0.17217018 0.10256214 0.15915407 0.0517549  0.\n",
            "  0.         0.         0.         0.04672502 0.         0.17837405\n",
            "  0.02716584 0.24863012 0.18232599 0.         0.11943529 0.\n",
            "  0.37398726 0.         0.2753843  0.         0.         0.00583087\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_sagittal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/94_conv1_71/\n",
            "torch.Size([121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.1538126, 0.07219845)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(3.62707, 0.6600363)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84985703 0.7527435  0.74607694 ... 0.7268908  0.7614291  0.75633585]\n",
            " [0.8163162  0.72153705 0.6754651  ... 0.68127525 0.7248423  0.77615225]\n",
            " [0.82956386 0.7479067  0.72845197 ... 0.71285963 0.7689258  0.7629725 ]\n",
            " ...\n",
            " [0.80941534 0.7341163  0.7135805  ... 0.7149296  0.75887144 0.73765373]\n",
            " [0.83640826 0.79853344 0.77250767 ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.795786   ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(2.4345405, 0.91317093)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.2066544  1.2237439  1.1980911  1.1712123  1.1800543  1.2069155\n",
            "  1.2194772  1.2201277  1.20531    1.1926677  1.1949236  1.1753027\n",
            "  1.1762091  1.1745124  1.1761099  1.1832112  1.1850047  1.2210535\n",
            "  1.1605729 ]\n",
            " [1.1447659  1.1262918  1.1067497  1.071639   1.1774967  1.2748028\n",
            "  1.2502222  1.1754369  1.1494766  1.1380105  1.1253141  1.1236895\n",
            "  1.1249238  1.1232919  1.1266171  1.109681   1.0812715  1.1416044\n",
            "  1.1553565 ]\n",
            " [1.0554463  1.0688046  0.99479496 1.0460111  1.1566471  1.1557759\n",
            "  1.1426955  1.1881313  1.1508793  1.0806323  1.0785857  1.0207196\n",
            "  1.0144957  0.99620247 0.98991215 1.0180079  0.9828952  1.0684294\n",
            "  1.1389946 ]\n",
            " [1.0619934  1.03387    0.98674023 0.97808087 1.1110138  1.2846726\n",
            "  1.4128788  1.3731806  1.4149224  1.26082    1.1555893  1.017923\n",
            "  0.979888   0.94815266 0.9513664  0.9572227  0.91317105 1.0242542\n",
            "  1.1243786 ]\n",
            " [1.0656046  1.0355633  1.0520619  1.0633801  1.1336565  1.466224\n",
            "  1.5172657  1.6779734  1.6084709  1.6430696  1.3566493  1.1617745\n",
            "  1.0219175  1.0028614  0.99973845 0.99806845 0.9539341  1.0494367\n",
            "  1.1204377 ]\n",
            " [1.0716742  1.0288544  1.0397428  0.99520594 1.1583735  1.5141681\n",
            "  1.7643546  2.1746168  2.1498942  1.8620731  1.581061   1.1632183\n",
            "  1.0630878  1.0027698  0.9831201  0.9840673  0.94380486 1.0544678\n",
            "  1.1139202 ]\n",
            " [1.0615082  1.0390011  1.0542051  0.98509943 1.1020902  1.3998157\n",
            "  1.9799014  2.4345405  2.3148742  2.1542802  1.6097347  1.3040332\n",
            "  1.0162711  1.0009102  1.003829   0.9938923  0.942228   1.057773\n",
            "  1.1121689 ]\n",
            " [1.0717759  1.0109133  1.0437833  0.93550134 1.0300251  1.3271596\n",
            "  1.7233039  1.9695125  2.262671   1.8375694  1.6217321  1.364827\n",
            "  1.1679724  1.0281473  1.0410835  1.0140272  0.9465182  1.0530483\n",
            "  1.1111773 ]\n",
            " [1.0580376  1.0204271  1.0290146  0.9612691  0.97099483 1.1632546\n",
            "  1.2683445  1.4270052  1.7177985  1.8102205  1.7157646  1.3985494\n",
            "  1.2692432  1.071945   1.0543364  0.9789318  0.93817014 1.0566907\n",
            "  1.1128322 ]\n",
            " [1.0593444  1.0239772  1.0292007  1.013663   1.0244633  0.989395\n",
            "  1.0991145  1.3053588  1.4042773  1.5837879  1.5353866  1.2794976\n",
            "  1.2345155  1.1466866  1.0048186  0.9947872  0.9415748  1.0547609\n",
            "  1.1137534 ]\n",
            " [1.0680348  1.0229266  1.0446469  1.030211   0.97418666 1.077897\n",
            "  1.0922915  1.2239739  1.2082101  1.1949332  1.2768168  1.2252637\n",
            "  1.1473782  1.0619885  1.0312513  0.9714761  0.92782676 1.0525373\n",
            "  1.112825  ]\n",
            " [1.06536    1.0152885  1.027692   0.9782731  1.001658   0.9842139\n",
            "  0.9771557  1.0297183  1.0364227  1.0685266  1.0960709  1.0955684\n",
            "  1.0906554  1.0274879  0.9842235  0.9885965  0.9418081  1.062199\n",
            "  1.0994008 ]\n",
            " [1.0600705  1.0299598  1.0403346  0.98837554 0.99804306 0.992205\n",
            "  0.98213065 0.9917547  1.001716   0.97646403 0.9345048  0.9789309\n",
            "  0.9708184  0.99096227 0.9632999  0.98405063 0.93571174 1.0497371\n",
            "  1.0924611 ]\n",
            " [1.0523504  1.0549942  1.043689   1.0185156  1.0324095  1.0394882\n",
            "  1.0381736  1.0335665  1.0277778  1.0198638  1.0294429  1.0532377\n",
            "  1.0361568  1.0275627  1.0352026  1.0122732  0.98825455 1.0709488\n",
            "  1.1073208 ]\n",
            " [1.0720844  1.0803093  1.0617124  1.036747   1.0216583  1.032717\n",
            "  1.0341187  1.026151   1.0388211  1.0254031  1.024163   1.0194772\n",
            "  1.0242532  1.029555   1.0255724  1.0367877  0.9918558  1.1039408\n",
            "  1.1376485 ]\n",
            " [1.0144575  1.0665613  1.0690995  1.043018   1.0436746  1.0411159\n",
            "  1.0427411  1.0438868  1.0445994  1.0432533  1.0456309  1.043481\n",
            "  1.0457345  1.0435846  1.0442609  1.0256015  1.0249745  1.0691353\n",
            "  1.0764649 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(2.6357732, 1.6374891)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.7824131 1.9176387 1.8688775 1.9089197 1.8584372 1.8171307 1.8291932\n",
            "  1.7962868 1.8114454 1.6415159]\n",
            " [1.98517   2.2537687 2.2619293 2.267176  2.2348013 2.2068176 2.096312\n",
            "  2.148517  2.1080494 1.7545357]\n",
            " [1.9621614 2.2624624 2.392049  2.463027  2.456526  2.2515974 2.0641942\n",
            "  2.0706148 2.140288  1.7757357]\n",
            " [1.9845934 2.2536812 2.37468   2.6357732 2.5694618 2.3259344 2.2430515\n",
            "  2.075593  2.0675006 1.7864395]\n",
            " [1.9768672 2.1890476 2.3730469 2.4137793 2.5493255 2.559939  2.332213\n",
            "  2.2005334 2.191759  1.8032782]\n",
            " [1.9249017 2.1464581 2.1526186 2.286005  2.3494136 2.2764757 2.2398648\n",
            "  2.1484356 2.0743723 1.7409009]\n",
            " [1.9642783 2.1349015 2.0871935 2.1851091 2.1537018 2.2156239 2.1713896\n",
            "  2.1548543 2.153092  1.7935537]\n",
            " [1.7763363 1.8600383 1.8146975 1.8303522 1.8243104 1.8204597 1.8147362\n",
            "  1.7650231 1.8326508 1.6374892]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.483356, 0.84141374)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84475446 1.0123029  1.02761    0.94771373 0.8469712 ]\n",
            " [0.9909567  1.3711853  1.4833561  1.2400185  0.9196664 ]\n",
            " [0.96097994 1.3561283  1.4756979  1.289298   0.9518539 ]\n",
            " [0.84141386 0.9874183  1.0473744  0.9981601  0.9073411 ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.12522559 0.02623784 0.         0.04551866 0.         0.01512557\n",
            "  0.06586773 0.         0.         0.         0.01745775 0.\n",
            "  0.147339   0.19536914 0.04237469 0.1647974  0.08943284 0.\n",
            "  0.         0.04726982 0.31555    0.         0.         0.06697377\n",
            "  0.         0.15183812 0.         0.         0.         0.\n",
            "  0.02225266 0.01145888 0.         0.         0.09014814 0.\n",
            "  0.1602122  0.03171058 0.09681477 0.02875527 0.         0.04816058\n",
            "  0.03623255 0.         0.         0.15287064 0.         0.1907736\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.01576207 0.         0.         0.         0.         0.19841483\n",
            "  0.07488059 0.06097868 0.04918846 0.         0.00865242 0.\n",
            "  0.12899621 0.         0.         0.0077763  0.         0.\n",
            "  0.         0.01426264 0.06060673 0.         0.01671513 0.08559524\n",
            "  0.05761708 0.06858347 0.         0.01476792 0.08702117 0.\n",
            "  0.02142783 0.16678226 0.         0.         0.         0.03527454\n",
            "  0.         0.         0.         0.22213842 0.2393751  0.07268971\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.214576   0.15689869 0.06650138 0.11191049 0.0149395  0.\n",
            "  0.         0.         0.00101765 0.08750997 0.         0.18284626\n",
            "  0.04072234 0.2374198  0.09571461 0.         0.14071074 0.\n",
            "  0.36641872 0.         0.23273641 0.         0.         0.01629641\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_coronal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d0566_cdr0.5/94_conv1_71/\n",
            "torch.Size([1, 1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.9288032, 0.09645441)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(6.147134, 0.66179967)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.849879   0.7527674  0.7459209  ... 0.726846   0.7613495  0.75630724]\n",
            " [0.816327   0.7215304  0.6760017  ... 0.680658   0.72480536 0.7760844 ]\n",
            " [0.8294494  0.7479528  0.7291835  ... 0.7118859  0.768706   0.7627816 ]\n",
            " ...\n",
            " [0.80941546 0.7341162  0.7135939  ... 0.7149297  0.75887144 0.73765373]\n",
            " [0.8364083  0.79853344 0.772508   ... 0.78179705 0.81395    0.7831061 ]\n",
            " [0.8018056  0.75318444 0.795786   ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(5.107865, 0.90875894)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.1987436  1.2226202  1.1958547  1.1885092  1.1705369  1.177589\n",
            "  1.1757302  1.1729208  1.1729692  1.1759052  1.1736954  1.1735176\n",
            "  1.1730684  1.1713766  1.1782714  1.1833655  1.1846687  1.2221453\n",
            "  1.1608577 ]\n",
            " [1.1358842  1.1105902  1.1281374  1.1204141  1.114731   1.1192594\n",
            "  1.112785   1.115853   1.1206673  1.1191081  1.1205584  1.1181492\n",
            "  1.1177129  1.1223656  1.125538   1.111166   1.0790837  1.1421984\n",
            "  1.1549377 ]\n",
            " [1.061423   1.0670179  1.0066375  0.9971703  1.0000476  0.99567246\n",
            "  1.0223011  1.0176464  1.0228966  1.0100073  1.0031375  1.003797\n",
            "  1.0045453  1.004058   1.0025897  1.017434   0.97764826 1.0698009\n",
            "  1.1411703 ]\n",
            " [1.0730983  1.0307463  1.0474963  1.0253466  1.030431   0.9892832\n",
            "  1.0036935  0.9940125  0.96400523 0.955819   0.9582131  0.9576323\n",
            "  0.9535882  0.951052   0.9610894  0.9554099  0.90875906 1.0223423\n",
            "  1.122582  ]\n",
            " [1.0924082  1.093166   1.1470801  1.2390429  1.3249252  1.232902\n",
            "  1.1641048  1.0977038  1.0305231  1.0300201  1.0146525  1.00142\n",
            "  0.99932337 0.99922216 1.0024554  1.0059484  0.9587804  1.0494313\n",
            "  1.1234521 ]\n",
            " [1.1280231  1.2699368  1.498779   1.7331024  1.6827191  1.6060377\n",
            "  1.4481093  1.3014634  1.0933701  1.032744   0.9825056  0.9888704\n",
            "  0.9861357  0.9948385  0.9908504  1.0081829  0.9646216  1.0610847\n",
            "  1.1163228 ]\n",
            " [1.1725538  1.5874971  2.0632753  2.4065366  2.7128234  2.2558334\n",
            "  2.106721   1.7819849  1.3860863  1.1078628  1.0212164  0.99433404\n",
            "  1.0039355  1.0059136  1.0006357  0.9868213  0.95770514 1.058307\n",
            "  1.104346  ]\n",
            " [1.1736678  1.6884025  2.6769109  3.704146   3.8010573  3.3159585\n",
            "  2.8042822  2.54006    1.7598711  1.1622666  1.0113896  1.0131041\n",
            "  1.0073519  1.0209213  0.970075   1.048813   0.9790506  1.0876732\n",
            "  1.1088992 ]\n",
            " [1.09655    1.6789256  2.947104   4.341614   4.7609606  4.655463\n",
            "  3.8108387  2.9133127  1.882153   1.193801   1.0496558  0.97764397\n",
            "  1.0025939  1.0514817  1.045585   1.0539775  1.000362   1.0791571\n",
            "  1.0950905 ]\n",
            " [1.1608573  1.6108011  2.7932582  3.8734503  5.107865   4.9915614\n",
            "  4.475814   2.8456173  1.958655   1.3526167  1.0274633  0.9832497\n",
            "  0.9601903  1.0673147  1.0954548  1.1160289  1.0141786  1.0877573\n",
            "  1.1068405 ]\n",
            " [1.1876265  1.5319581  2.3026252  3.3574476  4.7637258  4.948079\n",
            "  4.000326   2.9776077  1.786949   1.3260957  0.96494293 0.9598391\n",
            "  0.96984017 1.0133061  1.0785736  1.041371   0.9668161  1.0480938\n",
            "  1.0984763 ]\n",
            " [1.0936736  1.1983079  1.7426709  2.3778548  3.4195783  4.1081233\n",
            "  3.4599652  2.3053918  1.644564   1.2281328  0.9711761  0.9737874\n",
            "  0.9949819  1.0823431  1.0707926  1.0471331  0.9561323  1.0717324\n",
            "  1.0942966 ]\n",
            " [1.041511   1.1411425  1.3199714  1.9274657  2.284338   2.6163783\n",
            "  2.536459   2.0959392  1.4113883  1.193496   1.0199413  0.96761966\n",
            "  0.98222685 1.0035702  0.9205831  1.004327   0.9451866  1.0207893\n",
            "  1.0847255 ]\n",
            " [1.0410289  1.0795934  1.0962957  1.4586278  1.6297382  1.6631147\n",
            "  1.6358315  1.427683   1.2415556  1.0508255  1.0218426  1.0189072\n",
            "  1.0286486  1.0325204  1.0208403  1.0239687  0.9797638  1.0692246\n",
            "  1.1111964 ]\n",
            " [1.0763384  1.0818828  1.0902514  1.0722994  1.1716783  1.2002128\n",
            "  1.1128644  1.1405572  1.0636724  1.0319756  1.0413984  1.0249941\n",
            "  1.0381044  1.0243384  1.0292729  1.0364759  0.9859635  1.1068352\n",
            "  1.1435058 ]\n",
            " [1.0140897  1.082951   1.066275   1.0344175  1.0577143  1.0700035\n",
            "  1.0633012  1.0637113  1.0418438  1.0529089  1.0515739  1.0477048\n",
            "  1.0446447  1.0411779  1.0505162  1.0270549  1.0210341  1.0704132\n",
            "  1.0748442 ]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(4.7692904, 1.6167691)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.8378105 1.9071242 1.7868311 1.8325076 1.8181332 1.8327068 1.8337835\n",
            "  1.8144852 1.8100529 1.6485195]\n",
            " [2.1625574 2.4744792 2.2397165 2.314338  2.1597342 2.1509113 2.0804605\n",
            "  2.1398945 2.0719225 1.7573501]\n",
            " [2.375662  2.8595352 2.7119074 2.6964338 2.4049253 2.1394134 2.1724186\n",
            "  2.1162653 2.1056032 1.7695634]\n",
            " [2.818068  3.7808518 3.5814016 3.342143  2.6836534 2.1933088 2.1529272\n",
            "  2.118227  2.1168003 1.8290188]\n",
            " [2.8669274 4.1645317 4.7692904 3.957407  2.9219306 2.267577  2.1247265\n",
            "  2.1222844 2.0968502 1.8746115]\n",
            " [2.6484323 3.5597568 4.675204  3.878035  2.8385444 2.2925696 2.0554922\n",
            "  2.080388  2.1362817 1.76378  ]\n",
            " [2.245027  3.0009322 3.30448   3.1875138 2.5363214 2.17999   2.19539\n",
            "  2.0897074 2.1517725 1.7888671]\n",
            " [1.9664187 2.1957302 2.3383927 2.2798104 2.0677142 1.9031451 1.8767105\n",
            "  1.7693962 1.8256116 1.6167692]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(1.572961, 0.83843815)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.85192084 0.9882818  0.98978966 0.96101284 0.8384383 ]\n",
            " [0.94280994 1.2663234  1.2839695  1.2361311  0.93722886]\n",
            " [1.0194488  1.5729611  1.3148469  1.2224809  0.9506565 ]\n",
            " [0.89723146 1.1839355  1.0013     0.98589116 0.89389396]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.14369053 0.03601275 0.         0.03914073 0.         0.0207072\n",
            "  0.05004158 0.         0.         0.         0.0644193  0.\n",
            "  0.12496093 0.18584685 0.04704135 0.15935344 0.11698145 0.\n",
            "  0.         0.07602063 0.3320579  0.         0.         0.06551237\n",
            "  0.         0.11748761 0.         0.         0.         0.\n",
            "  0.         0.02234483 0.         0.         0.10337462 0.01002758\n",
            "  0.12319234 0.04338783 0.07820187 0.04512316 0.         0.04345451\n",
            "  0.03761004 0.         0.         0.14469557 0.         0.12969632\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.03997174 0.         0.         0.         0.         0.20990285\n",
            "  0.05318493 0.02464224 0.02900873 0.         0.         0.\n",
            "  0.12603644 0.         0.         0.01801212 0.         0.\n",
            "  0.         0.00517691 0.06834866 0.         0.01709674 0.07401015\n",
            "  0.05154116 0.09155732 0.         0.         0.07828297 0.\n",
            "  0.04366744 0.16587296 0.         0.         0.         0.0198911\n",
            "  0.         0.         0.         0.14837456 0.2604311  0.09392094\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.20550962 0.17761377 0.07536744 0.16314249 0.03783527 0.\n",
            "  0.         0.         0.         0.07876024 0.         0.21677004\n",
            "  0.02348447 0.23368259 0.1374804  0.01240956 0.1182322  0.\n",
            "  0.353916   0.         0.26437744 0.         0.         0.\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'> (121, 145, 121)\n",
            "(121, 145, 121)\n",
            "input shape=torch.Size([1, 121, 145, 1])\n",
            "1 torch.Size([1, 121, 145, 1]) OAS30720_MR_d4213\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_axial/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d4213_cdr1.0/82_conv1_71/\n",
            "torch.Size([1, 121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(1.6313387, 0.07323715)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(5.7638392, 0.6531695)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.84985703 0.7527417  0.7461109  ... 0.7268853  0.7614306  0.75633585]\n",
            " [0.8163176  0.7215058  0.6758888  ... 0.6812862  0.7248418  0.77615654]\n",
            " [0.8295602  0.7480034  0.7294172  ... 0.7128835  0.76894474 0.7629746 ]\n",
            " ...\n",
            " [0.80941534 0.73408467 0.7137027  ... 0.7149269  0.7588714  0.7376536 ]\n",
            " [0.83640826 0.79853326 0.77250785 ... 0.7817971  0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.7957859  ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(4.366294, 0.9831993)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.2011846 1.2449349 1.1922444 1.2601694 1.248679  1.4123223 1.4452645\n",
            "  1.5532562 1.5253552 1.4766791 1.580618  1.5369025 1.5930957 1.3846825\n",
            "  1.3154001 1.2261602 1.2246915 1.2268602 1.1641481]\n",
            " [1.1699919 1.1701353 1.1709268 1.2314212 1.5109395 1.726008  1.8389736\n",
            "  2.0660822 2.08882   2.1020834 2.0870697 2.037404  1.9552044 1.613686\n",
            "  1.3791419 1.2459555 1.1053338 1.1562877 1.1475075]\n",
            " [1.0602189 1.093665  1.1339792 1.3258251 1.6548673 2.326261  2.4779053\n",
            "  2.5950425 2.6958733 2.971745  2.8532648 2.7633257 2.4704368 2.0109284\n",
            "  1.6726531 1.412475  1.0939167 1.067377  1.1434253]\n",
            " [1.0880129 1.1056257 1.1239871 1.532195  2.1906018 2.6838512 3.3107162\n",
            "  3.1716297 3.4080615 3.3609133 3.7860937 3.4849768 2.994162  2.5124516\n",
            "  2.162626  1.6856802 1.2067438 1.0602089 1.1296581]\n",
            " [1.1161096 1.177889  1.3238827 1.7825068 2.2790933 2.660399  3.2011285\n",
            "  3.231605  2.9971983 3.406869  3.7371778 3.914646  3.8502436 3.5801902\n",
            "  2.5573487 2.1965008 1.4210403 1.1300694 1.1440368]\n",
            " [1.070292  1.1106435 1.4285529 1.9411999 2.329168  2.6595654 2.576818\n",
            "  2.851347  2.966457  2.8922644 3.2440097 3.4630945 3.72818   3.814138\n",
            "  3.1017568 2.1696033 1.7218728 1.153588  1.1402562]\n",
            " [1.1322294 1.1610907 1.6277509 2.336913  2.8441324 2.7038078 2.7729506\n",
            "  2.8094783 2.865323  2.4998105 3.1759906 3.6429596 3.4098113 3.6822329\n",
            "  3.4084716 2.5970387 1.8616139 1.4095582 1.1416537]\n",
            " [1.0697    1.1966138 1.6649086 2.6688547 2.8455548 3.2722933 3.2545745\n",
            "  3.0208645 3.21977   2.9817193 3.054348  3.4681213 3.4670196 3.5537024\n",
            "  3.569217  2.7015312 2.1239352 1.42992   1.201871 ]\n",
            " [1.0060087 1.0799242 1.4761795 2.2647834 2.9921339 2.8215532 3.2370384\n",
            "  3.3593364 3.1506028 3.5827036 3.6883316 4.1324053 3.8792243 3.981102\n",
            "  3.2236876 2.7684183 1.961773  1.2674522 1.185321 ]\n",
            " [1.0937899 1.0517572 1.4269164 2.0629716 2.7759783 2.8868823 2.5178392\n",
            "  3.2394018 3.2550395 3.5091796 3.8396034 3.7802653 3.7456913 3.7153897\n",
            "  3.1315746 2.4232154 1.7292379 1.3799797 1.2013422]\n",
            " [1.0860969 1.0052975 1.2561584 1.6706873 2.4863107 2.7928803 2.6179016\n",
            "  2.639408  3.0519795 3.7361403 4.047035  4.3422403 3.7586508 3.1647499\n",
            "  2.7903395 2.0147502 1.3096507 1.1490723 1.1113772]\n",
            " [1.0560666 1.1180817 1.1139768 1.4891797 2.1009512 2.713431  3.1164725\n",
            "  3.01399   3.0172896 3.7096987 4.366294  4.2903466 4.2672124 2.9502103\n",
            "  2.2333183 1.7022845 1.1781905 1.1115571 1.1278021]\n",
            " [1.0767351 1.0141461 1.0259506 1.30179   1.5526662 2.070264  2.5689297\n",
            "  2.6955578 2.5858264 3.1125169 4.00202   3.9027185 3.1176138 2.5282829\n",
            "  1.8176637 1.3586496 1.0912801 1.0795525 1.0947434]\n",
            " [1.0500038 1.0754585 1.0892357 1.2275559 1.2510937 1.5930262 1.7367333\n",
            "  2.0738435 2.230398  2.5009537 2.6550264 2.5137224 2.3931072 2.05223\n",
            "  1.4750296 1.2394048 1.032928  1.0781654 1.1221298]\n",
            " [1.0883685 1.1006733 1.0267216 1.0667275 1.0803808 1.2415012 1.374575\n",
            "  1.4344605 1.581794  1.7080292 1.9973663 1.8670675 1.7586368 1.5806206\n",
            "  1.2362881 1.1189014 0.9831994 1.1054202 1.1437471]\n",
            " [1.0137136 1.0694696 1.0562338 1.0291593 1.0072594 1.089191  1.1003071\n",
            "  1.1181105 1.1312046 1.1803951 1.323615  1.2685794 1.1858307 1.1237282\n",
            "  1.0915263 1.0305423 1.0065602 1.0698961 1.0780872]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(4.9107256, 1.6907487)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.8136296 2.0441146 2.2081614 2.4946246 2.4017797 2.5139875 2.326354\n",
            "  2.2281618 1.9508047 1.6907488]\n",
            " [2.0343204 2.5201423 2.7125127 3.0911791 3.2394428 3.4468737 2.9591515\n",
            "  2.7894812 2.2944741 1.835229 ]\n",
            " [2.1567788 2.7376184 3.1333845 3.8527431 3.965213  3.9619815 3.9856\n",
            "  3.3363576 2.5830462 1.9057167]\n",
            " [2.1210322 3.0054383 3.4287977 3.9272683 3.9450657 4.1612782 3.927153\n",
            "  3.731562  2.8147688 1.9525138]\n",
            " [2.164908  2.8499002 3.6410592 4.2098355 4.65678   4.9107256 4.6392994\n",
            "  3.9156313 2.7041564 1.9156189]\n",
            " [2.0815372 2.495147  3.089201  3.5599332 4.2277355 4.658126  4.356265\n",
            "  3.23528   2.4454994 1.9127775]\n",
            " [1.9442824 2.3654547 2.5016751 3.083776  3.3833532 3.999031  3.6186953\n",
            "  2.8425179 2.3245416 1.7944566]\n",
            " [1.7354356 1.9012975 2.0369508 2.3461213 2.668601  2.7472148 2.6343691\n",
            "  2.202317  1.8984752 1.695357 ]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(2.2699594, 0.8515079)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.8793678  1.1244947  1.2547082  1.0924474  0.851508  ]\n",
            " [1.0102247  1.576104   1.9507765  1.6141859  1.0253369 ]\n",
            " [1.0051651  1.7385137  2.2699594  2.0020926  0.9675902 ]\n",
            " [0.86319125 1.0972179  1.4046906  1.4307636  0.910017  ]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[1.9964728e-01 1.4628801e-02 3.1367398e-04 5.6385949e-02 8.3271699e-04\n",
            "  1.2453515e-01 1.1132362e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 1.5893213e-02 1.6792934e-01 0.0000000e+00\n",
            "  2.1876624e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.9607895e-02\n",
            "  3.2837132e-01 0.0000000e+00 0.0000000e+00 1.5474360e-01 0.0000000e+00\n",
            "  1.8827783e-01 5.4177525e-03 0.0000000e+00 8.8001899e-03 0.0000000e+00\n",
            "  3.0267293e-02 0.0000000e+00 4.9091175e-02 2.4807418e-02 9.4125550e-03\n",
            "  0.0000000e+00 2.1318224e-01 0.0000000e+00 1.3210583e-01 0.0000000e+00\n",
            "  0.0000000e+00 6.6048428e-02 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  1.6235615e-01 0.0000000e+00 2.9966083e-01 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.9933019e-02\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8350315e-01\n",
            "  2.1877287e-02 4.8710883e-02 0.0000000e+00 0.0000000e+00 5.7935666e-02\n",
            "  0.0000000e+00 1.2301108e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 2.8481346e-02 6.9734491e-02 3.5909884e-02\n",
            "  0.0000000e+00 0.0000000e+00 7.1644485e-02 0.0000000e+00 7.7921726e-02\n",
            "  0.0000000e+00 0.0000000e+00 1.2816142e-01 0.0000000e+00 3.3125356e-02\n",
            "  9.1041572e-02 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5530254e-03\n",
            "  2.0136118e-02 0.0000000e+00 8.2645871e-02 3.3715373e-01 1.8545796e-01\n",
            "  4.3283790e-02 0.0000000e+00 4.3526717e-02 5.0683049e-03 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 1.4431818e-01 1.5172070e-01 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 1.2714496e-01 6.6044115e-02 2.1679753e-01 1.0911050e-01\n",
            "  1.3983342e-01 0.0000000e+00 0.0000000e+00 1.4643164e-01 1.5330296e-02\n",
            "  2.2558577e-01 0.0000000e+00 1.3132137e-01 5.9860151e-02 8.9326473e-03\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n",
            "torch.Size([145, 1, 121])\n",
            "/content/drive/My Drive/BA_Estimation/results/sal_map_sagittal/23-08-2020-01-21_smoe_maps_blockend_scale_endlayers_equal_weights_exp2_exp2/OAS30720_MR_d4213_cdr1.0/82_conv1_71/\n",
            "torch.Size([121, 145, 1])\n",
            "3 0 activation_55 (None, 61, 73, 64)\n",
            "7 1 activation_56 (None, 31, 37, 64)\n",
            "10 2 activation_57 (None, 31, 37, 64)\n",
            "12 3 activation_58 (None, 31, 37, 64)\n",
            "15 4 activation_59 (None, 31, 37, 64)\n",
            "18 5 activation_60 (None, 31, 37, 64)\n",
            "20 6 activation_61 (None, 31, 37, 64)\n",
            "23 7 activation_62 (None, 31, 37, 64)\n",
            "26 8 activation_63 (None, 31, 37, 64)\n",
            "28 9 activation_64 (None, 31, 37, 64)\n",
            "31 10 activation_65 (None, 16, 19, 128)\n",
            "36 11 activation_66 (None, 16, 19, 128)\n",
            "37 12 activation_67 (None, 16, 19, 128)\n",
            "39 13 activation_68 (None, 16, 19, 128)\n",
            "42 14 activation_69 (None, 16, 19, 128)\n",
            "45 15 activation_70 (None, 16, 19, 128)\n",
            "47 16 activation_71 (None, 16, 19, 128)\n",
            "50 17 activation_72 (None, 16, 19, 128)\n",
            "53 18 activation_73 (None, 16, 19, 128)\n",
            "55 19 activation_74 (None, 16, 19, 128)\n",
            "58 20 activation_75 (None, 16, 19, 128)\n",
            "61 21 activation_76 (None, 16, 19, 128)\n",
            "63 22 activation_77 (None, 16, 19, 128)\n",
            "66 23 activation_78 (None, 8, 10, 256)\n",
            "71 24 activation_79 (None, 8, 10, 256)\n",
            "72 25 activation_80 (None, 8, 10, 256)\n",
            "74 26 activation_81 (None, 8, 10, 256)\n",
            "77 27 activation_82 (None, 8, 10, 256)\n",
            "80 28 activation_83 (None, 8, 10, 256)\n",
            "82 29 activation_84 (None, 8, 10, 256)\n",
            "85 30 activation_85 (None, 8, 10, 256)\n",
            "88 31 activation_86 (None, 8, 10, 256)\n",
            "90 32 activation_87 (None, 8, 10, 256)\n",
            "93 33 activation_88 (None, 8, 10, 256)\n",
            "96 34 activation_89 (None, 8, 10, 256)\n",
            "98 35 activation_90 (None, 8, 10, 256)\n",
            "101 36 activation_91 (None, 8, 10, 256)\n",
            "104 37 activation_92 (None, 8, 10, 256)\n",
            "106 38 activation_93 (None, 8, 10, 256)\n",
            "109 39 activation_94 (None, 8, 10, 256)\n",
            "112 40 activation_95 (None, 8, 10, 256)\n",
            "114 41 activation_96 (None, 8, 10, 256)\n",
            "117 42 activation_97 (None, 4, 5, 512)\n",
            "122 43 activation_98 (None, 4, 5, 512)\n",
            "123 44 activation_99 (None, 4, 5, 512)\n",
            "125 45 activation_100 (None, 4, 5, 512)\n",
            "128 46 activation_101 (None, 4, 5, 512)\n",
            "131 47 activation_102 (None, 4, 5, 512)\n",
            "133 48 activation_103 (None, 4, 5, 512)\n",
            "136 49 activation_104 (None, 4, 5, 512)\n",
            "139 50 activation_105 (None, 4, 5, 512)\n",
            "141 51 activation_106 (None, 4, 5, 512)\n",
            "144 52 activation_107 (None, 512)\n",
            "147 53 activation_108 (None, 256)\n",
            "150 54 activation_109 (None, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 61, 73, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 31, 37, 64)\n",
            "ouput shapes layerwise\n",
            "(1, 16, 19, 128)\n",
            "ouput shapes layerwise\n",
            "(1, 8, 10, 256)\n",
            "ouput shapes layerwise\n",
            "(1, 4, 5, 512)\n",
            " smoe input shape=(1, 61, 73)\n",
            "x range=(2.0851674, 0.055244382)\n",
            "False\n",
            "((1, 61, 73), (61, 73), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " ...\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]\n",
            " [0.2205804 0.2205804 0.2205804 ... 0.2205804 0.2205804 0.2205804]]\n",
            "smoe output shape=(61, 73)\n",
            "torch.Size([1, 4453])\n",
            " smoe input shape=(1, 31, 37)\n",
            "x range=(6.584508, 0.6567914)\n",
            "False\n",
            "((1, 31, 37), (31, 37), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " ...\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 ... 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[0.85123986 0.7620476  0.7474026  ... 0.72688526 0.76143056 0.7563359 ]\n",
            " [0.81645846 0.7246375  0.67085993 ... 0.68125796 0.72482896 0.7761551 ]\n",
            " [0.83308506 0.7332759  0.74984896 ... 0.71366775 0.76846194 0.76267374]\n",
            " ...\n",
            " [0.8094154  0.7341145  0.7135403  ... 0.71492934 0.75887144 0.73765373]\n",
            " [0.8364083  0.79853344 0.7725076  ... 0.78179705 0.81395006 0.7831061 ]\n",
            " [0.8018056  0.75318444 0.79578596 ... 0.7986542  0.8212099  0.8809458 ]]\n",
            "smoe output shape=(31, 37)\n",
            "torch.Size([1, 1147])\n",
            " smoe input shape=(1, 16, 19)\n",
            "x range=(5.3374352, 0.99518967)\n",
            "False\n",
            "((1, 16, 19), (16, 19), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07\n",
            "  1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.2829276 1.7209134 2.5172276 3.2247581 3.9694357 4.1003747 3.8994212\n",
            "  3.670443  3.232895  3.014948  2.6357741 2.2290096 1.9725001 1.7507393\n",
            "  1.5205938 1.305854  1.2585126 1.2487403 1.2008389]\n",
            " [1.289097  1.9049002 3.0530195 3.787446  5.165358  5.3374352 5.164508\n",
            "  5.094266  4.1571865 3.8088267 3.2446775 2.773259  2.4872327 2.211659\n",
            "  1.7920808 1.3608345 1.2554396 1.230568  1.2176353]\n",
            " [1.2868739 1.8994931 3.2412972 4.5379553 5.26805   5.165679  5.228152\n",
            "  5.225913  4.799714  5.0941315 4.5290155 3.7688875 3.3041906 2.5975308\n",
            "  1.9371282 1.6539745 1.4192463 1.3251759 1.2515355]\n",
            " [1.4259158 1.9520887 3.3253646 4.4191685 4.683735  5.2716765 5.2083797\n",
            "  4.904661  4.5720587 4.7884855 4.530669  4.651868  3.8185835 3.327134\n",
            "  2.5838866 2.2654972 1.974398  1.8599344 1.4745994]\n",
            " [1.3918949 2.0706272 2.87852   3.7164245 4.3392143 4.9499836 4.692971\n",
            "  4.726273  4.929638  4.4941177 4.626555  4.1649175 4.415767  3.5104418\n",
            "  3.2191887 2.938883  2.5915575 2.3736172 1.7980901]\n",
            " [1.4923782 2.1794224 3.012939  3.4416804 3.535647  4.16603   4.0090876\n",
            "  4.2788277 4.51993   4.8800545 4.5249805 4.650445  4.75504   4.163642\n",
            "  3.9954505 3.9400518 3.691154  2.6692991 1.8926858]\n",
            " [1.5677606 2.1782737 3.2103326 3.401917  3.5591006 3.2062778 3.234313\n",
            "  2.9968433 4.202306  4.3049664 4.3206644 4.3170176 4.5498095 4.5273533\n",
            "  3.8935597 4.3577642 3.8466983 2.8556852 1.8698472]\n",
            " [1.4674081 2.034895  2.8758898 3.3124802 3.3678298 2.9180834 3.0521777\n",
            "  2.7355227 3.3473125 3.5222087 3.8952558 3.857976  3.8601782 3.7830229\n",
            "  4.3477926 4.088834  3.527103  2.5947454 1.6745735]\n",
            " [1.2257506 1.7267298 2.5176415 3.1804924 3.2089372 2.9935632 3.0153413\n",
            "  2.5089285 2.7237394 2.7509367 3.051385  3.2631323 3.539032  3.5962493\n",
            "  3.735958  3.5474882 2.9013662 2.1381025 1.6680712]\n",
            " [1.0758284 1.4439989 1.9753801 2.6910322 3.2353537 2.9036403 2.5502286\n",
            "  2.6009645 2.3115983 2.7241817 2.844926  3.1298513 3.5587509 3.9154332\n",
            "  3.5234318 2.9627383 2.0401056 1.774146  1.391467 ]\n",
            " [1.0733149 1.2918057 1.7407137 2.2796645 3.0650601 3.3441646 2.5755758\n",
            "  2.3677614 3.043035  2.9091058 3.6557293 3.7619967 4.016835  3.699761\n",
            "  3.4231915 2.5159256 1.8276571 1.5205876 1.1910151]\n",
            " [1.0643963 1.1651005 1.3581159 1.6818982 2.170777  2.7031512 2.5213013\n",
            "  2.327434  2.38131   3.65502   3.4152737 4.0756226 3.9136617 3.2096987\n",
            "  2.5632582 1.8382915 1.5982034 1.1924638 1.1477426]\n",
            " [1.0391318 1.1360668 1.1938316 1.3927603 1.4898142 1.9043275 1.8718773\n",
            "  2.1324825 2.6065357 2.7232556 3.2049575 3.0972478 3.1021948 2.528006\n",
            "  1.8744478 1.6628995 1.2074077 1.0836929 1.1367724]\n",
            " [1.0418624 1.0622021 1.1071321 1.1730769 1.3110853 1.4562141 1.5298891\n",
            "  1.6815039 1.8636469 2.1496496 2.1454437 2.3751485 2.244399  1.7401447\n",
            "  1.6681327 1.3101966 1.0870318 1.0992769 1.1268727]\n",
            " [1.0794084 1.0836903 1.0459245 1.0240159 1.1416606 1.1227111 1.198521\n",
            "  1.2541354 1.4047257 1.499156  1.5509865 1.561215  1.6449465 1.4097891\n",
            "  1.2238193 1.0897352 0.9951898 1.0967369 1.1178449]\n",
            " [1.019864  1.0706857 1.0761268 1.0288144 1.03392   1.028221  1.0432392\n",
            "  1.0649984 1.0593442 1.168441  1.1952451 1.1452655 1.0640041 1.0660918\n",
            "  1.0376256 1.019447  1.0230818 1.0610524 1.0752465]]\n",
            "smoe output shape=(16, 19)\n",
            "torch.Size([1, 304])\n",
            " smoe input shape=(1, 8, 10)\n",
            "x range=(5.9113655, 1.744895)\n",
            "False\n",
            "((1, 8, 10), (8, 10), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[2.6551614 3.6353488 4.7060604 4.7611933 4.1203856 3.8964345 2.9477692\n",
            "  2.6469922 2.1824255 1.9306022]\n",
            " [2.8613677 4.3756123 5.3871593 5.5383644 5.326561  5.038535  4.19685\n",
            "  3.3745475 2.837254  2.278141 ]\n",
            " [2.8776317 4.280249  5.145611  5.882208  5.9113655 5.781677  5.0675383\n",
            "  4.781008  3.4090664 2.5677037]\n",
            " [2.85351   4.0588603 4.0162506 4.6496964 5.0336704 5.2493815 5.3561783\n",
            "  4.8844986 4.0345764 2.7132363]\n",
            " [2.5015163 3.191091  3.610577  3.8512049 4.278658  4.614092  4.4959807\n",
            "  4.551295  3.447381  2.4170341]\n",
            " [2.040137  2.766867  3.2504246 3.6525452 4.127076  4.44282   4.3685174\n",
            "  3.689504  2.9775984 2.0536447]\n",
            " [2.0175247 2.4344945 2.4690022 2.9334655 3.2263813 3.5955794 3.5524917\n",
            "  2.968091  2.4498034 1.8707193]\n",
            " [1.7466044 1.9571496 2.0134296 2.2375536 2.4067578 2.5692427 2.6506584\n",
            "  2.2990873 2.0348022 1.7448951]]\n",
            "smoe output shape=(8, 10)\n",
            "torch.Size([1, 80])\n",
            " smoe input shape=(1, 4, 5)\n",
            "x range=(2.1900856, 0.84828734)\n",
            "False\n",
            "((1, 4, 5), (4, 5), 1e-07)\n",
            "smoe map=[[1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]\n",
            " [1.e-07 1.e-07 1.e-07 1.e-07 1.e-07]]\n",
            "mean=[[1.058935   1.6361767  1.5984465  1.1928841  0.89715165]\n",
            " [1.2615222  2.1900856  2.0729446  1.6312515  1.0463723 ]\n",
            " [1.0492505  1.5791391  1.7726803  1.5505093  1.0718381 ]\n",
            " [0.84828746 1.0018849  1.1611161  1.1878083  0.86843324]]\n",
            "smoe output shape=(4, 5)\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 61, 73])\n",
            "torch.Size([1, 31, 37])\n",
            "torch.Size([1, 16, 19])\n",
            "torch.Size([1, 8, 10])\n",
            "torch.Size([1, 4, 5])\n",
            "predictions=[[0.20813271 0.05906065 0.04067586 0.03860343 0.         0.05940348\n",
            "  0.06898543 0.         0.         0.         0.         0.\n",
            "  0.06090251 0.15795743 0.         0.22328822 0.         0.01800968\n",
            "  0.         0.05498673 0.35127923 0.         0.         0.1320688\n",
            "  0.         0.16890074 0.         0.         0.01112654 0.\n",
            "  0.01365802 0.         0.05451579 0.04052467 0.00989858 0.\n",
            "  0.20390116 0.         0.12080902 0.         0.         0.08001896\n",
            "  0.         0.         0.         0.17837836 0.         0.2826933\n",
            "  0.         0.         0.         0.01791369 0.         0.\n",
            "  0.025586   0.         0.         0.         0.         0.18940787\n",
            "  0.03558727 0.05698583 0.         0.         0.04391278 0.\n",
            "  0.1409618  0.         0.         0.         0.         0.\n",
            "  0.00483453 0.07496769 0.03515504 0.         0.         0.0791311\n",
            "  0.         0.08430786 0.         0.         0.12364367 0.\n",
            "  0.05865213 0.05808626 0.         0.         0.         0.02597823\n",
            "  0.02995406 0.         0.06779846 0.30321562 0.18295719 0.06550899\n",
            "  0.00368307 0.02640566 0.0208052  0.         0.         0.\n",
            "  0.11357471 0.18277274 0.         0.04214681 0.         0.\n",
            "  0.         0.         0.         0.13526373 0.03227232 0.22790487\n",
            "  0.11265083 0.1689957  0.         0.         0.14142226 0.05344648\n",
            "  0.23924525 0.         0.15062474 0.07262337 0.         0.\n",
            "  0.         0.        ]]\n",
            "entering tape gradients\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "Crossed tape gradients\n",
            "Entering reduce mean using guided_grads with shape=(4, 5, 512)\n",
            "Computing CAM using output with shape:(4, 5, 512)\n",
            "weights=()\n",
            "(4, 5, 512)\n",
            "cam shape=(4, 5)\n",
            "(1, 121, 145, 1)\n",
            "(121, 145)\n",
            "heatmap_gcam shape=(121, 145, 1)\n",
            "grads shape =(4, 5, 512),tf.exp(loss) shape=(128,)\n",
            "conv_first_grad shape=(4, 5, 512),output.shape=(4, 5, 512),conv_second_grad shape=(4, 5, 512) ,  conv_third_grad shape=(4, 5, 512), global_sum.shape=(512,)  \n",
            "alphas_thresholding shape=(4, 5, 512)\n",
            "alpha_normalization_constant_processed shape=(512,)\n",
            "cam_map=(121, 145, 1)\n",
            "(1, 121, 145, 1) (121, 145) (121, 145) <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(121, 145) torch.Size([1, 121, 145]) (121, 145) (121, 145)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrMDCg6AY2Uh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "096e3c2c-e682-4921-b062-1ebc4d5c1db5"
      },
      "source": [
        "layers=[l.name for l in model.layers]\n",
        "lout=[]\n",
        "for i,l in enumerate(layers):\n",
        "  if l.startswith('activation'):\n",
        "    lout.append(l)\n",
        "for i,l in enumerate(lout):\n",
        "  print(i,l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 activation\n",
            "1 activation_1\n",
            "2 activation_2\n",
            "3 activation_4\n",
            "4 activation_6\n",
            "5 activation_3\n",
            "6 activation_5\n",
            "7 activation_7\n",
            "8 activation_8\n",
            "9 activation_10\n",
            "10 activation_12\n",
            "11 activation_9\n",
            "12 activation_11\n",
            "13 activation_13\n",
            "14 activation_14\n",
            "15 activation_15\n",
            "16 activation_16\n",
            "17 activation_17\n",
            "18 activation_19\n",
            "19 activation_21\n",
            "20 activation_18\n",
            "21 activation_20\n",
            "22 activation_22\n",
            "23 activation_23\n",
            "24 activation_25\n",
            "25 activation_27\n",
            "26 activation_24\n",
            "27 activation_26\n",
            "28 activation_28\n",
            "29 activation_29\n",
            "30 activation_30\n",
            "31 activation_31\n",
            "32 activation_32\n",
            "33 activation_34\n",
            "34 activation_36\n",
            "35 activation_33\n",
            "36 activation_35\n",
            "37 activation_37\n",
            "38 activation_38\n",
            "39 activation_40\n",
            "40 activation_42\n",
            "41 activation_39\n",
            "42 activation_41\n",
            "43 activation_43\n",
            "44 activation_44\n",
            "45 activation_45\n",
            "46 activation_46\n",
            "47 activation_47\n",
            "48 activation_49\n",
            "49 activation_51\n",
            "50 activation_48\n",
            "51 activation_50\n",
            "52 activation_52\n",
            "53 activation_53\n",
            "54 activation_55\n",
            "55 activation_57\n",
            "56 activation_54\n",
            "57 activation_56\n",
            "58 activation_58\n",
            "59 activation_59\n",
            "60 activation_60\n",
            "61 activation_61\n",
            "62 activation_62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxTtcaORydjn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ae85b957-5988-4c9c-91dc-73c614358d45"
      },
      "source": [
        "dt_string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'14-08-2020-14-50_smoe_maps_blockend_scale_endlayers_equal_weights'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oIcTFUTarD0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c373449f-6001-46a5-c164-d4c6915ab3d2"
      },
      "source": [
        "model.summary() # 0,2,17,47,62"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BioAgeNet_Regression\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 121, 145, 6, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d (Conv3D)                 (None, 61, 73, 6, 64 1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 61, 73, 6, 64 192         conv3d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 61, 73, 6, 64 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "MaxPool2d_2a_3x3 (MaxPooling3D) (None, 31, 37, 6, 64 0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_1 (Conv3D)               (None, 31, 37, 6, 64 110592      MaxPool2d_2a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 31, 37, 6, 64 192         conv3d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 31, 37, 6, 64 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_2 (Conv3D)               (None, 31, 37, 6, 19 331776      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 31, 37, 6, 19 576         conv3d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 31, 37, 6, 19 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "MaxPool2d_3a_3x3 (MaxPooling3D) (None, 16, 19, 6, 19 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_4 (Conv3D)               (None, 16, 19, 6, 96 18432       MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_6 (Conv3D)               (None, 16, 19, 6, 16 3072        MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 19, 6, 96 288         conv3d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 19, 6, 16 48          conv3d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 19, 6, 96 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 19, 6, 16 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D)    (None, 16, 19, 6, 19 0           MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_3 (Conv3D)               (None, 16, 19, 6, 64 12288       MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_5 (Conv3D)               (None, 16, 19, 6, 12 331776      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_7 (Conv3D)               (None, 16, 19, 6, 32 13824       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_8 (Conv3D)               (None, 16, 19, 6, 32 6144        max_pooling3d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 19, 6, 64 192         conv3d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 19, 6, 12 384         conv3d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 19, 6, 32 96          conv3d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 19, 6, 32 96          conv3d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 19, 6, 64 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 19, 6, 12 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 19, 6, 32 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 19, 6, 32 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b (Concatenate)          (None, 16, 19, 6, 25 0           activation_3[0][0]               \n",
            "                                                                 activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_10 (Conv3D)              (None, 16, 19, 6, 12 32768       Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_12 (Conv3D)              (None, 16, 19, 6, 32 8192        Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 19, 6, 12 384         conv3d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 19, 6, 32 96          conv3d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 19, 6, 12 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 19, 6, 32 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3D)  (None, 16, 19, 6, 25 0           Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_9 (Conv3D)               (None, 16, 19, 6, 12 32768       Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_11 (Conv3D)              (None, 16, 19, 6, 19 663552      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_13 (Conv3D)              (None, 16, 19, 6, 96 82944       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_14 (Conv3D)              (None, 16, 19, 6, 64 16384       max_pooling3d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 19, 6, 12 384         conv3d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 19, 6, 19 576         conv3d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 19, 6, 96 288         conv3d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 19, 6, 64 192         conv3d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 19, 6, 12 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 19, 6, 19 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 19, 6, 96 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 19, 6, 64 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c (Concatenate)          (None, 16, 19, 6, 48 0           activation_9[0][0]               \n",
            "                                                                 activation_11[0][0]              \n",
            "                                                                 activation_13[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_15 (Conv3D)              (None, 16, 19, 6, 16 7680        Mixed_3c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 19, 6, 16 0           conv3d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_16 (Conv3D)              (None, 16, 19, 6, 64 1024        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_17 (Conv3D)              (None, 16, 19, 6, 64 27648       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 19, 6, 64 0           conv3d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 19, 6, 64 0           conv3d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 16, 19, 6, 12 0           activation_16[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3D)  (None, 8, 10, 6, 128 0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_19 (Conv3D)              (None, 8, 10, 6, 96) 12288       max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_21 (Conv3D)              (None, 8, 10, 6, 16) 2048        max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 10, 6, 96) 288         conv3d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 10, 6, 16) 48          conv3d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 10, 6, 96) 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 10, 6, 16) 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3D)  (None, 8, 10, 6, 128 0           max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_18 (Conv3D)              (None, 8, 10, 6, 192 24576       max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_20 (Conv3D)              (None, 8, 10, 6, 208 539136      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_22 (Conv3D)              (None, 8, 10, 6, 48) 20736       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_23 (Conv3D)              (None, 8, 10, 6, 64) 8192        max_pooling3d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 10, 6, 192 576         conv3d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 10, 6, 208 624         conv3d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 10, 6, 48) 144         conv3d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 10, 6, 64) 192         conv3d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 10, 6, 192 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 10, 6, 208 0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 10, 6, 48) 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 10, 6, 64) 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b (Concatenate)          (None, 8, 10, 6, 512 0           activation_18[0][0]              \n",
            "                                                                 activation_20[0][0]              \n",
            "                                                                 activation_22[0][0]              \n",
            "                                                                 activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_25 (Conv3D)              (None, 8, 10, 6, 112 57344       Mixed_4b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_27 (Conv3D)              (None, 8, 10, 6, 24) 12288       Mixed_4b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 10, 6, 112 336         conv3d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 10, 6, 24) 72          conv3d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 8, 10, 6, 112 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 10, 6, 24) 0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_4 (MaxPooling3D)  (None, 8, 10, 6, 512 0           Mixed_4b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_24 (Conv3D)              (None, 8, 10, 6, 160 81920       Mixed_4b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_26 (Conv3D)              (None, 8, 10, 6, 224 677376      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_28 (Conv3D)              (None, 8, 10, 6, 64) 41472       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_29 (Conv3D)              (None, 8, 10, 6, 64) 32768       max_pooling3d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 10, 6, 160 480         conv3d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 10, 6, 224 672         conv3d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 10, 6, 64) 192         conv3d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 10, 6, 64) 192         conv3d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 10, 6, 160 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 10, 6, 224 0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 10, 6, 64) 0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 10, 6, 64) 0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c (Concatenate)          (None, 8, 10, 6, 512 0           activation_24[0][0]              \n",
            "                                                                 activation_26[0][0]              \n",
            "                                                                 activation_28[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_30 (Conv3D)              (None, 8, 10, 6, 16) 8192        Mixed_4c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 10, 6, 16) 0           conv3d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_31 (Conv3D)              (None, 8, 10, 6, 64) 1024        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_32 (Conv3D)              (None, 8, 10, 6, 64) 27648       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 10, 6, 64) 0           conv3d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 10, 6, 64) 0           conv3d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 8, 10, 6, 128 0           activation_31[0][0]              \n",
            "                                                                 activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_34 (Conv3D)              (None, 8, 10, 6, 128 16384       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_36 (Conv3D)              (None, 8, 10, 6, 24) 3072        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 10, 6, 128 384         conv3d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 10, 6, 24) 72          conv3d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 10, 6, 128 0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 10, 6, 24) 0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_5 (MaxPooling3D)  (None, 8, 10, 6, 128 0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_33 (Conv3D)              (None, 8, 10, 6, 128 16384       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_35 (Conv3D)              (None, 8, 10, 6, 256 884736      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_37 (Conv3D)              (None, 8, 10, 6, 64) 41472       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_38 (Conv3D)              (None, 8, 10, 6, 64) 8192        max_pooling3d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 10, 6, 128 384         conv3d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 10, 6, 256 768         conv3d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 10, 6, 64) 192         conv3d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 10, 6, 64) 192         conv3d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 10, 6, 128 0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 10, 6, 256 0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 10, 6, 64) 0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 10, 6, 64) 0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d (Concatenate)          (None, 8, 10, 6, 512 0           activation_33[0][0]              \n",
            "                                                                 activation_35[0][0]              \n",
            "                                                                 activation_37[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_40 (Conv3D)              (None, 8, 10, 6, 144 73728       Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_42 (Conv3D)              (None, 8, 10, 6, 32) 16384       Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 10, 6, 144 432         conv3d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 10, 6, 32) 96          conv3d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 8, 10, 6, 144 0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 8, 10, 6, 32) 0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_6 (MaxPooling3D)  (None, 8, 10, 6, 512 0           Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_39 (Conv3D)              (None, 8, 10, 6, 112 57344       Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_41 (Conv3D)              (None, 8, 10, 6, 288 1119744     activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_43 (Conv3D)              (None, 8, 10, 6, 64) 55296       activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_44 (Conv3D)              (None, 8, 10, 6, 64) 32768       max_pooling3d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 10, 6, 112 336         conv3d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 10, 6, 288 864         conv3d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 10, 6, 64) 192         conv3d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 10, 6, 64) 192         conv3d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 10, 6, 112 0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 10, 6, 288 0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 8, 10, 6, 64) 0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 8, 10, 6, 64) 0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e (Concatenate)          (None, 8, 10, 6, 528 0           activation_39[0][0]              \n",
            "                                                                 activation_41[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_45 (Conv3D)              (None, 8, 10, 6, 32) 16896       Mixed_4e[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 8, 10, 6, 32) 0           conv3d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_46 (Conv3D)              (None, 8, 10, 6, 128 4096        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_47 (Conv3D)              (None, 8, 10, 6, 128 110592      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 8, 10, 6, 128 0           conv3d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 10, 6, 128 0           conv3d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 8, 10, 6, 256 0           activation_46[0][0]              \n",
            "                                                                 activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_7 (MaxPooling3D)  (None, 4, 5, 3, 256) 0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_49 (Conv3D)              (None, 4, 5, 3, 160) 40960       max_pooling3d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_51 (Conv3D)              (None, 4, 5, 3, 32)  8192        max_pooling3d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 5, 3, 160) 480         conv3d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 5, 3, 32)  96          conv3d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 5, 3, 160) 0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 5, 3, 32)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_8 (MaxPooling3D)  (None, 4, 5, 3, 256) 0           max_pooling3d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_48 (Conv3D)              (None, 4, 5, 3, 256) 65536       max_pooling3d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_50 (Conv3D)              (None, 4, 5, 3, 320) 1382400     activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_52 (Conv3D)              (None, 4, 5, 3, 128) 110592      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_53 (Conv3D)              (None, 4, 5, 3, 128) 32768       max_pooling3d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 4, 5, 3, 256) 768         conv3d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 5, 3, 320) 960         conv3d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 5, 3, 256) 0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 5, 3, 320) 0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f (Concatenate)          (None, 4, 5, 3, 832) 0           activation_48[0][0]              \n",
            "                                                                 activation_50[0][0]              \n",
            "                                                                 activation_52[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_55 (Conv3D)              (None, 4, 5, 3, 160) 133120      Mixed_4f[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_57 (Conv3D)              (None, 4, 5, 3, 32)  26624       Mixed_4f[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 5, 3, 160) 480         conv3d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 5, 3, 32)  96          conv3d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 4, 5, 3, 160) 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 4, 5, 3, 32)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_9 (MaxPooling3D)  (None, 4, 5, 3, 832) 0           Mixed_4f[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_54 (Conv3D)              (None, 4, 5, 3, 256) 212992      Mixed_4f[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_56 (Conv3D)              (None, 4, 5, 3, 320) 1382400     activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_58 (Conv3D)              (None, 4, 5, 3, 128) 110592      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_59 (Conv3D)              (None, 4, 5, 3, 128) 106496      max_pooling3d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 5, 3, 256) 768         conv3d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 5, 3, 320) 960         conv3d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 4, 5, 3, 256) 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 4, 5, 3, 320) 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b (Concatenate)          (None, 4, 5, 3, 832) 0           activation_54[0][0]              \n",
            "                                                                 activation_56[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_60 (Conv3D)              (None, 4, 5, 3, 64)  53248       Mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 4, 5, 3, 64)  0           conv3d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_61 (Conv3D)              (None, 4, 5, 3, 256) 16384       activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_62 (Conv3D)              (None, 4, 5, 3, 256) 442368      activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 4, 5, 3, 256) 0           conv3d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 4, 5, 3, 256) 0           conv3d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 4, 5, 3, 512) 0           activation_61[0][0]              \n",
            "                                                                 activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d (Globa (None, 512)          0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "gender_input (InputLayer)       [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 513)          0           global_average_pooling3d[0][0]   \n",
            "                                                                 gender_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          263168      concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          131328      dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          32896       dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "regression (Dense)              (None, 1)            129         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 10,276,929\n",
            "Trainable params: 10,264,897\n",
            "Non-trainable params: 12,032\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ehkd5szaWlg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b5e7ae7-3bd3-41ee-c6cb-bc53e868380a"
      },
      "source": [
        "layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fc25a12d438>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25a0f2320>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25a068208>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25a0686a0>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc25a068860>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25a068a90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25a068da0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25982c0b8>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25982c278>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25982c588>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25982c860>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc25982ca20>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25982cc50>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25982cf60>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2598342b0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2598345c0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259834898>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259834a20>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc259834be0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259834e10>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597be160>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597be470>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597be780>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597bea90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597beda0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597c50f0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597c5400>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597c56d8>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597c5860>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597c59e8>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597c5b70>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc2597c5d30>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597c5fd0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597cf320>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597cf630>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597cf940>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597cfc18>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597cfda0>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc2597cff60>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597d91d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597d94e0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597d97f0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597d9b00>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597d9e10>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597e1160>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597e1470>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597e1780>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597e1a58>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597e1be0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597e1d68>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597e1ef0>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc2597ea0f0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597ea390>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597ea668>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597ea828>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597eab38>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597eae10>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597eaf98>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc2597f7198>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc2597f7390>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597f75c0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597f78d0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597f7be0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597f7ef0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259781208>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259781390>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc259781550>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259781780>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259781a90>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259781da0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597890f0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259789400>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259789710>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259789a20>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259789d30>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259793048>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597931d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259793358>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597934e0>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc2597936a0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259793940>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259793c50>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259793f60>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25979a2b0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25979a588>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25979a710>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc25979a8d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25979ab00>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25979ae10>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597a5160>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597a5470>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597a5780>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597a5a90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597a5da0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc2597af0f0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597af3c8>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597af550>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597af6d8>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597af860>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc2597afa20>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597afcc0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597aff98>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597b6198>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597b64a8>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597b6780>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597b6908>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc2597b6ac8>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597b6cc0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597b6fd0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25973f320>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25973f630>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25973f908>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25973fa90>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc25973fc50>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25973fe80>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597461d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597464e0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597467f0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259746b00>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259746e10>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259752160>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259752470>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259752748>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597528d0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259752a58>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259752be0>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc259752da0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25975a080>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25975a390>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25975a6a0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25975a9b0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25975ac88>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25975ae10>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc25975afd0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259766240>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259766550>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259766860>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259766b70>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259766e80>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25976d1d0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25976d4e0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25976d7f0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25976dac8>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25976dc50>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25976ddd8>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25976df60>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc259777160>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259777400>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597776d8>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259777898>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259777ba8>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259777e80>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259701048>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc259701208>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc259701400>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259701630>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259701940>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259701c50>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259701f60>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259707278>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259707400>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc2597075c0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2597077f0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259707b00>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259707e10>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25970f160>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25970f470>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25970f780>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25970fa90>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25970fda0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25971a0b8>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25971a240>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25971a3c8>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc25971a550>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc25971a710>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25971a9b0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25971acc0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25971afd0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259723320>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597235f8>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259723780>,\n",
              " <tensorflow.python.keras.layers.pooling.MaxPooling3D at 0x7fc259723940>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259723b70>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259723e80>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25972b1d0>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc25972b4e0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25972b7f0>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25972bb00>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc25972be10>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fc259735160>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259735438>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597355c0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc259735748>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2597358d0>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc259735a90>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc259735d30>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2596bf048>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2596bf208>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv3D at 0x7fc2596bf518>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2596bf7f0>,\n",
              " <tensorflow.python.keras.layers.core.Activation at 0x7fc2596bf978>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc2596bfb38>,\n",
              " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling3D at 0x7fc2596bfd30>,\n",
              " <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fc2596bfef0>,\n",
              " <tensorflow.python.keras.layers.merge.Concatenate at 0x7fc2596c60f0>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fc2596c6208>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fc2596c6668>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fc2596c6908>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fc2596c6ba8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs9gksc6yhm6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "e1435d72-dba0-46f0-d752-f503e12262d0"
      },
      "source": [
        "!pip install innvestigate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: innvestigate in /usr/local/lib/python3.6/dist-packages (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from innvestigate) (2.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from innvestigate) (1.4.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from innvestigate) (3.6.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from innvestigate) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from innvestigate) (1.18.5)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (from innvestigate) (2.2.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from innvestigate) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->innvestigate) (1.15.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (8.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (1.9.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (19.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->innvestigate) (49.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->innvestigate) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->innvestigate) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->innvestigate) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJSZy2PqVI4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cf={'Pretrained_Model':{'path':'/content/drive/My Drive/BA_Estimation/models/exp2/age_net_oasis1_3.hdf5'},'Paths':\\\n",
        "      {'labels':'/content/drive/My Drive/BA_Estimation/csv_data/oasis1_oasis3_labels.csv',\\\n",
        "       'test_tfrecord':'/content/drive/My Drive/BA_Estimation/tf_records_data/testing_exp2'}}#testing_all_cdr\n",
        "model =  tf.python.keras.models.load_model(cf['Pretrained_Model']['path'],compile=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy2pVowky-30",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "86f3f94f-96f4-4748-e0dd-d92f612b0624"
      },
      "source": [
        "import innvestigate\n",
        "analyzer = innvestigate.create_analyzer(\"lrp.flat\", model) #,kwargs={'rule':'BoundedRule'})\n",
        "# dir(analyzer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py:110: UserWarning: LRP is only tested for convolutional neural networks.\n",
            "Check triggerd by layers: [<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f5cb5775400>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb57a71d0>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb57a7710>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb57a79e8>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb57a7ba8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb57a7dd8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f68128>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f68400>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4f685c0>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f688d0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f68ba8>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4f68d68>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4f68f98>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4ef32e8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4ef35f8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4ef3908>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ef3be0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ef3d68>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4ef3f28>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4efb198>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4efb4a8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4efb7b8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4efbac8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4efbdd8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f04128>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f04438>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f04748>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f04a20>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f04ba8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f04d30>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f04eb8>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4f0d0b8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4f0d358>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4f0d668>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f0d978>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f0dc88>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f0df60>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f12128>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4f122e8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4f12518>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4f12828>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4f12b38>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4f12e48>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f1e198>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f1e4a8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f1e7b8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f1eac8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f1eda0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f1ef28>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f210f0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4f21278>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4f21438>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4f216d8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4f21908>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4f21c18>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4f21f28>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4eb1278>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4eb1550>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4eb16d8>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4eb1898>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4eb1ac8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4eb1dd8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4eb9128>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4eb9438>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4eb9748>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4eb9a58>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4eb9d68>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4ebd0b8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ebd390>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ebd518>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ebd6a0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ebd828>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4ebd9e8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4ebdc88>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ebdf60>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4ed1160>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4ed1470>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ed1748>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ed18d0>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4ed1a90>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4ed1c88>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4ed1f98>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4edc2e8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4edc5f8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4edc8d0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4edca58>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4edcc18>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4edce48>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4ee2198>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4ee24a8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4ee27b8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4ee2ac8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4ee2dd8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e6c128>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e6c438>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e6c710>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e6c898>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e6ca20>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e6cba8>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4e6cd68>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e77048>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e77358>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e77668>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e77978>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e77c50>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e77dd8>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4e77f98>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e80208>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e80518>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e80828>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e80b38>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e80e48>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e88198>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e884a8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e887b8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e88a90>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e88c18>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e88da0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e88f28>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4e8f128>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e8f3c8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e8f6d8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e8f9e8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e8fcf8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e8ffd0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e96198>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4e96358>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e96588>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e96898>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e96ba8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e96eb8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4ea2208>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4ea2518>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4ea2828>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4ea2b38>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ea2e10>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ea2f98>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ea3160>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4ea32e8>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4ea34a8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4ea3748>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4ea3a58>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4ea3d68>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e330b8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e33390>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e33518>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4e336d8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e33908>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e33c18>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e33f28>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e3d278>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e3d588>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e3d898>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e3dba8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e3deb8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e491d0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e49358>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e494e0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e49668>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4e49828>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4e49ac8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e49cf8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e49fd0>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e501d0>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e504e0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e507b8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e50940>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4e50b00>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e50cf8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e58048>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e58358>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e58668>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e58940>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e58ac8>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4e58c88>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e58eb8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e62208>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e62518>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e62828>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e62b38>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e62e48>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4deb198>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4deb4a8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4deb780>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4deb908>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4deba90>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4debc18>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4debdd8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4df40b8>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4df43c8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4df46d8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4df49e8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4df4cc0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4df4e48>, <tensorflow.python.keras.layers.pooling.MaxPooling3D object at 0x7f5cb4dfe048>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4dfe278>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4dfe588>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4dfe898>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4dfeba8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4dfeeb8>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e06208>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e06518>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f5cb4e06828>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e06b00>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e06c88>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e06e10>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e06f98>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4e0f198>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e0f438>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e0f710>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e0f8d0>, <tensorflow.python.keras.layers.convolutional.Conv3D object at 0x7f5cb4e0fbe0>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e0feb8>, <tensorflow.python.keras.layers.core.Activation object at 0x7f5cb4e1a080>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4e1a240>, <tensorflow.python.keras.layers.pooling.GlobalAveragePooling3D object at 0x7f5cb4e1a438>, <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f5cb4e1a5f8>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x7f5cb4e1a7b8>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5cb4e1a8d0>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5cb4e1ad30>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5cb4e1afd0>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5cb4e212b0>]\n",
            "  warnings.warn(tmp_message)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF81xLeC5wri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c896401b-3f7e-471e-a965-fcc56e4f8d3e"
      },
      "source": [
        "label_path= cf['Paths']['labels'] #'/content/drive/My Drive/BA_Estimation/csv_data/oasis1_oasis3_labels.csv'#/media/shashanks/My Passport/documents/Master_Thesis_Backup/data/\n",
        "data_path= cf['Paths']['test_tfrecord'] #'/home/raarmak1/.shashanks/BA_estimation/tfrecords_data/OASIS1_3_combined/tfrecords_data_exp3/testing/'\n",
        "exp='exp2'\n",
        "debug_mode_subject= ['OAS30884']#\n",
        "# debug_mode_subject = ['OAS30884','OAS30720','OAS30344','OAS30541'] #OAS30911\n",
        "# debug_mode_subject=None\n",
        "test_patients,scan_ids, test_labels,test_gender,test_cdr = get_test_files(label_path,data_path,debug_mode_subject)\n",
        "tfr=tf.data.TFRecordDataset(test_patients)\n",
        "img_tf=tfr.map(map_func=lambda a:parse_function_image(a))\n",
        "# final_model.eval()\n",
        "# final_model.load_state_dict(inflated_model_dict)\n",
        "gender_dict={0:'Female',1:'Male'}\n",
        "for i,im in enumerate(img_tf): #OAS30686_d0030\n",
        "  print(type(im),im.shape)\n",
        "  img=im.numpy()\n",
        "  print(img.shape)\n",
        "  # if  'OAS30344'in scan_ids[i] or 'OAS30884' in scan_ids[i]  or 'OAS30911' in scan_ids[i]:\n",
        "  # if 'OAS30884' in scan_ids[i] :# OAS30720 or 'OAS30884' in scan_ids[i] : #  OAS30911_MR_d0099\n",
        "  max_intensity=0\n",
        "  csmap_list=[]\n",
        "  for chunk_id in [7,9,11,14,18]:#range(1,21):\n",
        "\n",
        "    start = (chunk_id-1)*6\n",
        "    end = chunk_id*6\n",
        "    \n",
        "    img_chunk=torch.tensor(img[:,:,start:end])\n",
        "    img_chunk = img_chunk.unsqueeze(0).numpy()\n",
        "    break"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'> (121, 145, 121)\n",
            "(121, 145, 121)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'> (121, 145, 121)\n",
            "(121, 145, 121)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'> (121, 145, 121)\n",
            "(121, 145, 121)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2Y3pAUE9bDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b80743f0-097f-4aba-c155-2b83577c392b"
      },
      "source": [
        "img_chunk.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 121, 145, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th9PLV7438Op",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "184f78ec-eb92-44a5-951d-8b190c44a38e"
      },
      "source": [
        "analysis = analyzer.analyze(img_chunk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-7ce6bfae27a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, X, neuron_selection)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \"\"\"\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_analyzer_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_analyzer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py\u001b[0m in \u001b[0;36mcreate_analyzer_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \"\"\"\n\u001b[1;32m    404\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_analysis_at_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analysis_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysis_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/innvestigate/analyzer/base.py\u001b[0m in \u001b[0;36m_prepare_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneuron_selection_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"max_activation\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0milayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"iNNvestigate_max\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_special_helper_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mneuron_selection_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 raise ValueError('Layer ' + self.name + ' was called with '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \"\"\"\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[1;32m    474\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TensorLike\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dense_tensor_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFlBNOm5VLRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1245b02d-e9c6-4086-ed17-afce9d039d9c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BioAgeNet_Regression\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 121, 145, 6, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d (Conv3D)                 (None, 61, 73, 6, 64 1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 61, 73, 6, 64 192         conv3d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 61, 73, 6, 64 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "MaxPool2d_2a_3x3 (MaxPooling3D) (None, 31, 37, 6, 64 0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_1 (Conv3D)               (None, 31, 37, 6, 64 110592      MaxPool2d_2a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 31, 37, 6, 64 192         conv3d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 31, 37, 6, 64 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_2 (Conv3D)               (None, 31, 37, 6, 19 331776      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 31, 37, 6, 19 576         conv3d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 31, 37, 6, 19 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "MaxPool2d_3a_3x3 (MaxPooling3D) (None, 16, 19, 6, 19 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_4 (Conv3D)               (None, 16, 19, 6, 96 497664      MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_6 (Conv3D)               (None, 16, 19, 6, 16 82944       MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 19, 6, 96 288         conv3d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 19, 6, 16 48          conv3d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 19, 6, 96 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 19, 6, 16 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D)    (None, 16, 19, 6, 19 0           MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_3 (Conv3D)               (None, 16, 19, 6, 64 331776      MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_5 (Conv3D)               (None, 16, 19, 6, 12 331776      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_7 (Conv3D)               (None, 16, 19, 6, 32 13824       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_8 (Conv3D)               (None, 16, 19, 6, 32 165888      max_pooling3d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 19, 6, 64 192         conv3d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 19, 6, 12 384         conv3d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 19, 6, 32 96          conv3d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 19, 6, 32 96          conv3d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 19, 6, 64 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 19, 6, 12 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 19, 6, 32 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 19, 6, 32 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b (Concatenate)          (None, 16, 19, 6, 25 0           activation_3[0][0]               \n",
            "                                                                 activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_10 (Conv3D)              (None, 16, 19, 6, 12 884736      Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_12 (Conv3D)              (None, 16, 19, 6, 32 221184      Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 19, 6, 12 384         conv3d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 19, 6, 32 96          conv3d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 19, 6, 12 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 19, 6, 32 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3D)  (None, 16, 19, 6, 25 0           Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_9 (Conv3D)               (None, 16, 19, 6, 12 884736      Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_11 (Conv3D)              (None, 16, 19, 6, 19 663552      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_13 (Conv3D)              (None, 16, 19, 6, 96 82944       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_14 (Conv3D)              (None, 16, 19, 6, 64 442368      max_pooling3d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 19, 6, 12 384         conv3d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 19, 6, 19 576         conv3d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 19, 6, 96 288         conv3d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 19, 6, 64 192         conv3d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 19, 6, 12 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 19, 6, 19 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 19, 6, 96 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 19, 6, 64 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c (Concatenate)          (None, 16, 19, 6, 48 0           activation_9[0][0]               \n",
            "                                                                 activation_11[0][0]              \n",
            "                                                                 activation_13[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3D)  (None, 8, 10, 3, 480 0           Mixed_3c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_16 (Conv3D)              (None, 8, 10, 3, 96) 1244160     max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_18 (Conv3D)              (None, 8, 10, 3, 16) 207360      max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 10, 3, 96) 288         conv3d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 10, 3, 16) 48          conv3d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 10, 3, 96) 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 10, 3, 16) 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3D)  (None, 8, 10, 3, 480 0           max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_15 (Conv3D)              (None, 8, 10, 3, 192 2488320     max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_17 (Conv3D)              (None, 8, 10, 3, 208 539136      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_19 (Conv3D)              (None, 8, 10, 3, 48) 20736       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_20 (Conv3D)              (None, 8, 10, 3, 64) 829440      max_pooling3d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 10, 3, 192 576         conv3d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 10, 3, 208 624         conv3d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 10, 3, 48) 144         conv3d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 10, 3, 192 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 10, 3, 208 0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 10, 3, 48) 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b (Concatenate)          (None, 8, 10, 3, 512 0           activation_15[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_19[0][0]              \n",
            "                                                                 activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_21 (Conv3D)              (None, 8, 10, 3, 16) 8192        Mixed_4b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 10, 3, 16) 0           conv3d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_22 (Conv3D)              (None, 8, 10, 3, 64) 1024        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_23 (Conv3D)              (None, 8, 10, 3, 64) 27648       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 10, 3, 64) 0           conv3d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 10, 3, 64) 0           conv3d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 8, 10, 3, 128 0           activation_22[0][0]              \n",
            "                                                                 activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_25 (Conv3D)              (None, 8, 10, 3, 112 387072      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_27 (Conv3D)              (None, 8, 10, 3, 24) 82944       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 10, 3, 112 336         conv3d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 10, 3, 24) 72          conv3d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 8, 10, 3, 112 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 10, 3, 24) 0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_4 (MaxPooling3D)  (None, 8, 10, 3, 128 0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_24 (Conv3D)              (None, 8, 10, 3, 160 552960      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_26 (Conv3D)              (None, 8, 10, 3, 224 677376      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_28 (Conv3D)              (None, 8, 10, 3, 64) 41472       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_29 (Conv3D)              (None, 8, 10, 3, 64) 221184      max_pooling3d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 10, 3, 160 480         conv3d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 10, 3, 224 672         conv3d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 10, 3, 160 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 10, 3, 224 0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c (Concatenate)          (None, 8, 10, 3, 512 0           activation_24[0][0]              \n",
            "                                                                 activation_26[0][0]              \n",
            "                                                                 activation_28[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_31 (Conv3D)              (None, 8, 10, 3, 128 1769472     Mixed_4c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_33 (Conv3D)              (None, 8, 10, 3, 24) 331776      Mixed_4c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 10, 3, 128 384         conv3d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 10, 3, 24) 72          conv3d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 10, 3, 128 0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 10, 3, 24) 0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_5 (MaxPooling3D)  (None, 8, 10, 3, 512 0           Mixed_4c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_30 (Conv3D)              (None, 8, 10, 3, 128 1769472     Mixed_4c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_32 (Conv3D)              (None, 8, 10, 3, 256 884736      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_34 (Conv3D)              (None, 8, 10, 3, 64) 41472       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_35 (Conv3D)              (None, 8, 10, 3, 64) 884736      max_pooling3d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 10, 3, 128 384         conv3d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 10, 3, 256 768         conv3d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 10, 3, 128 0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 10, 3, 256 0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d (Concatenate)          (None, 8, 10, 3, 512 0           activation_30[0][0]              \n",
            "                                                                 activation_32[0][0]              \n",
            "                                                                 activation_34[0][0]              \n",
            "                                                                 activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_37 (Conv3D)              (None, 8, 10, 3, 144 1990656     Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_39 (Conv3D)              (None, 8, 10, 3, 32) 442368      Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 10, 3, 144 432         conv3d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 10, 3, 32) 96          conv3d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 10, 3, 144 0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 10, 3, 32) 0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_6 (MaxPooling3D)  (None, 8, 10, 3, 512 0           Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_36 (Conv3D)              (None, 8, 10, 3, 112 1548288     Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_38 (Conv3D)              (None, 8, 10, 3, 288 1119744     activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_40 (Conv3D)              (None, 8, 10, 3, 64) 55296       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_41 (Conv3D)              (None, 8, 10, 3, 64) 884736      max_pooling3d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 10, 3, 112 336         conv3d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 10, 3, 288 864         conv3d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 10, 3, 112 0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 10, 3, 288 0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e (Concatenate)          (None, 8, 10, 3, 528 0           activation_36[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_40[0][0]              \n",
            "                                                                 activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_43 (Conv3D)              (None, 8, 10, 3, 160 2280960     Mixed_4e[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_45 (Conv3D)              (None, 8, 10, 3, 32) 456192      Mixed_4e[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 8, 10, 3, 160 480         conv3d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 8, 10, 3, 32) 96          conv3d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 8, 10, 3, 160 0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 8, 10, 3, 32) 0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_7 (MaxPooling3D)  (None, 8, 10, 3, 528 0           Mixed_4e[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_42 (Conv3D)              (None, 8, 10, 3, 256 3649536     Mixed_4e[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_44 (Conv3D)              (None, 8, 10, 3, 320 1382400     activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_46 (Conv3D)              (None, 8, 10, 3, 128 110592      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_47 (Conv3D)              (None, 8, 10, 3, 128 1824768     max_pooling3d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 10, 3, 256 768         conv3d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 8, 10, 3, 320 960         conv3d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 8, 10, 3, 128 384         conv3d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 8, 10, 3, 128 384         conv3d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 8, 10, 3, 256 0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 8, 10, 3, 320 0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 8, 10, 3, 128 0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 10, 3, 128 0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f (Concatenate)          (None, 8, 10, 3, 832 0           activation_42[0][0]              \n",
            "                                                                 activation_44[0][0]              \n",
            "                                                                 activation_46[0][0]              \n",
            "                                                                 activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_8 (MaxPooling3D)  (None, 4, 5, 3, 832) 0           Mixed_4f[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_48 (Conv3D)              (None, 4, 5, 3, 32)  26624       max_pooling3d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 5, 3, 32)  0           conv3d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_49 (Conv3D)              (None, 4, 5, 3, 128) 4096        activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_50 (Conv3D)              (None, 4, 5, 3, 128) 110592      activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 5, 3, 128) 0           conv3d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 5, 3, 128) 0           conv3d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 4, 5, 3, 256) 0           activation_49[0][0]              \n",
            "                                                                 activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_52 (Conv3D)              (None, 4, 5, 3, 160) 1105920     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_54 (Conv3D)              (None, 4, 5, 3, 32)  221184      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 5, 3, 160) 480         conv3d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 5, 3, 32)  96          conv3d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 5, 3, 160) 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 4, 5, 3, 32)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_9 (MaxPooling3D)  (None, 4, 5, 3, 256) 0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_51 (Conv3D)              (None, 4, 5, 3, 256) 1769472     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_53 (Conv3D)              (None, 4, 5, 3, 320) 1382400     activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_55 (Conv3D)              (None, 4, 5, 3, 128) 110592      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_56 (Conv3D)              (None, 4, 5, 3, 128) 884736      max_pooling3d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 5, 3, 256) 768         conv3d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 5, 3, 320) 960         conv3d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 5, 3, 256) 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 4, 5, 3, 320) 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b (Concatenate)          (None, 4, 5, 3, 832) 0           activation_51[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_55[0][0]              \n",
            "                                                                 activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_58 (Conv3D)              (None, 4, 5, 3, 192) 4313088     Mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_60 (Conv3D)              (None, 4, 5, 3, 48)  1078272     Mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 5, 3, 192) 576         conv3d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 4, 5, 3, 48)  144         conv3d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 4, 5, 3, 192) 0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 4, 5, 3, 48)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_10 (MaxPooling3D) (None, 4, 5, 3, 832) 0           Mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_57 (Conv3D)              (None, 4, 5, 3, 384) 8626176     Mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_59 (Conv3D)              (None, 4, 5, 3, 384) 1990656     activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_61 (Conv3D)              (None, 4, 5, 3, 128) 165888      activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_62 (Conv3D)              (None, 4, 5, 3, 128) 2875392     max_pooling3d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 5, 3, 384) 1152        conv3d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 4, 5, 3, 384) 1152        conv3d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 4, 5, 3, 384) 0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 4, 5, 3, 384) 0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c (Concatenate)          (None, 4, 5, 3, 1024 0           activation_57[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "                                                                 activation_61[0][0]              \n",
            "                                                                 activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_63 (Conv3D)              (None, 4, 5, 3, 64)  65536       Mixed_5c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 4, 5, 3, 64)  0           conv3d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_64 (Conv3D)              (None, 4, 5, 3, 256) 16384       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_65 (Conv3D)              (None, 4, 5, 3, 256) 442368      activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 4, 5, 3, 256) 0           conv3d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 4, 5, 3, 256) 0           conv3d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 4, 5, 3, 512) 0           activation_64[0][0]              \n",
            "                                                                 activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d (Globa (None, 512)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "gender_input (InputLayer)       [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 513)          0           global_average_pooling3d[0][0]   \n",
            "                                                                 gender_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          263168      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          131328      dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          32896       dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "regression (Dense)              (None, 1)            129         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 59,442,449\n",
            "Trainable params: 59,427,889\n",
            "Non-trainable params: 14,560\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKhtHI_RbEHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eangsijutuOb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82e14213-d458-4b28-83d5-6b2793fe3f08"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BioAgeNet_CDR_classification\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 121, 145, 6, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d (Conv3D)                 (None, 61, 73, 6, 64 1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 61, 73, 6, 64 192         conv3d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 61, 73, 6, 64 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "MaxPool2d_2a_3x3 (MaxPooling3D) (None, 31, 37, 6, 64 0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_1 (Conv3D)               (None, 31, 37, 6, 64 110592      MaxPool2d_2a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 31, 37, 6, 64 192         conv3d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 31, 37, 6, 64 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_2 (Conv3D)               (None, 31, 37, 6, 19 331776      activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 31, 37, 6, 19 576         conv3d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 31, 37, 6, 19 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "MaxPool2d_3a_3x3 (MaxPooling3D) (None, 16, 19, 6, 19 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_4 (Conv3D)               (None, 16, 19, 6, 96 497664      MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_6 (Conv3D)               (None, 16, 19, 6, 16 82944       MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 19, 6, 96 288         conv3d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 19, 6, 16 48          conv3d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 19, 6, 96 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 19, 6, 16 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D)    (None, 16, 19, 6, 19 0           MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_3 (Conv3D)               (None, 16, 19, 6, 64 331776      MaxPool2d_3a_3x3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_5 (Conv3D)               (None, 16, 19, 6, 12 331776      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_7 (Conv3D)               (None, 16, 19, 6, 32 13824       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_8 (Conv3D)               (None, 16, 19, 6, 32 165888      max_pooling3d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 19, 6, 64 192         conv3d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 19, 6, 12 384         conv3d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 19, 6, 32 96          conv3d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 19, 6, 32 96          conv3d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 19, 6, 64 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 19, 6, 12 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 19, 6, 32 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 19, 6, 32 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b (Concatenate)          (None, 16, 19, 6, 25 0           activation_3[0][0]               \n",
            "                                                                 activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_10 (Conv3D)              (None, 16, 19, 6, 12 884736      Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_12 (Conv3D)              (None, 16, 19, 6, 32 221184      Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 19, 6, 12 384         conv3d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 19, 6, 32 96          conv3d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 19, 6, 12 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 19, 6, 32 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3D)  (None, 16, 19, 6, 25 0           Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_9 (Conv3D)               (None, 16, 19, 6, 12 884736      Mixed_3b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_11 (Conv3D)              (None, 16, 19, 6, 19 663552      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_13 (Conv3D)              (None, 16, 19, 6, 96 82944       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_14 (Conv3D)              (None, 16, 19, 6, 64 442368      max_pooling3d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 19, 6, 12 384         conv3d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 19, 6, 19 576         conv3d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 19, 6, 96 288         conv3d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 19, 6, 64 192         conv3d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 19, 6, 12 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 19, 6, 19 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 19, 6, 96 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 19, 6, 64 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c (Concatenate)          (None, 16, 19, 6, 48 0           activation_9[0][0]               \n",
            "                                                                 activation_11[0][0]              \n",
            "                                                                 activation_13[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3D)  (None, 8, 10, 3, 480 0           Mixed_3c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_16 (Conv3D)              (None, 8, 10, 3, 96) 1244160     max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_18 (Conv3D)              (None, 8, 10, 3, 16) 207360      max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 10, 3, 96) 288         conv3d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 10, 3, 16) 48          conv3d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 10, 3, 96) 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 8, 10, 3, 16) 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3D)  (None, 8, 10, 3, 480 0           max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_15 (Conv3D)              (None, 8, 10, 3, 192 2488320     max_pooling3d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_17 (Conv3D)              (None, 8, 10, 3, 208 539136      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_19 (Conv3D)              (None, 8, 10, 3, 48) 20736       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_20 (Conv3D)              (None, 8, 10, 3, 64) 829440      max_pooling3d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 10, 3, 192 576         conv3d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 10, 3, 208 624         conv3d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 10, 3, 48) 144         conv3d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 10, 3, 192 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 8, 10, 3, 208 0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 10, 3, 48) 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b (Concatenate)          (None, 8, 10, 3, 512 0           activation_15[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_19[0][0]              \n",
            "                                                                 activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_21 (Conv3D)              (None, 8, 10, 3, 16) 8192        Mixed_4b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 10, 3, 16) 0           conv3d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_22 (Conv3D)              (None, 8, 10, 3, 64) 1024        activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_23 (Conv3D)              (None, 8, 10, 3, 64) 27648       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 10, 3, 64) 0           conv3d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 10, 3, 64) 0           conv3d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 8, 10, 3, 128 0           activation_22[0][0]              \n",
            "                                                                 activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_25 (Conv3D)              (None, 8, 10, 3, 112 387072      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_27 (Conv3D)              (None, 8, 10, 3, 24) 82944       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 10, 3, 112 336         conv3d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 10, 3, 24) 72          conv3d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 8, 10, 3, 112 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 10, 3, 24) 0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_4 (MaxPooling3D)  (None, 8, 10, 3, 128 0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_24 (Conv3D)              (None, 8, 10, 3, 160 552960      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_26 (Conv3D)              (None, 8, 10, 3, 224 677376      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_28 (Conv3D)              (None, 8, 10, 3, 64) 41472       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_29 (Conv3D)              (None, 8, 10, 3, 64) 221184      max_pooling3d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 10, 3, 160 480         conv3d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 10, 3, 224 672         conv3d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 10, 3, 160 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 10, 3, 224 0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c (Concatenate)          (None, 8, 10, 3, 512 0           activation_24[0][0]              \n",
            "                                                                 activation_26[0][0]              \n",
            "                                                                 activation_28[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_31 (Conv3D)              (None, 8, 10, 3, 128 1769472     Mixed_4c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_33 (Conv3D)              (None, 8, 10, 3, 24) 331776      Mixed_4c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 10, 3, 128 384         conv3d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 10, 3, 24) 72          conv3d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 10, 3, 128 0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 10, 3, 24) 0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_5 (MaxPooling3D)  (None, 8, 10, 3, 512 0           Mixed_4c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_30 (Conv3D)              (None, 8, 10, 3, 128 1769472     Mixed_4c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_32 (Conv3D)              (None, 8, 10, 3, 256 884736      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_34 (Conv3D)              (None, 8, 10, 3, 64) 41472       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_35 (Conv3D)              (None, 8, 10, 3, 64) 884736      max_pooling3d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 10, 3, 128 384         conv3d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 10, 3, 256 768         conv3d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 10, 3, 128 0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 10, 3, 256 0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d (Concatenate)          (None, 8, 10, 3, 512 0           activation_30[0][0]              \n",
            "                                                                 activation_32[0][0]              \n",
            "                                                                 activation_34[0][0]              \n",
            "                                                                 activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_37 (Conv3D)              (None, 8, 10, 3, 144 1990656     Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_39 (Conv3D)              (None, 8, 10, 3, 32) 442368      Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 10, 3, 144 432         conv3d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 10, 3, 32) 96          conv3d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 10, 3, 144 0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 10, 3, 32) 0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_6 (MaxPooling3D)  (None, 8, 10, 3, 512 0           Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_36 (Conv3D)              (None, 8, 10, 3, 112 1548288     Mixed_4d[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_38 (Conv3D)              (None, 8, 10, 3, 288 1119744     activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_40 (Conv3D)              (None, 8, 10, 3, 64) 55296       activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_41 (Conv3D)              (None, 8, 10, 3, 64) 884736      max_pooling3d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 10, 3, 112 336         conv3d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 10, 3, 288 864         conv3d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 10, 3, 64) 192         conv3d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 10, 3, 112 0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 10, 3, 288 0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 8, 10, 3, 64) 0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e (Concatenate)          (None, 8, 10, 3, 528 0           activation_36[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_40[0][0]              \n",
            "                                                                 activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_43 (Conv3D)              (None, 8, 10, 3, 160 2280960     Mixed_4e[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_45 (Conv3D)              (None, 8, 10, 3, 32) 456192      Mixed_4e[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 8, 10, 3, 160 480         conv3d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 8, 10, 3, 32) 96          conv3d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 8, 10, 3, 160 0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 8, 10, 3, 32) 0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_7 (MaxPooling3D)  (None, 8, 10, 3, 528 0           Mixed_4e[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_42 (Conv3D)              (None, 8, 10, 3, 256 3649536     Mixed_4e[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_44 (Conv3D)              (None, 8, 10, 3, 320 1382400     activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_46 (Conv3D)              (None, 8, 10, 3, 128 110592      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_47 (Conv3D)              (None, 8, 10, 3, 128 1824768     max_pooling3d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 10, 3, 256 768         conv3d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 8, 10, 3, 320 960         conv3d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 8, 10, 3, 128 384         conv3d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 8, 10, 3, 128 384         conv3d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 8, 10, 3, 256 0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 8, 10, 3, 320 0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 8, 10, 3, 128 0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 8, 10, 3, 128 0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f (Concatenate)          (None, 8, 10, 3, 832 0           activation_42[0][0]              \n",
            "                                                                 activation_44[0][0]              \n",
            "                                                                 activation_46[0][0]              \n",
            "                                                                 activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_8 (MaxPooling3D)  (None, 4, 5, 3, 832) 0           Mixed_4f[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_48 (Conv3D)              (None, 4, 5, 3, 32)  26624       max_pooling3d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 5, 3, 32)  0           conv3d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_49 (Conv3D)              (None, 4, 5, 3, 128) 4096        activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_50 (Conv3D)              (None, 4, 5, 3, 128) 110592      activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 5, 3, 128) 0           conv3d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 5, 3, 128) 0           conv3d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 4, 5, 3, 256) 0           activation_49[0][0]              \n",
            "                                                                 activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_52 (Conv3D)              (None, 4, 5, 3, 160) 1105920     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_54 (Conv3D)              (None, 4, 5, 3, 32)  221184      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 5, 3, 160) 480         conv3d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 5, 3, 32)  96          conv3d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 5, 3, 160) 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 4, 5, 3, 32)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_9 (MaxPooling3D)  (None, 4, 5, 3, 256) 0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_51 (Conv3D)              (None, 4, 5, 3, 256) 1769472     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_53 (Conv3D)              (None, 4, 5, 3, 320) 1382400     activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_55 (Conv3D)              (None, 4, 5, 3, 128) 110592      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_56 (Conv3D)              (None, 4, 5, 3, 128) 884736      max_pooling3d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 5, 3, 256) 768         conv3d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 5, 3, 320) 960         conv3d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 5, 3, 256) 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 4, 5, 3, 320) 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b (Concatenate)          (None, 4, 5, 3, 832) 0           activation_51[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_55[0][0]              \n",
            "                                                                 activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_58 (Conv3D)              (None, 4, 5, 3, 192) 4313088     Mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_60 (Conv3D)              (None, 4, 5, 3, 48)  1078272     Mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 5, 3, 192) 576         conv3d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 4, 5, 3, 48)  144         conv3d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 4, 5, 3, 192) 0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 4, 5, 3, 48)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling3d_10 (MaxPooling3D) (None, 4, 5, 3, 832) 0           Mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_57 (Conv3D)              (None, 4, 5, 3, 384) 8626176     Mixed_5b[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_59 (Conv3D)              (None, 4, 5, 3, 384) 1990656     activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_61 (Conv3D)              (None, 4, 5, 3, 128) 165888      activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_62 (Conv3D)              (None, 4, 5, 3, 128) 2875392     max_pooling3d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 5, 3, 384) 1152        conv3d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 4, 5, 3, 384) 1152        conv3d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 4, 5, 3, 128) 384         conv3d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 4, 5, 3, 384) 0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 4, 5, 3, 384) 0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 4, 5, 3, 128) 0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c (Concatenate)          (None, 4, 5, 3, 1024 0           activation_57[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "                                                                 activation_61[0][0]              \n",
            "                                                                 activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_63 (Conv3D)              (None, 4, 5, 3, 64)  65536       Mixed_5c[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 4, 5, 3, 64)  0           conv3d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_64 (Conv3D)              (None, 4, 5, 3, 256) 16384       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3d_65 (Conv3D)              (None, 4, 5, 3, 256) 442368      activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 4, 5, 3, 256) 0           conv3d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 4, 5, 3, 256) 0           conv3d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 4, 5, 3, 512) 0           activation_64[0][0]              \n",
            "                                                                 activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling3d (Globa (None, 512)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "gender_input (InputLayer)       [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 513)          0           global_average_pooling3d[0][0]   \n",
            "                                                                 gender_input[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          263168      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          131328      dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          32896       dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "classification (Dense)          (None, 4)            516         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 59,442,836\n",
            "Trainable params: 59,428,276\n",
            "Non-trainable params: 14,560\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIWXRNXszCD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fccc0993-985e-4468-b4ba-1afeeed0b991"
      },
      "source": [
        "for l in model.layers:\n",
        "  print(l.name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_1\n",
            "conv3d\n",
            "batch_normalization\n",
            "activation\n",
            "MaxPool2d_2a_3x3\n",
            "conv3d_1\n",
            "batch_normalization_1\n",
            "activation_1\n",
            "conv3d_2\n",
            "batch_normalization_2\n",
            "activation_2\n",
            "MaxPool2d_3a_3x3\n",
            "conv3d_4\n",
            "conv3d_6\n",
            "batch_normalization_4\n",
            "batch_normalization_6\n",
            "activation_4\n",
            "activation_6\n",
            "max_pooling3d\n",
            "conv3d_3\n",
            "conv3d_5\n",
            "conv3d_7\n",
            "conv3d_8\n",
            "batch_normalization_3\n",
            "batch_normalization_5\n",
            "batch_normalization_7\n",
            "batch_normalization_8\n",
            "activation_3\n",
            "activation_5\n",
            "activation_7\n",
            "activation_8\n",
            "Mixed_3b\n",
            "conv3d_10\n",
            "conv3d_12\n",
            "batch_normalization_10\n",
            "batch_normalization_12\n",
            "activation_10\n",
            "activation_12\n",
            "max_pooling3d_1\n",
            "conv3d_9\n",
            "conv3d_11\n",
            "conv3d_13\n",
            "conv3d_14\n",
            "batch_normalization_9\n",
            "batch_normalization_11\n",
            "batch_normalization_13\n",
            "batch_normalization_14\n",
            "activation_9\n",
            "activation_11\n",
            "activation_13\n",
            "activation_14\n",
            "Mixed_3c\n",
            "max_pooling3d_2\n",
            "conv3d_16\n",
            "conv3d_18\n",
            "batch_normalization_16\n",
            "batch_normalization_18\n",
            "activation_16\n",
            "activation_18\n",
            "max_pooling3d_3\n",
            "conv3d_15\n",
            "conv3d_17\n",
            "conv3d_19\n",
            "conv3d_20\n",
            "batch_normalization_15\n",
            "batch_normalization_17\n",
            "batch_normalization_19\n",
            "batch_normalization_20\n",
            "activation_15\n",
            "activation_17\n",
            "activation_19\n",
            "activation_20\n",
            "Mixed_4b\n",
            "conv3d_21\n",
            "activation_21\n",
            "conv3d_22\n",
            "conv3d_23\n",
            "activation_22\n",
            "activation_23\n",
            "concatenate\n",
            "conv3d_25\n",
            "conv3d_27\n",
            "batch_normalization_22\n",
            "batch_normalization_24\n",
            "activation_25\n",
            "activation_27\n",
            "max_pooling3d_4\n",
            "conv3d_24\n",
            "conv3d_26\n",
            "conv3d_28\n",
            "conv3d_29\n",
            "batch_normalization_21\n",
            "batch_normalization_23\n",
            "batch_normalization_25\n",
            "batch_normalization_26\n",
            "activation_24\n",
            "activation_26\n",
            "activation_28\n",
            "activation_29\n",
            "Mixed_4c\n",
            "conv3d_31\n",
            "conv3d_33\n",
            "batch_normalization_28\n",
            "batch_normalization_30\n",
            "activation_31\n",
            "activation_33\n",
            "max_pooling3d_5\n",
            "conv3d_30\n",
            "conv3d_32\n",
            "conv3d_34\n",
            "conv3d_35\n",
            "batch_normalization_27\n",
            "batch_normalization_29\n",
            "batch_normalization_31\n",
            "batch_normalization_32\n",
            "activation_30\n",
            "activation_32\n",
            "activation_34\n",
            "activation_35\n",
            "Mixed_4d\n",
            "conv3d_37\n",
            "conv3d_39\n",
            "batch_normalization_34\n",
            "batch_normalization_36\n",
            "activation_37\n",
            "activation_39\n",
            "max_pooling3d_6\n",
            "conv3d_36\n",
            "conv3d_38\n",
            "conv3d_40\n",
            "conv3d_41\n",
            "batch_normalization_33\n",
            "batch_normalization_35\n",
            "batch_normalization_37\n",
            "batch_normalization_38\n",
            "activation_36\n",
            "activation_38\n",
            "activation_40\n",
            "activation_41\n",
            "Mixed_4e\n",
            "conv3d_43\n",
            "conv3d_45\n",
            "batch_normalization_40\n",
            "batch_normalization_42\n",
            "activation_43\n",
            "activation_45\n",
            "max_pooling3d_7\n",
            "conv3d_42\n",
            "conv3d_44\n",
            "conv3d_46\n",
            "conv3d_47\n",
            "batch_normalization_39\n",
            "batch_normalization_41\n",
            "batch_normalization_43\n",
            "batch_normalization_44\n",
            "activation_42\n",
            "activation_44\n",
            "activation_46\n",
            "activation_47\n",
            "Mixed_4f\n",
            "max_pooling3d_8\n",
            "conv3d_48\n",
            "activation_48\n",
            "conv3d_49\n",
            "conv3d_50\n",
            "activation_49\n",
            "activation_50\n",
            "concatenate_1\n",
            "conv3d_52\n",
            "conv3d_54\n",
            "batch_normalization_46\n",
            "batch_normalization_48\n",
            "activation_52\n",
            "activation_54\n",
            "max_pooling3d_9\n",
            "conv3d_51\n",
            "conv3d_53\n",
            "conv3d_55\n",
            "conv3d_56\n",
            "batch_normalization_45\n",
            "batch_normalization_47\n",
            "batch_normalization_49\n",
            "batch_normalization_50\n",
            "activation_51\n",
            "activation_53\n",
            "activation_55\n",
            "activation_56\n",
            "Mixed_5b\n",
            "conv3d_58\n",
            "conv3d_60\n",
            "batch_normalization_52\n",
            "batch_normalization_54\n",
            "activation_58\n",
            "activation_60\n",
            "max_pooling3d_10\n",
            "conv3d_57\n",
            "conv3d_59\n",
            "conv3d_61\n",
            "conv3d_62\n",
            "batch_normalization_51\n",
            "batch_normalization_53\n",
            "batch_normalization_55\n",
            "batch_normalization_56\n",
            "activation_57\n",
            "activation_59\n",
            "activation_61\n",
            "activation_62\n",
            "Mixed_5c\n",
            "conv3d_63\n",
            "activation_63\n",
            "conv3d_64\n",
            "conv3d_65\n",
            "activation_64\n",
            "activation_65\n",
            "concatenate_2\n",
            "global_average_pooling3d\n",
            "gender_input\n",
            "concatenate_3\n",
            "dense\n",
            "dense_1\n",
            "dense_2\n",
            "classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjNSW1xOO6_4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e63bfa2-46d1-43e2-c855-a1c677dff4a6"
      },
      "source": [
        "[i for i in range(65,0,-1)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[65,\n",
              " 64,\n",
              " 63,\n",
              " 62,\n",
              " 61,\n",
              " 60,\n",
              " 59,\n",
              " 58,\n",
              " 57,\n",
              " 56,\n",
              " 55,\n",
              " 54,\n",
              " 53,\n",
              " 52,\n",
              " 51,\n",
              " 50,\n",
              " 49,\n",
              " 48,\n",
              " 47,\n",
              " 46,\n",
              " 45,\n",
              " 44,\n",
              " 43,\n",
              " 42,\n",
              " 41,\n",
              " 40,\n",
              " 39,\n",
              " 38,\n",
              " 37,\n",
              " 36,\n",
              " 35,\n",
              " 34,\n",
              " 33,\n",
              " 32,\n",
              " 31,\n",
              " 30,\n",
              " 29,\n",
              " 28,\n",
              " 27,\n",
              " 26,\n",
              " 25,\n",
              " 24,\n",
              " 23,\n",
              " 22,\n",
              " 21,\n",
              " 20,\n",
              " 19,\n",
              " 18,\n",
              " 17,\n",
              " 16,\n",
              " 15,\n",
              " 14,\n",
              " 13,\n",
              " 12,\n",
              " 11,\n",
              " 10,\n",
              " 9,\n",
              " 8,\n",
              " 7,\n",
              " 6,\n",
              " 5,\n",
              " 4,\n",
              " 3,\n",
              " 2,\n",
              " 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YomghLXW2kQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}